{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02db14a9",
   "metadata": {},
   "source": [
    "编辑人:苏则茜\n",
    "Project:Prob-Na-Learning\n",
    "+ WBI行为数据分析：时间轴对齐，运动参数提取\n",
    "+ 数据裁剪，数据拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import heapq\n",
    "from scipy.ndimage import label\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.ndimage import grey_opening,grey_closing\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from matplotlib.lines import Line2D\n",
    "import networkx as nx\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import ndimage\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "# 设置字体为 SimHei（黑体）\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置字体为 SimHei\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d2d41",
   "metadata": {},
   "source": [
    "# Preprocessing function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfdfbb",
   "metadata": {},
   "source": [
    "## Coordination Alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c57dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Realign_coordinate(p_f, df_motion): \n",
    "    '''\n",
    "    不同琼脂垫空间对齐\n",
    "    2025.2.27：目前stage calibrate设置原点在右下角，这个代码将原点根据四个标定点重新设为左上角。\n",
    "    另外根据x轴轨迹做镜面翻转\n",
    "    '''\n",
    "    \n",
    "    # 初始化\n",
    "    df_ul = pd.DataFrame({})\n",
    "    df_ur = pd.DataFrame({})\n",
    "    df_ll = pd.DataFrame({})\n",
    "    df_lr = pd.DataFrame({})\n",
    "    files = [f for f in os.listdir(p_f) if '.txt' in f ]\n",
    "    \n",
    "    # 找到四个顶点的坐标\n",
    "    for f in files:\n",
    "        if 'upper_left' in f:\n",
    "            # 运动参数位置文件\n",
    "            column_names = [ 'X', 'Y']\n",
    "            df_ul = pd.read_csv(os.path.join(p_f, f),sep=r',', header=None, names=column_names)\n",
    "            if len(df_ul) == 1:\n",
    "                df_ul.index = ['upper_left']\n",
    "            else:\n",
    "                print('upper_left标记数量不对')\n",
    "        if 'upper_right' in f:\n",
    "            # 运动参数位置文件\n",
    "            column_names = [ 'X', 'Y']\n",
    "            df_ur = pd.read_csv(os.path.join(p_f, f),sep=r',', header=None, names=column_names)\n",
    "            if len(df_ur) == 1:\n",
    "                df_ur.index = ['upper_right']\n",
    "            else:\n",
    "                print('upper_right标记数量不对')\n",
    "        if ('lower_left' in f) :\n",
    "            # 运动参数位置文件\n",
    "            column_names = [ 'X', 'Y']\n",
    "            df_ll = pd.read_csv(os.path.join(p_f, f),sep=r',', header=None, names=column_names)\n",
    "            if len(df_ll) == 1:\n",
    "                df_ll.index = ['lower_left']\n",
    "            else:\n",
    "                print('lower_left标记数量不对')\n",
    "        if 'lower_right' in f:\n",
    "            # 运动参数位置文件\n",
    "            column_names = [ 'X', 'Y']\n",
    "            df_lr = pd.read_csv(os.path.join(p_f, f),sep=r',', header=None, names=column_names)\n",
    "            if len(df_lr) == 1:\n",
    "                df_lr.index = ['lower_right']\n",
    "            else:\n",
    "                print('lower_right标记数量不对')\n",
    "\n",
    "    df_label = pd.concat([df_ul, df_ll, df_ur, df_lr])\n",
    "    df_label['X'] = df_label['X'].astype(float)/1000\n",
    "    df_label['Y'] = df_label['Y'].astype(float)/1000\n",
    "    # 调整x坐标(平移y轴)\n",
    "    x_min = (df_label.loc['upper_left','X'] + df_label.loc['lower_left','X'])/2\n",
    "    x_max = (df_label.loc['upper_right','X'] + df_label.loc['lower_right','X'])/2\n",
    "    if x_min > x_max:\n",
    "        # 如果原点在高钠一侧，则需要减去left的坐标后将X坐标值变号\n",
    "        # 目前的情况\n",
    "        df_motion_r = df_motion.copy()\n",
    "        df_motion_r['X'] = df_motion_r['X']-x_min\n",
    "        df_motion_r['X'] = -df_motion_r['X']\n",
    "    else:\n",
    "        df_motion_r = df_motion.copy()\n",
    "        df_motion_r['X'] = df_motion_r['X']-x_min\n",
    "        \n",
    "    # 调整y轴坐标，平移x轴  (另外y轴会和原坐标翻转)\n",
    "    y_min = (df_label.loc['lower_left','Y'] + df_label.loc['lower_right','Y'])/2\n",
    "    y_max = (df_label.loc['upper_left','Y'] + df_label.loc['upper_right','Y'])/2\n",
    "    \n",
    "    if y_min > y_max:\n",
    "        df_motion_r1 = df_motion_r.copy()\n",
    "        df_motion_r1['Y'] = df_motion_r1['Y']-y_max\n",
    "    else:\n",
    "        # 目前的情况\n",
    "        df_motion_r1 = df_motion_r.copy()\n",
    "        df_motion_r1['Y'] = df_motion_r1['Y']-y_max\n",
    "        df_motion_r1['Y'] = -df_motion_r1['Y']\n",
    "    return df_label, df_motion_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Realign_coordinate_false(p_f, df_motion): \n",
    "    '''\n",
    "     手动设置四个角的坐标\n",
    "    '''\n",
    "    upper_left = [64000, 32000]\n",
    "    upper_right = [24000, 32000]\n",
    "    lower_left = [64000, 0]\n",
    "    df_label['X'] = df_label['X'].astype(float)/1000\n",
    "    df_label['Y'] = df_label['Y'].astype(float)/1000\n",
    "    # 调整x坐标(平移y轴)\n",
    "    x_min = upper_left[0]\n",
    "    x_max = upper_right[0]\n",
    "    if x_min > x_max:\n",
    "        # 如果原点在高钠一侧，则需要减去left的坐标后将X坐标值变号\n",
    "        df_motion_r = df_motion.copy()\n",
    "        df_motion_r['X'] = df_motion_r['X']-x_min\n",
    "        df_motion_r['X'] = -df_motion_r['X']\n",
    "    else:\n",
    "        df_motion_r = df_motion.copy()\n",
    "        df_motion_r['X'] = df_motion_r['X']-x_min\n",
    "        \n",
    "    # 调整y轴坐标，平移x轴  (另外y轴会和原坐标翻转)\n",
    "    y_min = lower_left[1]\n",
    "    y_max = upper_left[1]\n",
    "    \n",
    "    if y_min > y_max:\n",
    "        df_motion_r1 = df_motion_r.copy()\n",
    "        df_motion_r1['Y'] = df_motion_r1['Y']-y_max\n",
    "    else:\n",
    "        df_motion_r1 = df_motion_r.copy()\n",
    "        df_motion_r1['Y'] = df_motion_r1['Y']-y_max\n",
    "        df_motion_r1['Y'] = -df_motion_r1['Y']\n",
    "    return df_label, df_motion_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Realign_coordinate_by_edge(p_f, df_motion):\n",
    "    '''\n",
    "    旧数据标记edge0和edge45，需要判断0和4.5哪一个在左侧，然后将x轴原点设置在0g边缘，且正方向与梯度相同\n",
    "    只修改x轴，不改y轴数据\n",
    "    '''\n",
    "    # 初始化\n",
    "    df_ul = pd.DataFrame({})\n",
    "    df_ur = pd.DataFrame({})\n",
    "    df_ll = pd.DataFrame({})\n",
    "    df_lr = pd.DataFrame({})\n",
    "    files = [f for f in os.listdir(p_f) if ('.txt' in f) & ('edge' in f) ]\n",
    "    # 找到四个顶点的坐标\n",
    "    for f in files:\n",
    "        if 'edge0' in f:\n",
    "            edge0 = np.loadtxt(os.path.join(p_f, f),delimiter=',')\n",
    "            edge0_x = edge0[:,0]\n",
    "        if 'edge45' in f:\n",
    "            edge45 = np.loadtxt(os.path.join(p_f, f),delimiter=',')\n",
    "            edge45_x = edge45[:,0]\n",
    "    rand_seq_0 = np.random.randint(0, 2, size=len(edge0_x))\n",
    "    mean_x0 = np.mean(edge0_x[rand_seq_0==1])/1000\n",
    "    rand_seq_45 = np.random.randint(0, 2, size=len(edge45_x))\n",
    "    mean_x45 = np.mean(edge45_x[rand_seq_45==1])/1000\n",
    "    print(f'edge0的x平均坐标为{mean_x0},edge45的x平均坐标为{mean_x45}')\n",
    "    if mean_x0 > mean_x45:\n",
    "        print(\"0的坐标更大，说明需要在减去0边界的x坐标后完成坐标轴转换\")\n",
    "        df_motion_r = df_motion.copy()\n",
    "        df_motion_r['X'] = df_motion_r['X']-mean_x0\n",
    "        df_motion_r['X'] = -df_motion_r['X']\n",
    "    else:\n",
    "        print(\"不需要坐标轴转换，只需要将原点平移到edge0\")\n",
    "        df_motion_r = df_motion.copy()\n",
    "        df_motion_r['X'] = df_motion_r['X']-mean_x0\n",
    "    \n",
    "    # 作图验证\n",
    "    plt.scatter(df_motion_r['X'], df_motion_r['Y'], s=0.7, c=df_motion_r['Time'], cmap='Reds')\n",
    "    plt.title(os.path.basename(p_f))\n",
    "    plt.show()\n",
    "    \n",
    "    return df_motion_r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664810ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_f = r'Z:\\data space+\\C. elegans chemotaxis\\2025_WBI\\good_WBI\\no_binary\\20250116_4.5g-ov_05'\n",
    "# df_stage = pd.read_csv(os.path.join(p_f, \"stage_data.txt\"),sep=r'\\s+', header=None,names=column_names)\n",
    "# columns_to_clean = ['Time', 'p_X', 'p_Y', 'X', 'Y']\n",
    "# for col in columns_to_clean:\n",
    "#     df_stage[col] = df_stage[col].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "# df_motion = df_stage[['Time','X','Y']]/1000  # 转换单位为s和mm\n",
    "\n",
    "# Realign_coordinate_by_edge(p_f, df_motion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaa427",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee792f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_and_draw(df, n, alg_spd = 1, CTX = 0, speed = 0,turn=1, reorientation=1):\n",
    "    # 将轨迹分段可视化\n",
    "    df_idx_list = np.arange(0, len(df), n)   # 切数据\n",
    "    for i, idx in enumerate(df_idx_list[:-1]):\n",
    "        df_d = df.loc[idx: df_idx_list[i+1], :]\n",
    "        plt.figure()\n",
    "        plt.scatter(df_d.X,df_d.Y, s=0.8,c = df_d.Time, cmap='jet')\n",
    "        ax = plt.gca()\n",
    "        plt.title(f'idx{idx}')\n",
    "        plt.colorbar(label='Time(s)')\n",
    "        ax.set_aspect(1)\n",
    "        plt.show()\n",
    "        \n",
    "        if alg_spd:\n",
    "            #画角速度\n",
    "            fig, ax = plt.subplots(1,2)\n",
    "            ax1 = ax[0].scatter(df_d.X,df_d.Y, s=0.8,c = df_d.agl_speed, cmap='jet')\n",
    "            plt.title(f'idx{idx} agl_speed')\n",
    "            plt.colorbar(ax1, label='Time(s)')\n",
    "            ax[0].set_aspect(1)\n",
    "            ax[1].hist(df_d.agl_speed)\n",
    "            plt.show()\n",
    "        if CTX:\n",
    "            # 画CTX\n",
    "            fig, ax = plt.subplots(1,2)\n",
    "            ax1 = ax[0].scatter(df_d.X,df_d.Y, s=0.8,c = df_d.CTX_left, cmap='jet')\n",
    "            plt.title(f'idx{idx} CTX')\n",
    "            plt.colorbar(ax1, label='Time(s)')\n",
    "            ax[0].set_aspect(1)\n",
    "            ax[1].hist(df_d.CTX_left)\n",
    "            ax[1].set_title('CTX_left distribution')\n",
    "            plt.show()\n",
    "        if speed:\n",
    "            # 画速度\n",
    "            fig, ax = plt.subplots(1,2)\n",
    "            ax1 = ax[0].scatter(df_d.X,df_d.Y, s=0.8,c = df_d.speed, cmap='jet')\n",
    "            plt.title(f'idx{idx} agl_speed')\n",
    "            plt.colorbar(ax1,label='Time(s)')\n",
    "            ax[0].set_aspect(1)\n",
    "            ax[1].hist(df_d.speed)\n",
    "            ax[1].set_title('speed distribution')\n",
    "            plt.show()\n",
    "        if reorientation:\n",
    "            fig, ax = plt.subplots(1,2)\n",
    "            ax[1].scatter(df_d.X, df_d.Y, c = 'grey',s=0.8, alpha=0.05)\n",
    "            ax[0].scatter(df_d.X, df_d.Y, c = 'grey',s=0.8, alpha=0.05)\n",
    "            df_pt = df_d[df_d.Reorientation==1]\n",
    "            df_tn = df_d[df_d.Event==1]\n",
    "            ax[0].scatter(df_tn.X, df_tn.Y, s = 0.8, c='r')\n",
    "            ax[1].scatter(df_pt.X, df_pt.Y, s = 0.8, c='r')\n",
    "            ax[0].set_title('Sharp turn')\n",
    "            ax[1].set_title('Reorientation')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scatter_Pos_Grad_Dir(df, Pos_CTX_vec, scale_size = 2, folder = '', x_lim=[]):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    \n",
    "    mean_x = df.X.mean()\n",
    "    mean_y = df.Y.mean()\n",
    "    origin = [mean_x, mean_y]\n",
    "    vector = np.array(Pos_CTX_vec)*scale_size\n",
    "    \n",
    "    plt.quiver(*origin, *vector, angles='xy', scale_units='xy',\n",
    "               scale=1, color='k', label='Pos_CTX_vec')\n",
    "#     # 在指定位置标注向量值\n",
    "#     vector_ctr = np.array([mean_x+0.5*vector[0], mean_y+0.5*vector[0]])\n",
    "#     print(vector_ctr)\n",
    "#     position = vector_ctr + np.array([0, 1])\n",
    "#     print('Position', position)\n",
    "#     plt.text(position[0], position[1], f\"{Pos_CTX_vec}\", fontsize=8, color='k')\n",
    "    plt.legend()\n",
    "    plt.scatter(df.X,df.Y, s=0.8,c = df.Time, cmap='jet')\n",
    "    ax = plt.gca()\n",
    "    plt.colorbar(label='Time(s)')\n",
    "    \n",
    "    if len(x_lim):\n",
    "        plt.xlim(*x_lim)\n",
    "    ax.set_aspect(1)\n",
    "    if folder:\n",
    "        fig.savefig(folder+'Pos-Grad-Dir.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32327515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scatter_Pirouette(df, folder = '', x_lim = []):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    plt.scatter(df.X, df.Y, c = 'grey',s=0.8, alpha=0.05)\n",
    "    df_pt = df[df.Reorientation==1]\n",
    "    plt.scatter(df_pt.X, df_pt.Y, s = 0.8, c='r')\n",
    "    plt.title('Reorientation')\n",
    "    if len(x_lim):\n",
    "        plt.xlim(*x_lim)\n",
    "    ax.set_aspect(1)\n",
    "    if folder:\n",
    "        fig.savefig(folder+'Visual_Reorientation.png')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea565e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_color_dict(key_items,color_map='tab10'):\n",
    "    '''\n",
    "    # 提前生成统一的color_dict()\n",
    "    # color(根据dates元素的数量分配，输出以key_items元素(可以是condition或dates)为key的颜色映射字典)\n",
    "    '''\n",
    "    \n",
    "    color_vec_norm = np.linspace(0.01,0.99,len(key_items)+1)\n",
    "    colormap = mpl.colormaps[color_map]\n",
    "    colors  = colormap(color_vec_norm)\n",
    "    color_dict = {}\n",
    "    for i, d in enumerate(key_items):\n",
    "        color_dict[d] = colors[i]\n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357da5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_time_intervals(df, time_len, time_inv, time_col='Vol_Time'):\n",
    "    \"\"\"\n",
    "    设置时间间隔并为 DataFrame 添加时间段标签。\n",
    "    时间列单位为s，不需要frame_rate\n",
    "\n",
    "    Parameters:\n",
    "    - df: 输入 DataFrame，需包含 'Timestamp' 列。\n",
    "    - time_len: 总时间长度（小时）。\n",
    "    - time_inv: 时间间隔（分钟）。\n",
    "\n",
    "    Returns:\n",
    "    - 更新后的 DataFrame，添加了 'Period_label' 列。\n",
    "    - 时间段标签列表。\n",
    "    \"\"\"\n",
    "    periods = np.arange(0, time_len + 1, time_inv)\n",
    "    labels = [(periods[i] + periods[i + 1]) / 2 for i in range(len(periods) - 1)]\n",
    "    df['Period_label'] = pd.cut(\n",
    "        df[time_col], bins=periods, labels=labels, right=False, include_lowest=True\n",
    "    )\n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d3487",
   "metadata": {},
   "source": [
    "## Motion Parameter Extract Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93564dd",
   "metadata": {},
   "source": [
    "### 角度计算  \n",
    "  Pirouette labeling(outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b24a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义角度计算公式\n",
    "def ang_cal(vec_1,vec_2):\n",
    "    dot_pro = np.dot(vec_1, vec_2)\n",
    "    mod_1 = np.sqrt(np.dot(vec_1,vec_1))\n",
    "    mod_2 = np.sqrt(np.dot(vec_2,vec_2)) \n",
    "    if mod_1 == 0 or mod_2 ==0:\n",
    "        angle = 0\n",
    "    else:\n",
    "        cos = dot_pro/(mod_1*mod_2)\n",
    "        if np.isnan(cos) == True:\n",
    "            angle = np.nan\n",
    "        else:  \n",
    "            angle = np.arccos(round(cos,1))  #弧度制\n",
    "    return angle\n",
    "\n",
    "def clws_delta_phi(vec_1, vec_2, vec_0 = [1,0]):\n",
    "    # vec_1 is the first vector, and it rotates to the vec_2\n",
    "    vec_0 = np.array(vec_0)\n",
    "    agl_1 = ang_cal(vec_1,vec_2)\n",
    "    agl_2 = ang_cal(vec_0, vec_1)\n",
    "    agl_3 = ang_cal(vec_0, vec_2)\n",
    "    if agl_1 == 0:\n",
    "        agl_1 = 0\n",
    "        # print('angle = 0°')\n",
    "    elif agl_1 == np.pi:\n",
    "        agl_1 = np.pi\n",
    "        # print('angle = 180°')\n",
    "    elif vec_1[1] >= 0 and vec_2[1] >= 0:    #同时在第一第二象限\n",
    "        if (agl_3 - agl_2) > 0:\n",
    "            agl_1 = (-1)*np.abs(agl_1)\n",
    "        else:\n",
    "            agl_1 = np.abs(agl_1)\n",
    "    elif vec_1[1] <= 0 and vec_2[1] <= 0:    #同时在第三第四象限\n",
    "        if (agl_3 - agl_2) > 0:\n",
    "            agl_1 = np.abs(agl_1)\n",
    "        else:\n",
    "            agl_1 = (-1)*np.abs(agl_1)\n",
    "    elif vec_1[1] >= 0 and vec_2[1] <= 0:\n",
    "        if (agl_3 + agl_2) > np.pi:\n",
    "            agl_1 = (-1)*np.abs(agl_1)\n",
    "        else:                                  #如果等于180°认为是顺时针旋转\n",
    "            agl_1 = np.abs(agl_1)\n",
    "    elif vec_1[1] <= 0 and vec_2[1] >= 0:\n",
    "        if (agl_3 + agl_2) < np.pi:\n",
    "            agl_1 = (-1)*np.abs(agl_1)\n",
    "        else:                                  #如果等于180°认为是顺时针旋转\n",
    "            agl_1 = np.abs(agl_1)   \n",
    "    return agl_1*180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6af5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Label_Pirouettes_by_id(g_df,t_crit=6.16, frame_rate = 30,min_agl_spd=None):\n",
    "#     '''根据角速度和turn的间隔标记Pirouette'''\n",
    "#     # 输入：同一个id的df, t_crit(sec)，如果需要重新分turn可以输入min_agl_spd\n",
    "#     # 注意输入的df中不能是不同组混合的，id必须独立\n",
    "#     t_f_crit = t_crit*frame_rate\n",
    "#     if min_agl_spd:\n",
    "#         g_df.loc[:,'Event']=0\n",
    "#         g_df.loc[g_df.agl_speed >= min_agl_spd, 'Event']=1\n",
    "        \n",
    "#     g_df.loc[:,'Reorientation']=g_df['Event']\n",
    "#     g_df = g_df.reset_index(drop=True)\n",
    "#     # nan值是计算速度和角速度时舍弃的点，默认是0不计算c\n",
    "#     event_vec = g_df.Event.fillna(0).values\n",
    "\n",
    "#     # 向后移动一位，上下相减\n",
    "#     diff_vec = event_vec[1:]-event_vec[:-1]\n",
    "#     if (diff_vec==0).all():\n",
    "#         pass\n",
    "#     elif ~(diff_vec==-1).any() or ~(diff_vec==1).any():\n",
    "#         print('只有结尾有turn或只有开头有turn')\n",
    "#     else:\n",
    "#         # 统计所有出现1和-1的索引\n",
    "#         idx_1 = np.where(diff_vec == 1)[0]\n",
    "#         idx_n1 = np.where(diff_vec == -1)[0]\n",
    "#         if np.min(idx_1)<np.min(idx_n1):\n",
    "#             # 如果最小的索引是1而不是-1，说明开头是run，然后才开始出现turn\n",
    "#             if len(idx_1)>len(idx_n1):\n",
    "#                 # 如果1的数量大于-1,说明最后一个turn没有结束\n",
    "# #                 print(f'验证最后是否以turn结束:{(np.max(idx_n1)<np.max(idx_1))}')\n",
    "#                 # 去掉第一个1的索引，用1减去所有-1的索引，判断长度\n",
    "#                 run_lens_btw_turns = idx_1[1:]-idx_n1\n",
    "#                 for i,r in enumerate(run_lens_btw_turns):\n",
    "#                     if r <= t_f_crit:\n",
    "#                         low_b = idx_n1[i]+1\n",
    "#                         upper_b = idx_1[i+1]+1      # 注意idx_1去头，所以索引内加以\n",
    "#                         g_df.loc[low_b:upper_b, 'Reorientation']=1\n",
    "\n",
    "#             elif len(idx_1)==len(idx_n1):\n",
    "#                 # 如果两者数量相同，说明最后以run结尾，这条轨迹前后是完整的多个turn\n",
    "# #                 print(f'验证最后是否以run结束:{(np.max(idx_n1)>np.max(idx_1))}')\n",
    "#                 # 去掉1的所索引的首部和-1的尾部，用1减去-1的索引判断长度\n",
    "#                 run_lens_btw_turns = idx_1[1:]-idx_n1[:-1]\n",
    "#                 for i,r in enumerate(run_lens_btw_turns):\n",
    "#                     if r <= t_f_crit:\n",
    "#                         low_b = idx_n1[i]+1\n",
    "#                         upper_b = idx_1[i+1]+1\n",
    "#                         g_df.loc[low_b:upper_b, 'Reorientation']=1\n",
    "\n",
    "#             # 无论是上述哪种情况，turn的数量都记为出现1的数量\n",
    "#             num_turn = len(idx_1)\n",
    "# #             print(f'Turn number: {num_turn}')\n",
    "#         elif np.min(idx_1)>np.min(idx_n1):\n",
    "#             # 如果最小索引是-1而不是1，说明开头就是turn\n",
    "#             if (len(idx_n1)== len(idx_1)+1):\n",
    "#                 # 如果-1比1多一个，说明以run结尾\n",
    "# #                 print(f'验证最后是否以run结束:{(np.max(idx_n1)>np.max(idx_1))}')\n",
    "#                 # 去掉最后一个-1的索引，用1减去-1\n",
    "#                 run_lens_btw_turns = idx_1-idx_n1[:-1]\n",
    "#                 for i,r in enumerate(run_lens_btw_turns):\n",
    "#                     if r <= t_f_crit:\n",
    "#                         low_b = idx_n1[i]+1\n",
    "#                         upper_b = idx_1[i]+1\n",
    "#                         g_df.loc[low_b:upper_b, 'Reorientation']=1\n",
    "#             elif (len(idx_n1) == len(idx_1)):\n",
    "#                 # 如果-1和1一样多，说明以turn结尾\n",
    "# #                 print(f'验证最后是否以turn结束:{(np.max(idx_n1)<np.max(idx_1))}')\n",
    "#                 # 直接用1的索引减去-1\n",
    "#                 run_lens_btw_turns = idx_1-idx_n1\n",
    "#                 for i,r in enumerate(run_lens_btw_turns):\n",
    "#                     if r <= t_f_crit:\n",
    "#                         low_b = idx_n1[i]+1\n",
    "#                         upper_b = idx_1[i]+1\n",
    "#                         g_df.loc[low_b:upper_b, 'Reorientation']=1\n",
    "#     return g_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c19145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Insert_SharpTurn(df_als, min_agl = 50, max_agl = 110):\n",
    "#     print('=======================开始事件分类===================================')\n",
    "#     print('注：事件分类仅完成分类（turn = 1, run = 0），tunrning rate的计算较为灵活，在汇总分析作图时使用')\n",
    "#     # 新建一列为Event\n",
    "#     if 'Event' in df_als:\n",
    "#         df_als = df_als.drop('Event', axis = 1)\n",
    "#     df_als['Event'] = np.nan\n",
    "#     # 选择角速度区间\n",
    "#     # sharp turn 标记为1\n",
    "#     df_als.loc[(np.abs(df_als.agl_velocity)>= min_agl)&(np.abs(df_als.agl_velocity) <= max_agl), 'Event'] = 1  \n",
    "#     # run 标记为0\n",
    "#     df_als.loc[(np.abs(df_als.agl_velocity) < min_agl), 'Event'] = 0\n",
    "#     return df_als"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84378803",
   "metadata": {},
   "source": [
    "### 计算连续运动参数变量:速度，CTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度，角速度，bearing angle, Dist_to_center, ctx计算函数\n",
    "# 输入：筛选为虫子的csv文件，画圆的csv文件，ID索引，速度平滑，角速度速度平滑，角速度平滑，追踪帧率，跳帧数量，像素长度\n",
    "def Sliding_CTX_calation(df_worms, grad_vec,sm_inv, spd_sm_inv, hlf_sm_inv,window_size = 20,\n",
    "                         frame_rate = 20, track_jump_frame = 1,\n",
    "                        X_shift = 0):\n",
    "    # 只适用于线性梯度，因为CTX只计算到左和右边缘的\n",
    "    # 速度平滑窗s sm_inv\n",
    "    # 计算角速度的速度平滑窗s spd_sm_inv\n",
    "    # 角速度平滑床s hlf_sm_inv\n",
    "    \n",
    "    # ======================================数据平滑=================\n",
    "    # 1. 移动窗口平滑\n",
    "    # 对X和Y坐标进行移动平均\n",
    "    df_slide = df_worms.copy()\n",
    "    df_slide['X_org'] = df_slide['X']                     # 将平滑前的轨迹也加入df\n",
    "    df_slide['Y_org'] = df_slide['Y']\n",
    "    df_slide['X'] = df_slide['X'].rolling(window=window_size, center=True).mean()\n",
    "    df_slide['Y'] = df_slide['Y'].rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    if X_shift:\n",
    "        # 如果X有偏离，向左偏离减去，向右偏移加上，单位mm\n",
    "        df_slide['X'] += X_shift\n",
    "        \n",
    "        \n",
    "    # ======================================速度计算=================\n",
    "    \n",
    "    \n",
    "    # 根据帧率计算半平滑窗，但是具体计算速度和角速度的时间根据时间戳来确定\n",
    "    print(sm_inv, frame_rate, track_jump_frame)\n",
    "    half_bins_spd = int((sm_inv*frame_rate)//track_jump_frame)\n",
    "    bins_spd = 2*half_bins_spd\n",
    "\n",
    "    trajectory_0 = df_slide[['X','Y','Time']].values      # 提取x,y坐标和时间戳\n",
    "    trajectory_1 = (trajectory_0.copy()).astype('float')                            # 转float数据类型\n",
    "    x = trajectory_1[:,0]\n",
    "    y = trajectory_1[:,1]\n",
    "    delta_x = (x[bins_spd:]-x[:len(x)-bins_spd])                       # 得到减去头尾数据点的delta_x和delta_y\n",
    "    delta_y = (y[bins_spd:]-y[:len(y)-bins_spd])\n",
    "    time_step = trajectory_1[:,2] \n",
    "    print('time_step', time_step)\n",
    "    time_step_vec = time_step[bins_spd:]-time_step[:len(time_step)-bins_spd]        # 得到减去头尾数据点的对应delta-x和delta-y的时间\n",
    "    print('平均时间间隔'+str(np.average(time_step_vec))+'应该等于总平滑窗'+str(sm_inv*2))\n",
    "    mean_time_inv = np.average(time_step_vec)   # 平均时间间隔\n",
    "    velocity_bef = np.dstack((delta_x/time_step_vec,delta_y/time_step_vec))[0]      # 使用dstack函数合并x,y方向计算的速度为一个矩阵\n",
    "    speed_bef = np.linalg.norm(velocity_bef, axis = 1).reshape(-1,1)\n",
    "    nan_bin_vec = np.full((half_bins_spd,2),np.nan)\n",
    "    nan_bin_spd = np.full((half_bins_spd,1),np.nan)\n",
    "    velocity = np.vstack((nan_bin_vec, velocity_bef, nan_bin_vec))                  # 速度向量\n",
    "    speed = np.vstack((nan_bin_spd, speed_bef, nan_bin_spd))                        # 速率向量\n",
    "    \n",
    "    df_idx = df_slide[['X','X_org','Y','Y_org','Time']].copy()      \n",
    "    df_idx['speed'] = pd.Series(speed[:,0],index = df_idx.index)                    # 将speed和velocity加入dataframe\n",
    "    df_idx['x_velocity'] = pd.Series(velocity[:,0],index = df_idx.index)\n",
    "    df_idx['y_velocity'] = pd.Series(velocity[:,1],index = df_idx.index)\n",
    "    \n",
    "    # ======================================================角速度计算===========\n",
    "\n",
    "    half_bins_spd_agl = int((spd_sm_inv*frame_rate)//track_jump_frame)\n",
    "    bins_spd_agl = 2*half_bins_spd_agl                                 # 角速度速度平滑窗\n",
    "    hlf_agl_bins = int((hlf_sm_inv*frame_rate)//track_jump_frame)      # 角速度半平滑窗\n",
    "    print('角速度半平滑窗：', hlf_agl_bins)\n",
    "    trajectory_0 = df_slide[['X','Y','Time']].values      # 提取x,y坐标和时间戳\n",
    "    trajectory_1 = (trajectory_0.copy()).astype('float')                            # 转float数据类型\n",
    "    x = trajectory_1[:,0]\n",
    "    y = trajectory_1[:,1]\n",
    "    delta_x = (x[bins_spd_agl:]-x[:len(x)-bins_spd_agl])                                     # 得到减去头尾数据点的delta_x和delta_y\n",
    "    delta_y = (y[bins_spd_agl:]-y[:len(y)-bins_spd_agl])\n",
    "    time_step = trajectory_1[:,2]   \n",
    "    time_step_vec = time_step[bins_spd_agl:]-time_step[:len(time_step)-bins_spd_agl]        # 得到减去头尾数据点的对应delta-x和delta-y的时间\n",
    "    velocity_bef = np.dstack((delta_x/time_step_vec,delta_y/time_step_vec))[0]             # 使用dstack函数合并x,y方向计算的速度为一个矩阵\n",
    "    speed_bef = np.linalg.norm(velocity_bef, axis = 1).reshape(-1,1)\n",
    "    nan_bin_vec = np.full((half_bins_spd_agl,2),np.nan)\n",
    "    nan_bin_spd = np.full((half_bins_spd_agl,1),np.nan)\n",
    "    velocity = np.vstack((nan_bin_vec, velocity_bef, nan_bin_vec))\n",
    "    speed = np.vstack((nan_bin_spd, speed_bef, nan_bin_spd))[:,0]\n",
    "    time_step_vector = np.vstack((nan_bin_spd, time_step_vec.reshape(-1,1), nan_bin_spd))[:,0]\n",
    "    \n",
    "    df_speed_cal = pd.DataFrame(speed,columns = ['speed'])              # 生成一个包含speed的df并提取索引\n",
    "    \n",
    "    idx_vec = df_speed_cal.index                                        # 生成索引列表时先掐头去尾再取非零值\n",
    "    df_washed_speed = df_speed_cal[half_bins_spd_agl:len(idx_vec)-half_bins_spd_agl]\n",
    "    idx_nz_vec = df_washed_speed[df_washed_speed['speed']!=0].index     # 索引是相对于总长度，但是这个列表中直接去掉了头尾速度为nan的和速率为0的点的索引\n",
    "    df_speed_cal['agl_velocity'] = pd.Series([np.nan]*len(idx_vec))\n",
    "\n",
    "    for i in range(hlf_agl_bins,len(idx_nz_vec)-hlf_agl_bins):\n",
    "        n = hlf_agl_bins\n",
    "        agl_i = clws_delta_phi(velocity[idx_nz_vec[i-n]],velocity[idx_nz_vec[i+n]]) \n",
    "        time_end= time_step[idx_nz_vec[i+n]]\n",
    "        time_start = time_step[idx_nz_vec[i-n]]\n",
    "        delta_t_i = time_end-time_start\n",
    "#         print(\"timestamp\", time_start, time_start)\n",
    "#         print('角度计算时间间隔：',delta_t_i)\n",
    "        agl_vel_i = agl_i/delta_t_i\n",
    "\n",
    "        if np.isnan(agl_vel_i) == False:\n",
    "            df_speed_cal['agl_velocity'].loc[idx_nz_vec[i]] = agl_vel_i\n",
    "        else:\n",
    "#             print(velocity[idx_nz_vec[i-n]],velocity[idx_nz_vec[i+n]])\n",
    "            df_speed_cal['agl_velocity'].loc[idx_nz_vec[i]] = 0 \n",
    "    # 将angular_velocity和angular_speed加入dataframe(df_idx)\n",
    "    df_idx['agl_velocity'] = pd.Series(df_speed_cal['agl_velocity'].values,index = df_idx.index)\n",
    "    # df_idx.loc[:,'x_velocity_agl'] = velocity[:,0]\n",
    "    # df_idx.loc[:,'y_velocity_agl'] = velocity[:,1]\n",
    "    df_idx.loc[df_idx.speed == 0, 'agl_velocity'] = 0                 # 将speed为0的点角速度设为0\n",
    "    df_idx['agl_speed'] = np.abs(df_idx['agl_velocity'])\n",
    "    \n",
    "    # ===========================================计算CTX=====================================\n",
    "    vec_left = grad_vec\n",
    "    vel_vec = df_idx[['x_velocity','y_velocity']].values\n",
    "    bearing_left = []\n",
    "    \n",
    "    ctxs = []\n",
    "    for i in range(len(vel_vec)):\n",
    "        agl_i_left = clws_delta_phi(vec_left,vel_vec[i,:])\n",
    "        agl_i_left = agl_i_left/180*np.pi     # bearing angle使用的是弧度制\n",
    "        bearing_left.append(agl_i_left)                                      # 使用弧度制\n",
    "        \n",
    "        vel_vec_i = vel_vec[i,:]\n",
    "        deno = np.sqrt(vel_vec_i[0]**2+vel_vec_i[1]**2)\n",
    "        if deno==0:\n",
    "            ctx_i = 0.0\n",
    "        else:\n",
    "            ctx_i = -vel_vec_i[0]/deno\n",
    "        ctxs.append(ctx_i)\n",
    "        \n",
    "    df_idx['CTX_left'] = pd.Series(ctxs, index = df_idx.index)\n",
    "    df_idx['bearing_left'] = pd.Series(bearing_left, index = df_idx.index)\n",
    "#     df_idx['CTX_left']=df_idx['bearing_left'].apply(np.cos,axis = 0)\n",
    "    \n",
    "\n",
    "\n",
    "    return df_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6139cbc",
   "metadata": {},
   "source": [
    "### 前后后退"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smth_forward_reverse(df, close_size = 50, open_size = 50):\n",
    "    '''\n",
    "    df: 包含angle_m列\n",
    "    返回：df：包含平滑后的forward列\n",
    "    '''\n",
    "    # 前进后退\n",
    "    # 根据阈值分割前进后退\n",
    "    threshold = 120\n",
    "    df.loc[:,'forward'] = 0\n",
    "    df.loc[df[\"angle_m\"]>=threshold,'forward'] = 1\n",
    "\n",
    "    # 平滑reversal\n",
    "    # 先进行闭操作（先膨胀后腐蚀）：填充小的空洞\n",
    "    closed = ndimage.binary_closing(df['forward'].values, structure=np.ones(close_size))\n",
    "    # 再进行开操作（先腐蚀后膨胀）：去除小的噪声点\n",
    "    opened = ndimage.binary_opening(closed, structure=np.ones(open_size))\n",
    "    # 将结果转换为整数并添加到 DataFrame\n",
    "    df['forward'] = opened.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835abdd",
   "metadata": {},
   "source": [
    "### 身体前进方向速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算根据运动方向旋转后的朝头部方向运动向量\n",
    "def rotation_mat_2(theta_degrees, mov_vec):\n",
    "    mov_vec = np.array(mov_vec)\n",
    "    if (theta_degrees == np.nan) or (theta_degrees == None):\n",
    "        return None\n",
    "    if (mov_vec is None) or (mov_vec.any()==np.nan):\n",
    "        return None\n",
    "    \"\"\"\n",
    "    统一的 2D 旋转矩阵\n",
    "    θ > 0: 逆时针旋转\n",
    "    θ < 0: 顺时针旋转\n",
    "    \"\"\"\n",
    "    theta_rad = -np.radians(theta_degrees)\n",
    "    cos_theta = np.cos(theta_rad)\n",
    "    sin_theta = np.sin(theta_rad)\n",
    "    rot_mat =  np.array([\n",
    "        [cos_theta, -sin_theta],\n",
    "        [sin_theta,  cos_theta]\n",
    "    ])\n",
    "    return np.dot(rot_mat, mov_vec)\n",
    "\n",
    "rotation_mat_vec = np.vectorize(rotation_mat_2)\n",
    "# 求向量投影\n",
    "def project_vector_A_on_B(vector_A, vector_B):\n",
    "    \"\"\"\n",
    "    返回C向量,方向与B相同,长度为A的投影\n",
    "    \"\"\"\n",
    "    # 转换为numpy数组\n",
    "    A = np.array(vector_A)\n",
    "    B = np.array(vector_B)\n",
    "    # 计算向量B的单位向量\n",
    "    B_norm = np.linalg.norm(B)\n",
    "    if B_norm == 0:\n",
    "        print('B向量不能为零')\n",
    "        return None\n",
    "        # raise ValueError(\"向量B不能为零向量\")\n",
    "    B_unit = B / B_norm\n",
    "    \n",
    "    # 计算向量A在向量B上的投影长度\n",
    "    projection_length = np.dot(A, B_unit)\n",
    "    # 计算向量C = 投影长度 × 单位向量B\n",
    "    vector_C = projection_length * B_unit\n",
    "    return vector_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_moving(df):\n",
    "    '''\n",
    "    df: 包含'x_velocity'和'y_velocity'列\n",
    "    返回 df includes:\n",
    "        'moving_vec': moving direction vector;\n",
    "        'heading_vec': heading vector ;\n",
    "       \"head_moving\": the component of velocity projected on the direction of heading\n",
    "    '''\n",
    "    # 运动方向向量\n",
    "    df.loc[:,'moving_vec'] = df.apply(lambda x: (x['x_velocity'], x['y_velocity']),axis = 1)\n",
    "\n",
    "    # 头部朝向向量\n",
    "    df.loc[:,'heading_vec'] = df.apply(lambda x: rotation_mat_2(x['angle_md'], x['moving_vec']), axis=1)\n",
    "    # 求投影头部朝向后的运动向量\n",
    "    df.loc[:,'head_moving'] = df.apply(lambda x: project_vector_A_on_B(x['moving_vec'], x['heading_vec']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c24e26",
   "metadata": {},
   "source": [
    "### 标记Omega turn和头部曲率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a14d2",
   "metadata": {},
   "source": [
    "#### 骨架合并与拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_length(coords: np.ndarray) -> float:\n",
    "    \"\"\"polyline 长度（逐段欧氏距离累加）。coords: (N,2)\"\"\"\n",
    "    if coords is None or len(coords) < 2:\n",
    "        return 0.0\n",
    "    d = np.diff(coords, axis=0)\n",
    "    return float(np.sum(np.sqrt((d**2).sum(axis=1))))\n",
    "\n",
    "def quantize_point(pt, q=1):\n",
    "    \"\"\"把像素点量化到网格上（容差聚类）。q=1 等价于原始整数像素匹配。\"\"\"\n",
    "    return (int(round(pt[0]/q)*q), int(round(pt[1]/q)*q))\n",
    "\n",
    "def cut_paths_to_edges(all_paths, q=1):\n",
    "    \"\"\"\n",
    "    把多条骨架在“端点”和“交叉点”处切段，返回 MultiGraph：\n",
    "    - 节点：关键点坐标（量化后）\n",
    "    - 边：两关键点之间的子折线，数据里带 coords/length\n",
    "    \"\"\"\n",
    "    # 1) 统计每个像素（量化后）被多少条路径/多少次命中，用于发现交叉\n",
    "    occ = {}  # key -> list of (path_idx, local_idx)\n",
    "    for pi, path in enumerate(all_paths):\n",
    "        if path is None or len(path) == 0:\n",
    "            continue\n",
    "        for si, pt in enumerate(path):\n",
    "            key = quantize_point(pt, q)\n",
    "            occ.setdefault(key, []).append((pi, si))\n",
    "\n",
    "    # 2) 关键点：每条 polyline 的首尾点 + 被多个 path 命中的点（交叉/重合点）\n",
    "    keypoints = set()\n",
    "    for pi, path in enumerate(all_paths):\n",
    "        if path is None or len(path) == 0:\n",
    "            continue\n",
    "        keypoints.add(quantize_point(path[0], q))\n",
    "        keypoints.add(quantize_point(path[-1], q))\n",
    "    for key, hits in occ.items():\n",
    "        # 命中数量>1，基本可视为交叉/共点（或一个 path 在该点有重复）\n",
    "        if len(hits) > 1:\n",
    "            keypoints.add(key)\n",
    "\n",
    "    # 3) 在关键点处把每条 polyline 切段，生成边\n",
    "    G = nx.MultiGraph()\n",
    "    # 节点位置（用于方向判断）\n",
    "    node_pos = {}  # key(node) -> 原始坐标（选第一次出现的真实像素）\n",
    "    for kp in keypoints:\n",
    "        # 选一个代表性坐标（量化格内实际像素），尽量从 occ 里取真实像素\n",
    "        if kp in occ:\n",
    "            pi, si = occ[kp][0]\n",
    "            node_pos[kp] = tuple(map(int, all_paths[pi][si]))\n",
    "        else:\n",
    "            node_pos[kp] = (int(kp[0]), int(kp[1]))\n",
    "        G.add_node(kp)\n",
    "\n",
    "    for pi, path in enumerate(all_paths):\n",
    "        if path is None or len(path) < 2:\n",
    "            continue\n",
    "        # 找到该 path 上属于关键点的索引\n",
    "        cut_idx = []\n",
    "        for si, pt in enumerate(path):\n",
    "            if quantize_point(pt, q) in keypoints:\n",
    "                cut_idx.append(si)\n",
    "        # 确保首尾在里头\n",
    "        if 0 not in cut_idx:\n",
    "            cut_idx.insert(0, 0)\n",
    "        if (len(path)-1) not in cut_idx:\n",
    "            cut_idx.append(len(path)-1)\n",
    "        # 去重并排序\n",
    "        cut_idx = sorted(set(cut_idx))\n",
    "        # 相邻关键点之间形成一条边\n",
    "        for a, b in zip(cut_idx[:-1], cut_idx[1:]):\n",
    "            if b <= a:\n",
    "                continue\n",
    "            sub = path[a:b+1]\n",
    "            if len(sub) < 2:\n",
    "                continue\n",
    "            u = quantize_point(sub[0], q)\n",
    "            v = quantize_point(sub[-1], q)\n",
    "            w = path_length(sub)\n",
    "            # 可能 u==v（零长度/回折），跳过\n",
    "            if u == v or w == 0:\n",
    "                continue\n",
    "            # 入图，边上存 coords 和 length\n",
    "            G.add_edge(u, v, length=w, coords=sub)\n",
    "\n",
    "    # 把 node 真实坐标也记录上（便于后续方向判断）\n",
    "    nx.set_node_attributes(G, {n: {'pos': node_pos[n]} for n in G.nodes})\n",
    "    return G\n",
    "\n",
    "def oriented_coords(edge_data, start_node_pos):\n",
    "    \"\"\"\n",
    "    根据起点节点位置，返回边的坐标正向/反向（避免连接时倒序）。\n",
    "    edge_data['coords'] 是边的折线；start_node_pos 是当前节点实际坐标\n",
    "    \"\"\"\n",
    "    coords = edge_data['coords']\n",
    "    if len(coords) == 0:\n",
    "        return coords\n",
    "    # 与起点更接近的一端作为开头\n",
    "    d0 = np.linalg.norm(coords[0] - start_node_pos)\n",
    "    d1 = np.linalg.norm(coords[-1] - start_node_pos)\n",
    "    return coords if d0 <= d1 else coords[::-1]\n",
    "\n",
    "def extract_non_branching_chains(G: nx.MultiGraph):\n",
    "    \"\"\"\n",
    "    提取所有“非分叉链”：中间节点度数=2，端点为“度!=2 的节点”或到头。\n",
    "    同时处理纯环（整个连通分量所有节点度数=2）的情况。\n",
    "    返回：list of dict，每个 dict 包含 {'nodes': [...], 'coords': np.ndarray, 'length': float}\n",
    "    \"\"\"\n",
    "    chains = []\n",
    "    visited = set()  # 记录已走过的具体边 (u,v,key)（无向，按排序存）\n",
    "\n",
    "    def mark(u,v,k):  # 无向规范化\n",
    "        return (u, v, k) if u <= v else (v, u, k)\n",
    "\n",
    "    deg = dict(G.degree())\n",
    "    terminals = [n for n,d in deg.items() if d != 2]\n",
    "\n",
    "    # —— 从端点出发的所有链 —— #\n",
    "    for start in terminals:\n",
    "        for _, nbr, key, data in G.edges(start, keys=True, data=True):\n",
    "            m = mark(start, nbr, key)\n",
    "            if m in visited:\n",
    "                continue\n",
    "            # 开始沿着非分叉链走\n",
    "            path_nodes = [start]\n",
    "            path_coords = []\n",
    "            cur, prev = start, None\n",
    "\n",
    "            while True:\n",
    "                # 选择从 cur 出发且未访问的边\n",
    "                next_edge = None\n",
    "                for _, nb, k, d in G.edges(cur, keys=True, data=True):\n",
    "                    mk = mark(cur, nb, k)\n",
    "                    if mk in visited:\n",
    "                        continue\n",
    "                    # 避免立刻折返（如果有其它可选）\n",
    "                    if prev is not None and nb == prev:\n",
    "                        continue\n",
    "                    next_edge = (cur, nb, k, d)\n",
    "                    break\n",
    "                if next_edge is None:\n",
    "                    # 如果没有其它边，就看看能不能把“回退那条”也并上（首步情况）\n",
    "                    for _, nb, k, d in G.edges(cur, keys=True, data=True):\n",
    "                        mk = mark(cur, nb, k)\n",
    "                        if mk not in visited:\n",
    "                            next_edge = (cur, nb, k, d)\n",
    "                            break\n",
    "                if next_edge is None:\n",
    "                    break\n",
    "\n",
    "                u, v, k, d = next_edge\n",
    "                # 方向化坐标并拼接（避免重复第一个点）\n",
    "                start_pos = np.array(G.nodes[u]['pos'])\n",
    "                seg = oriented_coords(d, start_pos)\n",
    "                if len(path_coords) == 0:\n",
    "                    path_coords = seg.copy()\n",
    "                else:\n",
    "                    path_coords = np.vstack([path_coords, seg[1:]])\n",
    "                visited.add(mark(u, v, k))\n",
    "\n",
    "                prev, cur = u, v\n",
    "                path_nodes.append(cur)\n",
    "                # 到达端点（度!=2）就停止\n",
    "                if deg[cur] != 2:\n",
    "                    break\n",
    "\n",
    "            if len(path_coords) >= 2:\n",
    "                chains.append({\n",
    "                    'nodes': path_nodes,\n",
    "                    'coords': path_coords,\n",
    "                    'length': path_length(path_coords)\n",
    "                })\n",
    "\n",
    "    # —— 处理纯环：该连通分量所有节点度数都为2 —— #\n",
    "    # 还有未访问的边说明它们属于环\n",
    "    for u, v, k in list(G.edges(keys=True)):\n",
    "        m = mark(u, v, k)\n",
    "        if m in visited:\n",
    "            continue\n",
    "        # 从 u 出发沿环走到无法继续\n",
    "        path_nodes = [u]\n",
    "        path_coords = []\n",
    "        cur, prev = u, None\n",
    "        while True:\n",
    "            next_edge = None\n",
    "            for _, nb, kk, d in G.edges(cur, keys=True, data=True):\n",
    "                mk = mark(cur, nb, kk)\n",
    "                if mk in visited:\n",
    "                    continue\n",
    "                # 在环里第一次可以任意选边，之后避免直接折返\n",
    "                if prev is not None and nb == prev:\n",
    "                    continue\n",
    "                next_edge = (cur, nb, kk, d)\n",
    "                break\n",
    "            if next_edge is None:\n",
    "                # 没有未访问边了，环走完\n",
    "                break\n",
    "            a, b, kk, d = next_edge\n",
    "            start_pos = np.array(G.nodes[a]['pos'])\n",
    "            seg = oriented_coords(d, start_pos)\n",
    "            if len(path_coords) == 0:\n",
    "                path_coords = seg.copy()\n",
    "            else:\n",
    "                path_coords = np.vstack([path_coords, seg[1:]])\n",
    "            visited.add(mark(a, b, kk))\n",
    "            prev, cur = a, b\n",
    "            path_nodes.append(cur)\n",
    "\n",
    "        if len(path_coords) >= 2:\n",
    "            chains.append({\n",
    "                'nodes': path_nodes,\n",
    "                'coords': path_coords,\n",
    "                'length': path_length(path_coords)\n",
    "            })\n",
    "\n",
    "    return chains\n",
    "\n",
    "def merge_and_find_longest_non_branching(all_paths, quant=1):\n",
    "    \"\"\"\n",
    "    主入口：\n",
    "    1) 在交叉/端点处切段并建图\n",
    "    2) 提取所有非分叉链（含纯环）\n",
    "    3) 返回最长链的坐标和长度\n",
    "    \"\"\"\n",
    "    G = cut_paths_to_edges(all_paths, q=quant)\n",
    "    chains = extract_non_branching_chains(G)\n",
    "    if not chains:\n",
    "        return None, 0.0, chains, G\n",
    "    longest = max(chains, key=lambda c: c['length'])\n",
    "    chain_dict = chains[0]\n",
    "    all_coords = chain_dict['coords']\n",
    "    # 返回所有轨迹\n",
    "    # all_chains = list({key: value['coords'] for key, value in chain_dict.items()}.values())\n",
    "    return longest['coords'], longest['length'], all_coords, G\n",
    "def merge_and_find_non_branching(all_paths, quant=1):\n",
    "    \"\"\"\n",
    "    主入口：\n",
    "    1) 在交叉/端点处切段并建图\n",
    "    2) 提取所有非分叉链（含纯环）\n",
    "    3) 返回所有链的坐标\n",
    "    \"\"\"\n",
    "    G = cut_paths_to_edges(all_paths, q=quant)\n",
    "    chains = extract_non_branching_chains(G)\n",
    "    if not chains:\n",
    "        return None, G\n",
    "    # chains是list，找一个元素，使得其在某个指标下最大，指标为元素长度\n",
    "    # chain的每个元素是一条链的各个信息\n",
    "    chains = sorted(chains, key=lambda c: c['length'], reverse=True)\n",
    "    all_seq_path = [i['coords'] for i in chains]\n",
    "    # 返回所有轨迹\n",
    "    # all_chains = list({key: value['coords'] for key, value in chain_dict.items()}.values())\n",
    "    return all_seq_path, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分相关函数\n",
    "def path_length(path):\n",
    "    \"\"\"计算一条 path 的几何长度\"\"\"\n",
    "    diffs = np.diff(path, axis=0)\n",
    "    return np.sum(np.sqrt((diffs ** 2).sum(axis=1)))\n",
    "\n",
    "def build_graph(paths, tol=1e-6):\n",
    "    \"\"\"把所有 paths 合并成一张图\"\"\"\n",
    "    G = nx.Graph()\n",
    "    node_id = {}\n",
    "\n",
    "    def get_node_id(point):\n",
    "        for nid, coord in node_id.items():\n",
    "            if np.allclose(coord, point, atol=tol):\n",
    "                return nid\n",
    "        nid = len(node_id)\n",
    "        node_id[nid] = point\n",
    "        return nid\n",
    "\n",
    "    for path in paths:\n",
    "        src, dst = path[0], path[-1]\n",
    "        src_id, dst_id = get_node_id(src), get_node_id(dst)\n",
    "        length = path_length(path)\n",
    "        G.add_edge(src_id, dst_id, weight=length, coords=path)\n",
    "\n",
    "    return G, node_id\n",
    "\n",
    "def longest_path(paths):\n",
    "    \"\"\"返回合并后图上的最长路径坐标\"\"\"\n",
    "    G, node_id = build_graph(paths)\n",
    "    degrees = dict(G.degree())\n",
    "    endpoints = [n for n, d in degrees.items() if d == 1]\n",
    "\n",
    "    longest_len = -1\n",
    "    longest_coords = None\n",
    "\n",
    "    for i in range(len(endpoints)):\n",
    "        for j in range(i+1, len(endpoints)):\n",
    "            u, v = endpoints[i], endpoints[j]\n",
    "            for path_nodes in nx.all_simple_paths(G, u, v):\n",
    "                coords = []\n",
    "                total_len = 0\n",
    "                for k in range(len(path_nodes)-1):\n",
    "                    a, b = path_nodes[k], path_nodes[k+1]\n",
    "                    edge = G[a][b]\n",
    "                    coords_edge = edge[\"coords\"]\n",
    "\n",
    "                    # 判断方向\n",
    "                    if np.allclose(coords_edge[0], node_id[a]):\n",
    "                        coords.extend(coords_edge.tolist())\n",
    "                    else:\n",
    "                        coords.extend(coords_edge[::-1].tolist())\n",
    "\n",
    "                    total_len += edge[\"weight\"]\n",
    "\n",
    "                if total_len > longest_len:\n",
    "                    longest_len = total_len\n",
    "                    longest_coords = np.array(coords)\n",
    "\n",
    "    return longest_coords\n",
    "def longest_path_old(paths):\n",
    "    \"\"\"返回合并后图上的最长路径坐标\"\"\"\n",
    "    G, node_id = build_graph(paths)\n",
    "    degrees = dict(G.degree())\n",
    "    endpoints = [n for n, d in degrees.items() if d == 1]\n",
    "\n",
    "    longest_len = -1\n",
    "    longest_coords = None\n",
    "\n",
    "    for i in range(len(endpoints)):\n",
    "        for j in range(i+1, len(endpoints)):\n",
    "            u, v = endpoints[i], endpoints[j]\n",
    "            for path_nodes in nx.all_simple_paths(G, u, v):\n",
    "                coords = []\n",
    "                total_len = 0\n",
    "                for k in range(len(path_nodes)-1):\n",
    "                    edge = G[path_nodes[k]][path_nodes[k+1]]\n",
    "                    coords.extend(edge[\"coords\"].tolist())\n",
    "                    total_len += edge[\"weight\"]\n",
    "                if total_len > longest_len:\n",
    "                    longest_len = total_len\n",
    "                    longest_coords = np.array(coords)\n",
    "\n",
    "    return longest_coords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ca4a0",
   "metadata": {},
   "source": [
    "#### 身体曲率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_skeleton(skeleton, step=1.0):\n",
    "    \"\"\"\n",
    "    把骨架按固定像素间距重采样\n",
    "    skeleton: (N,2) 数组\n",
    "    step: 每隔多少像素取一个点\n",
    "    \"\"\"\n",
    "    skeleton = np.array(skeleton)\n",
    "    if len(skeleton) < 3:\n",
    "        return skeleton\n",
    "    \n",
    "    # 去掉重复点\n",
    "    diffs = np.diff(skeleton, axis=0)\n",
    "    mask = np.any(diffs != 0, axis=1)\n",
    "    skeleton = skeleton[np.r_[True, mask]]\n",
    "    if len(skeleton) < 3:\n",
    "        return skeleton\n",
    "    \n",
    "    x, y = skeleton[:,0], skeleton[:,1]\n",
    "    seglen = np.sqrt(np.diff(x)**2 + np.diff(y)**2)\n",
    "    dist = np.concatenate(([0], np.cumsum(seglen)))   # 单位：像素\n",
    "    total_len = dist[-1]\n",
    "    \n",
    "    # 归一化到 [0,1] 供 splprep 使用\n",
    "    u = dist / total_len\n",
    "    try:\n",
    "        tck, _ = splprep([x, y], s=0, u=u)\n",
    "    except Exception as e:\n",
    "        print(\"splprep error:\", e)\n",
    "        return skeleton\n",
    "    \n",
    "    # 在 [0,total_len] 上等间距采样\n",
    "    new_dist = np.arange(0, total_len, step)\n",
    "    new_u = new_dist / total_len\n",
    "    new_points = np.array(splev(new_u, tck)).T\n",
    "    return new_points\n",
    "\n",
    "def resample_skeleton_2(skeleton, step=1.0):\n",
    "    # 等像素间距采样\n",
    "    total_len = len(skeleton)\n",
    "    if total_len <= step*2:\n",
    "        return []\n",
    "    idx_series = np.arange(0,total_len, step)\n",
    "    selected_points = skeleton[idx_series,:]\n",
    "    return selected_points\n",
    "\n",
    "def curvature_from_angles(skeleton, step=1.0):\n",
    "    \"\"\"\n",
    "    通过角度差计算骨架曲率\n",
    "    返回：\n",
    "        curvatures: 每个点的角度差 (带正负号)\n",
    "        mean_curvature: 曲率强度指标\n",
    "    \"\"\"\n",
    "    pts = resample_skeleton_2(skeleton, step=step)\n",
    "    if not len(pts):\n",
    "        return np.nan\n",
    "\n",
    "    diffs = np.diff(pts, axis=0)\n",
    "    angles = np.arctan2(diffs[:,1], diffs[:,0])\n",
    "    # 解开角度防止跳变\n",
    "    angles_unwrapped = np.unwrap(angles)\n",
    "    # 相邻角度差\n",
    "    dtheta = np.diff(angles_unwrapped)\n",
    "    # # 曲率指标（你可以换成 np.mean(np.abs(dtheta))）\n",
    "    # mean_curvature = np.sqrt(np.mean(dtheta**2))\n",
    "    return np.degrees(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等间距重采样和滑动平均\n",
    "\n",
    "def moving_average(data, window_size=5):\n",
    "    \"\"\"对二维数据做滑动平均\"\"\"\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    x = np.convolve(data[:,0], kernel, mode='same')\n",
    "    y = np.convolve(data[:,1], kernel, mode='same')\n",
    "    return np.vstack((x,y)).T\n",
    "\n",
    "def resample_and_smooth(points, window_size=5):\n",
    "    # --- 等弧长重采样 ---\n",
    "    x, y = points[:,0], points[:,1]\n",
    "    dist = np.sqrt(np.diff(x)**2 + np.diff(y)**2)\n",
    "    s = np.concatenate(([0], np.cumsum(dist)))\n",
    "    total_length = s[-1]\n",
    "    s_uniform = np.linspace(0, total_length, len(points))\n",
    "    fx = interp1d(s, x, kind='linear')\n",
    "    fy = interp1d(s, y, kind='linear')\n",
    "    resampled = np.vstack((fx(s_uniform), fy(s_uniform))).T\n",
    "    \n",
    "    # --- 平滑 ---\n",
    "    smoothed = moving_average(resampled, window_size)\n",
    "    return resampled, smoothed\n",
    "def remove_outliers_by_jump(points, max_step=10):\n",
    "    \"\"\"\n",
    "    去掉瞬间跨度过大的点\n",
    "    points: (N,2) array\n",
    "    max_step: 相邻点最大允许移动距离 (像素)\n",
    "    \"\"\"\n",
    "    x, y = points[:,0], points[:,1]\n",
    "    dist = np.sqrt(np.diff(x)**2 + np.diff(y)**2)\n",
    "    # 第一个点保留\n",
    "    mask = np.ones(len(points), dtype=bool)\n",
    "    \n",
    "    # 如果跨度大于阈值，就把后一帧去掉\n",
    "    mask[1:][dist > max_step] = False\n",
    "    \n",
    "    return points[mask]\n",
    "def resample_and_dthatas(long_paths, step):\n",
    "    # 采样和平均\n",
    "    resampled = {}\n",
    "    # smoothed = {}\n",
    "    for key, skelon in long_paths.items():\n",
    "        if type(skelon) == type(None):\n",
    "            resampled[key] =  []\n",
    "        else:\n",
    "            # 去掉瞬移离群点\n",
    "            skelon = remove_outliers_by_jump(skelon, max_step=10)\n",
    "            resampled[key],_ = resample_and_smooth(skelon, window_size=50)\n",
    "    dtheta = {}\n",
    "    for key, skelon in resampled.items():\n",
    "        if type(None)==type(skelon):\n",
    "            dtheta[key] = None\n",
    "        elif (len(skelon)) < 200:\n",
    "            dtheta[key] = None\n",
    "        else:\n",
    "            dtheta_i = curvature_from_angles(skelon, step=step)\n",
    "            sum_dtheta_i = round(np.sum(dtheta_i),2)\n",
    "            dtheta[key]={'dtheta':dtheta_i,'sum_dtheta':sum_dtheta_i}\n",
    "    return resampled, dtheta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce05923",
   "metadata": {},
   "source": [
    "#### 路径比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5bd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求路径比例和前两条路径长度的df\n",
    "def path_ratio(paths):\n",
    "    ratios = {}\n",
    "    len_dict = {}\n",
    "    for key, value in paths.items():\n",
    "        if len(value) >= 2:\n",
    "            # 如果大于2，求前两个的比例\n",
    "            len_ls = [len(path) for path in value]\n",
    "            len_ls.sort(reverse=True)\n",
    "            ratio = len_ls[0]/len_ls[1]\n",
    "            lens = len_ls[:2]\n",
    "        else:\n",
    "            ratio = np.nan\n",
    "            lens = []\n",
    "        ratios[key] = ratio\n",
    "        len_dict[key] = lens\n",
    "    df_ratio = pd.Series(ratios, name='path_ratio').to_frame()\n",
    "    df_len = pd.Series(len_dict, name='path_len').to_frame()\n",
    "    df = df_ratio.join(df_len, how='left')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0088b3",
   "metadata": {},
   "source": [
    "#### 标记turn主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cor_turn_reversal(df,  after_turn_rev_lim = 50):\n",
    "    '''\n",
    "    df:包含已经平滑后的turn_pc列, 以及'forward'列\n",
    "    return: 包含两列的子df: 'turn_cor', 'forward_subs_turn'\n",
    "    '''\n",
    "    # 首先根据turn将reversal切断，只保留不为turn的reversal段\n",
    "    df.loc[:, 'forward_subs_turn'] = df['forward'].copy()\n",
    "    df.loc[df['turn_pc']==1,'forward_subs_turn'] = 0\n",
    "    df.loc[:,'turn_cor']  = df['turn_pc'].copy()\n",
    "\n",
    "    # 计算所有reversal段的长度 ，如果大于threshold,看前面是否有turn，有的话将这个turn删除\n",
    "    n = len(df)\n",
    "    # 找到连续的 reversal 段\n",
    "    df['turn_group'] = (df[\"turn_cor\"].diff().ne(0)).cumsum() * df[\"turn_cor\"]\n",
    "    # 遍历每个 reversal 区间\n",
    "    for gid, grp in df.groupby('turn_group'):\n",
    "        if gid == 0:\n",
    "            continue\n",
    "        # turn 段的结束位置（label）与位置索引（整数位置）\n",
    "        end_label = grp.index[-1]\n",
    "        end_pos = df.index.get_loc(end_label)\n",
    "\n",
    "        # 寻找紧接着出现 reversal 的位置（允许 small gap）\n",
    "        start_search_pos = end_pos + 1\n",
    "        if start_search_pos >= n:\n",
    "            # 如果数据到头了，舍弃\n",
    "            continue\n",
    "        found_pos = None\n",
    "\n",
    "        for offset in range(after_turn_rev_lim + 1):\n",
    "            p = start_search_pos + offset\n",
    "            if p >= n:\n",
    "                break\n",
    "            if df['forward_subs_turn'].iat[p] == 1:\n",
    "                found_pos = p\n",
    "                break\n",
    "        if found_pos is None:\n",
    "            # 没有紧接的 reversal\n",
    "            continue\n",
    "\n",
    "        # 计数 reversal 连续长度\n",
    "        q = found_pos\n",
    "        while q + 1 < n and df['forward_subs_turn'].iat[q + 1] == 1:\n",
    "            # 遍历reversal的长度，当超过reversal时停\n",
    "            q += 1\n",
    "        rev_len = q - found_pos + 1\n",
    "\n",
    "        if rev_len >= after_turn_rev_lim:\n",
    "            # 仅把这个 turn 段（grp.index）置 0：相当于不算这个turn\n",
    "            df.loc[grp.index, \"turn_cor\"] = 0\n",
    "            # 另外将这一段turn对应的前进状态改为reversal，便于后续分类转向行为\n",
    "            df.loc[grp.index, \"forward_subs_turn\"] = 1 \n",
    "        else:\n",
    "            df.iloc[found_pos:found_pos+rev_len, df.columns.get_loc('forward_subs_turn')] = 0\n",
    "            # 如果这个长度小于阈值，则仍然保留turn，并且把接着的这个reverse变为前进\n",
    "    return df[['turn_cor', 'forward_subs_turn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3751b31",
   "metadata": {},
   "source": [
    "##### 可视化验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求1的开始和结束点\n",
    "def get_turn_interval(df, col_name):\n",
    "    # labels打印矩形\n",
    "    labels = df[col_name].values\n",
    "    n = len(labels)\n",
    "    # 找出连续的 label==1 区间\n",
    "    turn_intervals = []\n",
    "    in_turn = False\n",
    "    start = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if labels[i] == 1 and not in_turn:\n",
    "            # turn开始\n",
    "            start = i\n",
    "            in_turn = True\n",
    "        elif labels[i] != 1 and in_turn:\n",
    "            # turn结束\n",
    "            end = i - 1\n",
    "            turn_intervals.append((start, end))\n",
    "            in_turn = False\n",
    "\n",
    "    # 如果最后一个点也是turn状态\n",
    "    if in_turn:\n",
    "        turn_intervals.append((start, n - 1))\n",
    "    return turn_intervals\n",
    "def visualize_turn_correction(df, close_size = 50, open_size = 50, x_lim = []):\n",
    "    '''\n",
    "    这个函数是可视化turn的矫正(删除reversal内部的)\n",
    "    '''\n",
    "    # 可视化验证\n",
    "    reversal_ints = get_turn_interval(df, 'forward')\n",
    "    subs_reversal_ints = get_turn_interval(df, 'forward_subs_turn')\n",
    "\n",
    "    # Turns: 开闭后的turn\n",
    "    # turn_ints = get_turn_interval(df, 'turn')\n",
    "    turn_pc_ints = get_turn_interval(df, 'turn_pc')\n",
    "    turn_cor_ints = get_turn_interval(df, 'turn_cor')\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        # artificial labeling\n",
    "        label_ints = get_turn_interval(df, 'label')\n",
    "        # 绘图\n",
    "        fig,ax = plt.subplots(5,1,figsize=(20,10), sharex=True)\n",
    "        for start, end in subs_reversal_ints:\n",
    "            ax[-1].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "        for start, end in label_ints:\n",
    "            ax[-1].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "        ax[-1].set_title(f'artificial labels')\n",
    "    else:\n",
    "        fig,ax = plt.subplots(4,1,figsize=(20,10), sharex=True)\n",
    "\n",
    "    # 矫正前自动标记\n",
    "    for start, end in reversal_ints:\n",
    "        ax[0].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "    for start, end in turn_pc_ints:\n",
    "        ax[0].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[0].set_title(f'automated labeling of turn close:{close_size},open:{open_size}')\n",
    "\n",
    "    # 矫正后标记\n",
    "    for start, end in subs_reversal_ints:\n",
    "        ax[1].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "    for start, end in turn_cor_ints:\n",
    "        ax[1].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[1].set_title(f'corrected turn labeling')\n",
    "\n",
    "    # 纯平滑后的reversal\n",
    "    for start, end in subs_reversal_ints:\n",
    "        ax[2].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "    ax[2].set_title(f'reversal_substract_turn')\n",
    "\n",
    "    # 平滑前的reversal\n",
    "    for start, end in reversal_ints:\n",
    "        ax[3].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "    ax[3].set_title(f'reversal')\n",
    "\n",
    "    desired_ticks = np.arange(0, len(df), 2000) # 生成一个数组 [10, 30, 50, 70, 90]\n",
    "    ax[-1].set_xticks(desired_ticks)\n",
    "    if len(x_lim):\n",
    "        ax[-1].set_xlim(*x_lim)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_turn_labeling(df,path_ratio_uplim,dtheta_lim, close_size = 50, open_size = 50, x_lim=[]):\n",
    "    '''\n",
    "    可视化turn的标注条件和标注\n",
    "    '''\n",
    "    # 可视化验证\n",
    "    reversal_ints = get_turn_interval(df, 'forward')\n",
    "    turn_ints = get_turn_interval(df, 'turn')\n",
    "    # 开闭后的turn\n",
    "    turn_pc_ints = get_turn_interval(df, 'turn_pc')\n",
    "    # circular\n",
    "    cir_ints = get_turn_interval(df, 'circular')\n",
    "    # branching\n",
    "    bra_ints = get_turn_interval(df, 'branching')\n",
    "    # path_ratio\n",
    "    df['path_ratio_dis'] = np.where(df['path_ratio'] <= path_ratio_uplim, 1, 0)\n",
    "    ratio_ints = get_turn_interval(df,'path_ratio_dis')\n",
    "    # sum_dtheta\n",
    "    df['sum_dtheta_dis'] = np.where(df['sum_dtheta'].abs() >= dtheta_lim , 1, 0)\n",
    "    dtheta_ints = get_turn_interval(df, \"sum_dtheta_dis\")\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        # artificial labeling\n",
    "        label_ints = get_turn_interval(df, 'label')\n",
    "        # 绘图\n",
    "        fig,ax = plt.subplots(7,1,figsize=(20,10), sharex=True)\n",
    "        for start, end in label_ints:\n",
    "            ax[-1].axvspan(start, end, color='red', alpha=1)  # alpha控制透明度\n",
    "        ax[-1].set_title('artificial labeling of turn')\n",
    "    else:\n",
    "        fig,ax = plt.subplots(6,1,figsize=(20,10), sharex=True)\n",
    "\n",
    "    # 绘制人工打标记\n",
    "\n",
    "    for start, end in turn_ints:\n",
    "        ax[-2].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[-2].set_title('automated labeling of turn')\n",
    "    # for start, end in reversal_ints:\n",
    "    #     ax[1].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "\n",
    "    for start, end in cir_ints:\n",
    "        ax[0].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[0].set_title('circular')\n",
    "    for start, end in bra_ints:\n",
    "        ax[1].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[1].set_title('branching')\n",
    "    for start, end in ratio_ints:\n",
    "        ax[2].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[2].set_title('path_ratio')\n",
    "    for start, end in dtheta_ints:\n",
    "        ax[3].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[3].set_title('sum_dtheta')\n",
    "    desired_ticks = np.arange(0, len(df), 2000) # 生成一个数组 [10, 30, 50, 70, 90]\n",
    "    ax[-1].set_xticks(desired_ticks)\n",
    "    for start, end in reversal_ints:\n",
    "        ax[4].axvspan(start, end, color='blue', alpha=1)  # alpha控制透明度\n",
    "    for start, end in turn_pc_ints:\n",
    "        ax[4].axvspan(start, end, color='orange', alpha=1)  # alpha控制透明度\n",
    "    ax[4].set_title(f'turn_open_closed. close:{close_size},open:{open_size}')\n",
    "    if len(x_lim):\n",
    "        ax[-1].set_xlim(*x_lim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79de44f",
   "metadata": {},
   "source": [
    "#### 头部曲率和角速度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78131e",
   "metadata": {},
   "source": [
    "确定骨架和头朝向函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ed311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数根据咽喉位置决定翻转\n",
    "def process_skeleton(points, pos_phr, n_segments=30):\n",
    "    \"\"\"\n",
    "    points: list of (x, y)\n",
    "    pos_phr: (x, y) 咽喉点\n",
    "    n_segments: 要分的段数\n",
    "    \"\"\"\n",
    "    # 1. 转成 DataFrame\n",
    "    points = np.array(points)\n",
    "    points = resample_skeleton(points,step=1)\n",
    "    n_points = len(points)\n",
    "\n",
    "    # 2. 找最近的点\n",
    "    pos_phr = np.array(pos_phr)\n",
    "    dist = np.linalg.norm(points - pos_phr, axis=1)\n",
    "    idx_phr = np.argmin(dist)\n",
    "    # 4. 比较长度\n",
    "    # len_left = idx_phr\n",
    "    # len_right = len(dist)-idx_phr\n",
    "    # 3. 计算左侧累计长度\n",
    "    len_left = np.sum(\n",
    "        np.linalg.norm(points[1:idx_phr+1] - points[:idx_phr], axis=1)\n",
    "    )\n",
    "    # 4. 计算右侧累计长度\n",
    "    len_right = np.sum(\n",
    "        np.linalg.norm(points[idx_phr+1:] - points[idx_phr:-1], axis=1)\n",
    "    )\n",
    "\n",
    "    # 5. 是否翻转\n",
    "    if len_left > len_right:\n",
    "        points = points[::-1]\n",
    "    return points\n",
    "def select_backbone(paths, closest_idx,pos_phr, longest_paths):\n",
    "    '''\n",
    "    提取最长骨架，确定头朝向\n",
    "    '''\n",
    "    # 提取原最接近咽喉的骨架\n",
    "    closest_path = {}\n",
    "    for key, all_paths in paths.items():   # paths 是 dict 的情况\n",
    "        idx = closest_idx[key]\n",
    "        closest_path[key] = all_paths[idx]\n",
    "    # 选择骨架\n",
    "    sel_paths = {}\n",
    "    for key, longest in longest_paths.items():\n",
    "        closest = closest_path[key]\n",
    "        if type(longest)==type(None):\n",
    "            # 重合并无骨架，选择closest\n",
    "            sel_paths[key] = closest_path[key]\n",
    "            continue\n",
    "        # 计算头尾距离\n",
    "        head = longest[0]\n",
    "        tail = longest[-1]\n",
    "        closest_head = closest[0]\n",
    "        dist_head = (head[0]-closest_head[0])**2 + (head[1]-closest_head[1])**2\n",
    "        dist_tail = (tail[0]-closest_head[0])**2 + (tail[1]-closest_head[1])**2\n",
    "        if len(longest) > len(closest) & (min(dist_tail, dist_head) <= 10):\n",
    "            # 两条轨迹的头是一样的，否则即使longest更长也不要\n",
    "            sel_paths[key] = longest\n",
    "        else:\n",
    "            sel_paths[key] = closest\n",
    "    # 根据咽喉坐标,距离咽喉近的设为头\n",
    "    rev_sel_path = {}\n",
    "    for key, sel in sel_paths.items():\n",
    "        pos_phr_i = pos_phr[key]\n",
    "        re_pos_phr_i = [pos_phr_i[1], pos_phr_i[0]]\n",
    "        rev_sel = process_skeleton(sel, re_pos_phr_i)\n",
    "        rev_sel_path[key] = rev_sel\n",
    "    return rev_sel_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19553a60",
   "metadata": {},
   "source": [
    "计算曲率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(vector1, vector2):\n",
    "    \"\"\"\n",
    "        计算带符号的角度（-180° 到 180°）\n",
    "        正角度表示从 vector1 到 vector2 是逆时针旋转\n",
    "        负角度表示顺时针旋转\n",
    "        \"\"\"\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    cross_product = np.cross(vector1, vector2)  # 在 2D 中，这给出标量值\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    denominator = magnitude1 * magnitude2\n",
    "    if denominator < 1e-10:  # 设置一个很小的阈值\n",
    "        # 处理零向量的情况\n",
    "        cos_theta = 1.0 if np.allclose(vector1, vector2) else np.nan\n",
    "        sin_theta = 0.0\n",
    "    else:\n",
    "        cos_theta = dot_product / (magnitude1 * magnitude2)\n",
    "        sin_theta = cross_product / (magnitude1 * magnitude2)\n",
    "    # 使用 arctan2 获取带符号的角度\n",
    "    angle_rad = np.arctan2(sin_theta, cos_theta)\n",
    "    return np.degrees(angle_rad)\n",
    "\n",
    "def label_segments(points, step=30):\n",
    "    df_bb = pd.DataFrame(points, columns=['X', 'Y'])\n",
    "    seg_len = np.linalg.norm(points[1:] - points[:-1], axis=1)\n",
    "    # 累计距离\n",
    "    cumdist = np.concatenate([[0], np.cumsum(seg_len)])\n",
    "    # 分段编号\n",
    "    seg_id = (cumdist // step).astype(int)\n",
    "    df_bb = df_bb.copy()\n",
    "    df_bb['seg_id'] = seg_id\n",
    "    colors_bgr = [\n",
    "    (255, 0, 0),     # 蓝色\n",
    "    (0, 255, 0),     # 绿色\n",
    "    (0, 0, 255),     # 红色\n",
    "    (0, 255, 255),   # 黄色 (BGR: cyan+red)\n",
    "    (255, 0, 255),   # 洋红 (magenta)\n",
    "    (255, 255, 0)    # 青色 (cyan)\n",
    "    ]\n",
    "    # 建立 seg_id 到颜色的映射\n",
    "    dict_colors = {key: colors_bgr[key % len(colors_bgr)] for key in range(len(colors_bgr))}\n",
    "    df_bb['color'] = df_bb['seg_id'].apply(lambda x: dict_colors[x%6])\n",
    "    return df_bb\n",
    "def label_segments_curvature(points,pos_phr, step=30):\n",
    "    if not len(points):\n",
    "        return None, None, None\n",
    "    df_bb = pd.DataFrame(points, columns=['X', 'Y'])\n",
    "    seg_len = np.linalg.norm(points[1:] - points[:-1], axis=1)\n",
    "    # 累计距离\n",
    "    cumdist = np.concatenate([[0], np.cumsum(seg_len)])\n",
    "    # 分段编号\n",
    "    seg_id = (cumdist // step).astype(int)\n",
    "    df_bb = df_bb.copy()\n",
    "    df_bb['seg_id'] = seg_id\n",
    "\n",
    "    # 分段后根据咽喉所在段求头部曲率\n",
    "    pos_phr = np.array([pos_phr[0],pos_phr[1]])\n",
    "    dist = np.linalg.norm(points - pos_phr, axis=1)\n",
    "    idx_phr = np.argmin(dist)\n",
    "    df_phr = df_bb.iloc[idx_phr]\n",
    "    seg_phr_idx = df_bb.iloc[idx_phr]['seg_id']\n",
    "    # 提取靠后的点\n",
    "    # seg_pos_idx = (seg_phr_idx-2)\n",
    "    df_seg_pos = df_bb[df_bb['seg_id']==(seg_phr_idx+6)]\n",
    "    # 头部顶点\n",
    "    df_start = df_bb.iloc[0]\n",
    "    if not len(df_seg_pos):\n",
    "        curvature = None\n",
    "        pos_phr = None\n",
    "        phr_node = None\n",
    "    else:\n",
    "        df_pos_end = df_seg_pos.iloc[-1]\n",
    "        # 指向向量\n",
    "        pos_phr = [df_phr['X']-df_pos_end['X'], df_phr['Y']-df_pos_end['Y']]\n",
    "        phr_node = [df_start['X']-df_phr['X'], df_start['Y']-df_phr['Y']]\n",
    "        curvature = calc_angle(pos_phr, phr_node)\n",
    "\n",
    "    # 根据前两段求头部指向向量用于计算角速度\n",
    "    df_seg2 = df_bb[df_bb['seg_id']==2]\n",
    "    if not len(df_seg2):\n",
    "        head_vector = [np.nan, np.nan]\n",
    "    else:\n",
    "        seg2_end = df_seg2.iloc[-1]\n",
    "        head_vector = [df_start['X']-seg2_end['X'], df_start['Y']-seg2_end['Y']]\n",
    "\n",
    "    return df_bb, curvature, head_vector, pos_phr, phr_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d449a",
   "metadata": {},
   "source": [
    "计算头部角速度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_head_agl_velocity(df, window_size=300, frame_rate=38):\n",
    "    '''\n",
    "    对向量的角度变化进行滤波计算\n",
    "    '''\n",
    "    # 平滑计算角速度\n",
    "    head_vectors =  df['head_vector']\n",
    "    head_vec_filled = head_vectors.apply(lambda v: v if isinstance(v, (list, np.ndarray)) else [np.nan, np.nan])\n",
    "    head_arr = np.vstack(head_vec_filled)\n",
    "    dy = head_arr[:,0]\n",
    "    dx = head_arr[:,1]\n",
    "    # 假设 dx, dy shape (T,)\n",
    "    angles = np.arctan2(dy,dx)          # radians\n",
    "    angles_interp = pd.Series(angles).interpolate(limit_direction=\"both\").to_numpy()\n",
    "    angles_unwrapped = np.unwrap(angles_interp)\n",
    "\n",
    "    # 采样间隔 dt (s) —— 例：300帧 = 15s -> dt = 15/300 = 0.05\n",
    "    dt = 1 / frame_rate  # = 0.05\n",
    "\n",
    "    angles_unwrapped = np.array(angles_unwrapped, dtype=float)\n",
    "    mask = np.isfinite(angles_unwrapped)\n",
    "    angles_clean = angles_unwrapped[mask]\n",
    "\n",
    "    # 2. 设置合适的窗口\n",
    "    window = min(window_size, len(angles_clean) // 2 * 2 + 1)  # 不超过数据长度的最大奇数\n",
    "    if window < 3: \n",
    "        window = 3  # 最小窗口\n",
    "\n",
    "    # 3. 滤波\n",
    "    angular_velocity = savgol_filter(\n",
    "        angles_clean,\n",
    "        window_length=window,\n",
    "        polyorder=1,\n",
    "        deriv=1,\n",
    "        delta=dt,\n",
    "        mode=\"interp\"\n",
    "    )\n",
    "    # 若需要度/秒：\n",
    "    angular_velocity_deg = np.degrees(angular_velocity)\n",
    "    df['ang_velocity'] = angular_velocity_deg\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582020a9",
   "metadata": {},
   "source": [
    "#### Turn和曲率主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63562f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 输入中线地址，输出包含中线相关参数的df\n",
    "# def get_omega_turn_curvature(mid_line_path, path_ratio_uplim = 1.5, path_uplim=200, dtheta_lim=220,\n",
    "#                               cl_turn_size = 50, op_turn_size = 50, visualize_turn=True,\n",
    "#                               seg_pixel_step = 13, agl_window = 300, frame_rate=38, ):\n",
    "#     '''\n",
    "#     mid_line_path:中线文件路径\n",
    "#     '''\n",
    "#     npz_data = np.load(mid_line_path, allow_pickle=True)\n",
    "#     data = npz_data['arr_0'].item()\n",
    "#     # data_keys = list(data.keys())\n",
    "#     paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "#     closest_idx = {key: value['closest path'] for key, value in data.items()}\n",
    "#     pos_phr = {key: value['pos_phr'] for key, value in data.items()}\n",
    "#     vector_h = {key: value['vector_h'] for key, value in data.items()} \n",
    "#     circular = {key: value['circular'] for key, value in data.items()}\n",
    "#     branching = {key: value['branching'] for key, value in data.items()}\n",
    "\n",
    "#     # 骨架拆分\n",
    "#     seq_paths = {}\n",
    "#     for key, value in paths.items():\n",
    "#         # print(merge_and_find_non_branching(value, quant=3))\n",
    "#         seq_path_i, _ = merge_and_find_non_branching(\n",
    "#             value,\n",
    "#             quant=3   # 若不同段的交点不完全重合，可设 quant=2 或 3 像素，把近点聚成同一节点\n",
    "#         )\n",
    "#         seq_paths[key] = seq_path_i\n",
    "#     # 骨架合并，得到最长骨架\n",
    "#     longest_paths = {}\n",
    "#     for key, value in seq_paths.items():\n",
    "#         longest_coords = longest_path(value)\n",
    "#         longest_paths[key] = longest_coords\n",
    "\n",
    "#     # 对最常骨架重采样计算曲率\n",
    "#     resampled, dtheta = resample_and_dthatas(longest_paths,step=20)\n",
    "#     sum_dtheta = {\n",
    "#         key: (value['sum_dtheta'] if isinstance(value, dict) and 'sum_dtheta' in value else None)\n",
    "#         for key, value in dtheta.items()\n",
    "#         }\n",
    "#     # 曲率计算结果生成df\n",
    "#     df_sum_dtheta = pd.Series(sum_dtheta, name='sum_dtheta')\n",
    "#     print(f'df_sum_dtheta的长度{len(df_sum_dtheta)}和最大index{df_sum_dtheta.index.max()}')\n",
    "#     # 计算路径比例\n",
    "#     df = path_ratio(paths)\n",
    "#     df['sum_dtheta'] = df_sum_dtheta\n",
    "#     # 合并环形和分叉信息\n",
    "#     df_cir_branching = pd.DataFrame.from_dict(\n",
    "#         {\"circular\": circular, \"branching\": branching},\n",
    "#         orient=\"index\").T\n",
    "#     df[[\"circular\", \"branching\"]] = df_cir_branching[[\"circular\", \"branching\"]]\n",
    "\n",
    "#     # 分类turn\n",
    "#     df['path_lim'] = df['path_len'].apply(\n",
    "#         lambda x: 1 if (isinstance(x, (list, tuple, np.ndarray)) and len(x) > 1 and x[1] >= path_uplim) else 0\n",
    "#     )\n",
    "#     df['turn'] = 0\n",
    "#     df.loc[\n",
    "#         (df['circular'] == 1) |\n",
    "#         (df['branching'] == 1) |\n",
    "#         ((df['path_ratio'] <= path_ratio_uplim)&(df['path_lim'] == 1)) |\n",
    "#         (df['sum_dtheta'].abs() >= dtheta_lim)\n",
    "#         ,\n",
    "#         'turn'] = 1\n",
    "#     # 平滑turn\n",
    "#     closed = ndimage.binary_closing(df['turn'].values, structure=np.ones(cl_turn_size))\n",
    "#     # 再进行开操作（先腐蚀后膨胀）：去除小的噪声点\n",
    "#     opened = ndimage.binary_opening(closed, structure=np.ones(op_turn_size))\n",
    "#     # 将结果转换为整数并添加到 DataFrame\n",
    "#     df['turn_pc'] = opened.astype(int)\n",
    "#     df_turn_forw = df[['turn_pc', 'forward']]\n",
    "#     df[['turn_cor', 'forward_subs_turn']] = get_cor_turn_reversal(df_turn_forw)\n",
    "\n",
    "#     if visualize_turn:\n",
    "#         visualize_turn_labeling(df,path_ratio_uplim,dtheta_lim, cl_turn_size, op_turn_size)\n",
    "#         visualize_turn_correction(df, close_size=cl_turn_size, open_size=op_turn_size)\n",
    "    \n",
    "#     # 计算曲率\n",
    "#     rev_sel_path = select_backbone(paths, closest_idx,pos_phr, longest_paths)\n",
    "#     curvature = {}\n",
    "#     head_vector = {}\n",
    "#     post_phr = {}\n",
    "#     phr_noses = {}\n",
    "#     for key, sel in rev_sel_path.items():\n",
    "#         pos_phr_i = pos_phr[key]\n",
    "#         re_pos_phr_i = [pos_phr_i[1], pos_phr_i[0]]\n",
    "#         df_bb_i, curvature_i, head_vector_i, post_phr_i, phr_noses_i = label_segments_curvature(sel,re_pos_phr_i, step=seg_pixel_step)\n",
    "#         curvature[key] = curvature_i\n",
    "#         head_vector[key] = head_vector_i\n",
    "#         post_phr[key] = post_phr_i\n",
    "#         phr_noses[key] = phr_noses_i\n",
    "#     df_cur = pd.Series(curvature, name='curvature')\n",
    "#     df_head_vec = pd.Series(head_vector, name='head_vector')\n",
    "#     df_sel = pd.Series(rev_sel_path, name='sel_paths')\n",
    "#     df['curvature'] = df_cur\n",
    "#     df['head_vector'] = df_head_vec\n",
    "#     df['sel_paths'] = df_sel\n",
    "#     df = cal_head_agl_velocity(df, window_size=agl_window, frame_rate=frame_rate)\n",
    "#     print(df.columns)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入中线地址，和运动参数df\n",
    "# 输出包含中线相关参数（以及时间列‘Time’）的df\n",
    "def get_omega_turn_curvature(df_merged, mid_line_path, path_ratio_uplim = 1.5, path_uplim=200, dtheta_lim=220,\n",
    "                              cl_turn_size = 50, op_turn_size = 50, cl_rev_size = 50, op_rev_size = 50,\n",
    "                                visualize_turn=True, seg_pixel_step = 13, agl_window = 300, frame_rate=38):\n",
    "    '''\n",
    "    处理中线标记前进后退和turn,reversal等加入运动参数df\n",
    "    df_merged: 包含运动数据和其它中线数据(row和choose_frame)\n",
    "    mid_line_path:中线文件路径\n",
    "    '''\n",
    "    npz_data = np.load(mid_line_path, allow_pickle=True)\n",
    "    data = npz_data['arr_0'].item()\n",
    "    # data_keys = list(data.keys())\n",
    "    paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "    closest_idx = {key: value['closest path'] for key, value in data.items()}\n",
    "    pos_phr = {key: value['pos_phr'] for key, value in data.items()}\n",
    "    # angle_m = {key: value['angle_m'] for key, value in data.items()}\n",
    "    vector_h = {key: value['vector_h'] for key, value in data.items()} \n",
    "    circular = {key: value['circular'] for key, value in data.items()}\n",
    "    branching = {key: value['branching'] for key, value in data.items()}\n",
    "\n",
    "    # 前进后退\n",
    "    df_mot_slice = df_merged[['Time','X', 'Y',\"x_velocity\",\"y_velocity\", 'angle_m','angle_md']].copy()\n",
    "    df_mot_slice = label_smth_forward_reverse(df_mot_slice, close_size = cl_rev_size, open_size = op_rev_size)\n",
    "    # 运动方向向量\n",
    "    df_mot_slice.loc[:,'moving_vec'] = df_mot_slice.apply(lambda x: (x['x_velocity'], x['y_velocity']),axis = 1)\n",
    "    # 头部朝向向量\n",
    "    df_mot_slice.loc[:,'heading_vec'] = df_mot_slice.apply(lambda x: rotation_mat_2(x['angle_md'], x['moving_vec']), axis=1)\n",
    "    # 求投影头部朝向后的运动向量\n",
    "    df_mot_slice.loc[:,'head_moving'] = df_mot_slice.apply(lambda x: project_vector_A_on_B(x['moving_vec'], x['heading_vec']), axis=1)\n",
    "\n",
    "    # 骨架拆分\n",
    "    seq_paths = {}\n",
    "    for key, value in paths.items():\n",
    "        # print(merge_and_find_non_branching(value, quant=3))\n",
    "        seq_path_i, _ = merge_and_find_non_branching(\n",
    "            value,\n",
    "            quant=3   # 若不同段的交点不完全重合，可设 quant=2 或 3 像素，把近点聚成同一节点\n",
    "        )\n",
    "        seq_paths[key] = seq_path_i\n",
    "    # 骨架合并，得到最长骨架\n",
    "    longest_paths = {}\n",
    "    for key, value in seq_paths.items():\n",
    "        longest_coords = longest_path(value)\n",
    "        longest_paths[key] = longest_coords\n",
    "\n",
    "    # 对最常骨架重采样计算曲率\n",
    "    resampled, dtheta = resample_and_dthatas(longest_paths,step=20)\n",
    "    sum_dtheta = {\n",
    "        key: (value['sum_dtheta'] if isinstance(value, dict) and 'sum_dtheta' in value else None)\n",
    "        for key, value in dtheta.items()\n",
    "        }\n",
    "    # 曲率计算结果生成df\n",
    "    df_sum_dtheta = pd.Series(sum_dtheta, name='sum_dtheta')\n",
    "    print(f'df_sum_dtheta的长度{len(df_sum_dtheta)}和最大index{df_sum_dtheta.index.max()}')\n",
    "    # 计算路径比例\n",
    "    df = path_ratio(paths)\n",
    "    df.loc[:,'sum_dtheta'] = df_sum_dtheta\n",
    "    # 合并环形和分叉信息\n",
    "    df_cir_branching = pd.DataFrame.from_dict(\n",
    "        {\"circular\": circular, \"branching\": branching},\n",
    "        orient=\"index\").T\n",
    "    df[[\"circular\", \"branching\"]] = df_cir_branching[[\"circular\", \"branching\"]].copy()\n",
    "\n",
    "    # 分类turn\n",
    "    df['path_lim'] = df['path_len'].apply(\n",
    "        lambda x: 1 if (isinstance(x, (list, tuple, np.ndarray)) and len(x) > 1 and x[1] >= path_uplim) else 0\n",
    "    )\n",
    "    df['turn'] = 0\n",
    "    df.loc[\n",
    "        (df['circular'] == 1) |\n",
    "        (df['branching'] == 1) |\n",
    "        ((df['path_ratio'] <= path_ratio_uplim)&(df['path_lim'] == 1)) |\n",
    "        (df['sum_dtheta'].abs() >= dtheta_lim)\n",
    "        ,\n",
    "        'turn'] = 1\n",
    "    # 平滑turn\n",
    "    closed = ndimage.binary_closing(df['turn'].values, structure=np.ones(cl_turn_size))\n",
    "    # 再进行开操作（先腐蚀后膨胀）：去除小的噪声点\n",
    "    opened = ndimage.binary_opening(closed, structure=np.ones(op_turn_size))\n",
    "    # 将结果转换为整数并添加到 DataFrame\n",
    "    df.loc[:,'turn_pc'] = opened.astype(int)\n",
    "\n",
    "\n",
    "    # 合并前进后退信息\n",
    "    df[['Time','forward', 'moving_vec', 'heading_vec', 'head_moving']] = df_mot_slice[['Time', 'forward', 'moving_vec', 'heading_vec', 'head_moving']].copy()\n",
    "    df_turn_forw = df[['turn_pc', 'forward']].copy()\n",
    "    df[['turn_cor', 'forward_subs_turn']] = get_cor_turn_reversal(df_turn_forw)\n",
    "\n",
    "    if visualize_turn:\n",
    "        visualize_turn_labeling(df,path_ratio_uplim,dtheta_lim, cl_turn_size, op_turn_size)\n",
    "        visualize_turn_correction(df, close_size=cl_turn_size, open_size=op_turn_size)\n",
    "\n",
    "    # 计算曲率\n",
    "    rev_sel_path = select_backbone(paths, closest_idx,pos_phr, longest_paths)\n",
    "    curvature = {}\n",
    "    head_vector = {}\n",
    "    post_phr = {}\n",
    "    phr_noses = {}\n",
    "    for key, sel in rev_sel_path.items():\n",
    "        pos_phr_i = pos_phr[key]\n",
    "        re_pos_phr_i = [pos_phr_i[1], pos_phr_i[0]]\n",
    "        df_bb_i, curvature_i, head_vector_i, post_phr_i, phr_noses_i = label_segments_curvature(sel,re_pos_phr_i, step=seg_pixel_step)\n",
    "        curvature[key] = curvature_i\n",
    "        head_vector[key] = head_vector_i\n",
    "        post_phr[key] = post_phr_i\n",
    "        phr_noses[key] = phr_noses_i\n",
    "    df_cur = pd.Series(curvature, name='curvature')\n",
    "    df_head_vec = pd.Series(head_vector, name='head_vector')\n",
    "    df_sel = pd.Series(rev_sel_path, name='sel_paths')\n",
    "    df['curvature'] = df_cur\n",
    "    df['head_vector'] = df_head_vec\n",
    "    df['sel_paths'] = df_sel\n",
    "    df = cal_head_agl_velocity(df, window_size=agl_window, frame_rate=frame_rate)\n",
    "    print('Columns:',df.columns)\n",
    "    return df\n",
    "    # 最后得到的中线df的key和运动相同，并且时间是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13acb14",
   "metadata": {},
   "source": [
    "## 单文件处理函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61cbd1",
   "metadata": {},
   "source": [
    "### 函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contained_or_not(p_f, label_corner = True):\n",
    "    '''\n",
    "    判断文件夹中是否包含所需要的文件\n",
    "    '''\n",
    "    if label_corner:\n",
    "        required_files = ['stage_data.txt', 'c1.txt',\n",
    "                        'upper_left.txt', 'lower_left.txt', 'upper_right.txt',\n",
    "                        'lower_right.txt']   # 数据处理需要的文件\n",
    "    else:   \n",
    "        required_files = ['stage_data.txt', 'c1.txt',\n",
    "                        'edge0.txt', 'edge45.txt']   # 数据处理需要的文件\n",
    "    required_files_set = set(required_files)\n",
    "    files = [f for f in os.listdir(p_f) if '.txt' in f ]\n",
    "    files_set = set(files)\n",
    "    is_contained = required_files_set.issubset(files_set)\n",
    "    # 找出missing_files\n",
    "    missing_files = [file for file in required_files if file not in files]\n",
    "    return is_contained, missing_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choose_frame(data, data_keys, data_iou):\n",
    "    # 步骤零点五：根据骨架处理结果筛选符合条件的帧\n",
    "    circle = np.array(list({key: value['circular'] for key, value in data.items()}.values())) \n",
    "    branching = np.array(list({key: value['branching'] for key, value in data.items()}.values())) \n",
    "    choose = np.array(list({key: value['choose_frame'] for key, value in data.items()}.values())) \n",
    "    circular_frames = np.array(data_keys)[circle == 1]\n",
    "    branching_frames = np.array(data_keys)[branching == 1]\n",
    "    merged = list(heapq.merge(circular_frames, branching_frames))\n",
    "    choose_frame = np.array(data_keys)[choose == 1]\n",
    "    # 识别面积抖动区间\n",
    "    # iou = np.load(os.path.join(folder_path,'iou_results.npz'))['iou']\n",
    "    iou = data_iou[data_iou.files[0]] \n",
    "    ano_f = np.vstack([[1, 1], iou]) < 0.7\n",
    "    pre_f= np.where(ano_f[:,0] == True)[0]\n",
    "    post_f= np.where(ano_f[:,1] == True)[0]\n",
    "    ano_merged = list(heapq.merge(pre_f, post_f))\n",
    "    # 排除IOU异常帧\n",
    "    choose_frame = np.setdiff1d(choose_frame, ano_merged)\n",
    "    return choose_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laser_on_and_off(s_data):\n",
    "    # 提取每个激光照射起始和终止时刻\n",
    "    s_start = s_data[s_data[1] == 2].index\n",
    "    s_stop = s_data[s_data[1] == 3].index\n",
    "    if s_stop.empty:\n",
    "        s_s = [len(s_data[1])]\n",
    "    else:\n",
    "        s_s = []\n",
    "        for m in s_start:\n",
    "            # 如果有多个照射片段，分别找到结束的时间装入s_s\n",
    "            data_1 = (s_stop >= m)\n",
    "            s_s.append(s_stop[data_1 == 1][0])\n",
    "\n",
    "    i=0  # 不考虑其它段，只考虑第一段\n",
    "    #         t_l = int(round((s_data[0][s_start[i] + 1 : s_s[i] - 1]).diff().sum(skipna=True) / (s_s[i] - s_start[i] - 2)))\n",
    "    t_l = (s_data[0][s_start[i] + 1 : s_s[i] - 1]).diff().sum(skipna=True) / (s_s[i] - s_start[i] - 2)\n",
    "    print('时间间隔t_l',t_l)\n",
    "    t = s_data[0].values\n",
    "    # 取从激光照射开始前2帧到激光照射结束的数据\n",
    "    t_frame = np.concatenate([[t[s_start[i]] - t_l *2, t[s_start[i]] - t_l], t[s_start[i]:s_s[i]]])\n",
    "    t_start = s_data[0][s_start[i]] - t_l *2\n",
    "    t_stop = s_data[0][s_s[i] - 1] + t_l\n",
    "    return t_frame, t_start, t_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90327a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对齐时间\n",
    "def get_nearest_value(df1, df2, col_1 = 'Time', col_2 = 'Vol_Time', ind_col = 'Nearest_Index', col1_copy = 'Nearest_Time'):\n",
    "    '''\n",
    "    根据df2的col2找最接近的df1中的col1的值生成新列赋值与df2\n",
    "    '''\n",
    "    nearest_times = []\n",
    "    nearest_indices = []\n",
    "    for vol_time in df2[col_2]:\n",
    "        # 计算时间差的绝对值\n",
    "        differences = np.abs(df1[col_1] - vol_time)\n",
    "        # 找到最小差值的索引\n",
    "        nearest_idx = np.argmin(differences)\n",
    "        # 保存最近时间值和索引\n",
    "        nearest_times.append(df1.iloc[nearest_idx, df1.columns.get_loc(col_1)])\n",
    "        nearest_indices.append(nearest_idx)\n",
    "\n",
    "    # 将结果存入 DataFrame\n",
    "    df2[col1_copy] = nearest_times\n",
    "    df2[ind_col] = nearest_indices\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b403b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入condition\n",
    "def add_condition_info(folder_name, df_matched):\n",
    "    features = folder_name.split('_')\n",
    "    if len(features) <3:\n",
    "        print('文件名异常，未检测到Condition信息')\n",
    "    else:\n",
    "        # 如果Condition信息存在，在df_vol_time中加入condition信息\n",
    "        df_matched['Date'] = features[0]\n",
    "        df_matched['Rep_id'] = features[-1]\n",
    "        condition_ls = features[-2].split('-')\n",
    "        print(f'日期:{features[0]}, 编号：{features[-1]}\\n Conditions:{condition_ls}')\n",
    "        for i, c in enumerate(condition_ls):\n",
    "            if len(condition_ls) > 1:\n",
    "                con_col = 'Condition'+str(i)\n",
    "                df_matched[con_col] = c\n",
    "            else:\n",
    "                df_matched['Condition'] = features[-2]\n",
    "        if len(condition_ls) > 1:\n",
    "            df_matched['Date_Con_Rep'] = features[-2]\n",
    "    return df_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d33c6",
   "metadata": {},
   "source": [
    "### 单文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7095ef",
   "metadata": {},
   "source": [
    "##### 1. 参数定义与数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd704fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_f = r'Y:\\SZX\\\\2025_wbi_analysis\\\\good_WBI\\\\new_midline_done\\\\20250225_0g-ov-24d_004'\n",
    "label_corner = True\n",
    "new_data = True   # 202502之后的数据是新数据，视频帧率为30-40Hz\n",
    "half_width = 20   # 脂片宽度一半\n",
    "x_shift  = 0 \n",
    "hlf_spd_inv = 0.5\n",
    "hlf_agl_spd_inv = 0.2\n",
    "hlf_agl_inv = 1\n",
    "Pos_CTX_vec = [-1,0]\n",
    "t_crit = 3.8\n",
    "min_agl_spd = 75\n",
    "df_motion = {}\n",
    "df_vol_time = {}\n",
    "calcium = True   # 默认有荧光信号的所有文件都包含钙信号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60989de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_contained, missing_files = is_contained_or_not(p_f, label_corner=label_corner)\n",
    "if is_contained:\n",
    "    pass\n",
    "else:\n",
    "    print(f'missing files{missing_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7fd30",
   "metadata": {},
   "source": [
    "##### 2.钙信号数据处理和检查\n",
    "+ 需要对时将钙信号帧转为时间信息方便查看异常帧/时间段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取(在步骤一已读取可省略)\n",
    "basename = os.path.basename(os.path.normpath(p_f))\n",
    "files = [f for f in os.listdir(p_f)]\n",
    "for f in files:\n",
    "    if ('laser-data' in f)or (\"s-data\" in f):\n",
    "        # 读取激光激活数据\n",
    "        print('读取激光照射时间数据并转为s_data')\n",
    "        try:\n",
    "            s_data = pd.read_csv(p_f + '\\laser-data.txt', header=None)  #读取激光激活数据\n",
    "        except FileNotFoundError:\n",
    "            s_data = pd.read_csv(p_f + '\\s-data.txt', header=None)\n",
    "        # 注意：这里s_data已经转换成秒\n",
    "        s_data.loc[:,0] = s_data.loc[:,0]/1000\n",
    "    elif 'calcium_intensity.npy' in f:\n",
    "                # 钙信号数据(* 如果有)\n",
    "                print(f'读取钙信号数据：{p_f+f}')\n",
    "                Cal_data_path = os.path.join(p_f, f)\n",
    "                calcium_intensity= np.load(Cal_data_path)\n",
    "\n",
    "# 根据激光开始和结束时间生成vol_time时间戳\n",
    "t_frame, _, _ = get_laser_on_and_off(s_data)\n",
    "df_vol_time = pd.DataFrame(t_frame, columns=['Vol_Time']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_traces(calcium_intensity,smooth=True, sigma=0.3,delta_y=1.2 ):\n",
    "    # 平滑和可视化\n",
    "    n_neurons, _ = calcium_intensity.shape\n",
    "    t_max = df_vol_time['Vol_Time'].max()\n",
    "    plt.figure(figsize=(t_max/100, n_neurons/4))\n",
    "\n",
    "    for i in range(n_neurons):\n",
    "        trace = calcium_intensity[i, :]\n",
    "        if smooth:\n",
    "            trace = gaussian_filter1d(trace, sigma=sigma)\n",
    "        plt.plot(df_vol_time['Vol_Time'], trace + i * delta_y,lw=0.5, label=f\"Neuron {i}\")\n",
    "\n",
    "    # 设置坐标轴\n",
    "    plt.xlabel(\"Time (frames)\")\n",
    "    plt.ylabel(\"Neuron index\")\n",
    "    desired_ticks = np.arange(0, df_vol_time['Vol_Time'].max(), 30) # 生成一个数组 [10, 30, 50, 70, 90]\n",
    "    plt.xticks(desired_ticks, rotation=30)\n",
    "    plt.yticks(np.arange(0, n_neurons * delta_y, delta_y), np.arange(n_neurons))\n",
    "    plt.title(f\"Calcium Traces (Gaussian smoothed){sigma}\")\n",
    "    plt.grid(True, linestyle='--', color='grey', linewidth=0.35, alpha=0.5)\n",
    "    plt.show()\n",
    "visualize_traces(calcium_intensity,smooth=True, sigma=0.3,delta_y=1.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcium_filter(calcium_traces,std_low = 0.05, std_high = 2, cvs_low=0.1, cvs_high=2):\n",
    "    means = np.mean(calcium_traces, axis=1)\n",
    "    stds = np.std(calcium_traces, axis=1)\n",
    "    cvs = stds / (means + 1e-8)  # 避免除0\n",
    "\n",
    "    # 条件：std 不太小、不太大，CV 也在合理范围\n",
    "    valid_mask = (stds > std_low) & (stds < std_high) & (cvs > cvs_low) & (cvs < cvs_high)\n",
    "    filtered_traces = calcium_traces[valid_mask]\n",
    "    return filtered_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cal = calcium_filter(calcium_intensity,std_low = 0.1, std_high = 0.3, cvs_low=0.5, cvs_high=1.5)\n",
    "visualize_traces(filtered_cal,smooth=True, sigma=0.3,delta_y=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68062769",
   "metadata": {},
   "source": [
    "##### 3. 运动参数计算和视频对时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e24bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "basename = os.path.basename(os.path.normpath(p_f))\n",
    "files = [f for f in os.listdir(p_f)]\n",
    "print(f'\\n\\n=======开始处理{basename}中文件，\\n包含:{files}')\n",
    "for f in files:\n",
    "    if 'stage_data' in f:\n",
    "        # 使用载物台位置信息计算速度角速度\n",
    "        print('读取载物台位置信息文件')\n",
    "        column_names = ['Time','p_X','p_Y', 'X', 'Y']\n",
    "        df_stage = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None,\n",
    "                                names=column_names)\n",
    "        columns_to_clean = ['Time', 'p_X', 'p_Y', 'X', 'Y']\n",
    "        for col in columns_to_clean:\n",
    "            df_stage[col] = df_stage[col].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "        df_motion = df_stage[['Time','X','Y']]/1000  # 转换单位为s和mm\n",
    "        # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "        # 保证p_f中有四个顶点文件\n",
    "        if label_corner:\n",
    "            df_label, df_motion = Realign_coordinate(p_f, df_motion)\n",
    "        else:\n",
    "            df_motion = Realign_coordinate_by_edge(p_f, df_motion)\n",
    "    elif 'c1.txt' in f:\n",
    "        # 读取视频帧文件\n",
    "        print('读取视频帧时间戳文件并转为f_data')\n",
    "        f_data = np.loadtxt(p_f + '\\c1.txt', delimiter=',') # 读取相机时间戳\n",
    "        if new_data: \n",
    "            # 202502之后的数据是新数据，视频帧率为30-40Hz\n",
    "            t_c = f_data[f_data[:,3] == 1, 1]/1000\n",
    "        else:\n",
    "            # 旧数据视频帧率150Hz 每隔5帧取一帧（适用于老数据）\n",
    "            indices = np.concatenate(([0], np.arange(5, len(f_data), 5)))\n",
    "            t_c = f_data[indices, 1].astype(int)/1000\n",
    "    elif ('laser-data' in f)or (\"s-data\" in f):\n",
    "        # 读取激光激活数据\n",
    "        print('读取激光照射时间数据并转为s_data')\n",
    "        try:\n",
    "            s_data = pd.read_csv(p_f + '\\laser-data.txt', header=None)  #读取激光激活数据\n",
    "        except FileNotFoundError:\n",
    "            s_data = pd.read_csv(p_f + '\\s-data.txt', header=None)\n",
    "        # 注意：这里s_data已经转换成秒\n",
    "        s_data.loc[:,0] = s_data.loc[:,0]/1000\n",
    "    # elif 'calcium_intensity.npy' in f:\n",
    "    #     # 钙信号数据(* 如果有)\n",
    "    #     print(f'读取钙信号数据：{f}')\n",
    "    #     Cal_data_path = os.path.join(p_f, f)\n",
    "    #     calcium_intensity= np.load(Cal_data_path)\n",
    "    #     Calcium = True    # 包含钙信号数据\n",
    "    # elif 'output-0516' in f:\n",
    "    #     # 中线npz数据(* 如果有)\n",
    "    #     print(f'读取中线文件：{f}')\n",
    "    #     midline_path = os.path.join(p_f,f)\n",
    "    #     npz_data = np.load(midline_path, allow_pickle=True)\n",
    "    #     data = npz_data['arr_0'].item()\n",
    "    #     # paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "    #     data_keys = list(data.keys())   # 存储的符合条件的帧数\n",
    "    #     midline_extract = True   # 是否包含抽取中线后的文件\n",
    "    # elif 'iou_results' in f:\n",
    "    #     print(f'读取中线处理相关文件{f}')\n",
    "    #     data_iou = np.load(os.path.join(p_f,'iou_results.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤一：根据载物台运动位置信息抽取运动参数\n",
    "mean_frame_rate = np.median(1/np.diff(df_motion.Time.values))\n",
    "print(np.argmin(np.diff(df_motion.Time.values)))\n",
    "df_motion_als = Sliding_CTX_calation(df_motion, Pos_CTX_vec, hlf_spd_inv, hlf_agl_spd_inv, hlf_agl_inv,\n",
    "                                    window_size=100, frame_rate=mean_frame_rate,X_shift=x_shift)\n",
    "\n",
    "\n",
    "# 步骤二： 提取视频帧（根据中线处理结果只保留可用帧）对应的运动参数，输出对齐视频可用帧时间后的运动参数文件\n",
    "df_motion_pt_sorted = df_motion_als.sort_values('Time')  # 确保 df_motion_pt 也是按时间升序\n",
    "df_t = pd.DataFrame({'Time': t_c})\n",
    "df_t = df_t.sort_values('Time')  # merge_asof 要求升序\n",
    "df_motion_pt_sorted = df_motion_pt_sorted.rename(columns={'Time': 'Stage_time'})\n",
    "# 使用 merge_asof 匹配最近的时间点\n",
    "df_matched = pd.merge_asof(\n",
    "    df_t,\n",
    "    \n",
    "    df_motion_pt_sorted,\n",
    "    left_on='Time',\n",
    "    right_on='Stage_time',\n",
    "    direction='nearest'  # 可选：'backward', 'forward', or 'nearest'\n",
    ")\n",
    "\n",
    "# 步骤三：加入condition\n",
    "folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "df_matched = add_condition_info(folder_name, df_matched)\n",
    "# 重置中线位置\n",
    "df_matched['Disp_to_mid'] = df_matched['X']-half_width\n",
    "# 保存合并运动参数和视频帧对应时间戳的df数据\n",
    "df_matched.to_csv(os.path.join(p_f,basename+ '_mot_vid.csv'), index=False)\n",
    "'''文件长度与视频帧数相同'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11198961",
   "metadata": {},
   "source": [
    "##### 2. 中线数据处理和输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取（中线）\n",
    "basename = os.path.basename(os.path.normpath(p_f))\n",
    "files = [f for f in os.listdir(p_f)]\n",
    "print(f'\\n\\n=======开始处理{basename}中文件')\n",
    "for f in files:\n",
    "    if 'output-0516' in f:\n",
    "        # 中线npz数据(* 如果有)\n",
    "        print(f'读取中线文件：{f}')\n",
    "        midline_path = os.path.join(p_f,f)\n",
    "        npz_data = np.load(midline_path, allow_pickle=True)\n",
    "        data = npz_data['arr_0'].item()\n",
    "        # paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "        data_keys = list(data.keys())   # 存储的符合条件的帧数\n",
    "        midline_extract = True   # 是否包含抽取中线后的文件\n",
    "    elif 'iou_results' in f:\n",
    "        print(f'读取中线处理相关文件{f}')\n",
    "        data_iou = np.load(os.path.join(p_f,'iou_results.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''与视频帧数相同的完成对时的运动数据df_matched'''\n",
    "# 这一步可以直接接上一步，或者从文件夹读取'_mot_vid.csv'\n",
    "df_matched = df_matched\n",
    "\n",
    "# 提取可靠帧\n",
    "choose_frame = get_choose_frame(data, data_keys, data_iou)\n",
    "df_choose = pd.DataFrame(choose_frame, columns=['key'])\n",
    "df_choose.loc[:,'choose_frame'] = int(1)\n",
    "df_choose = df_choose.set_index('key')\n",
    "\n",
    "# 提取所需中线信息\n",
    "rows = []\n",
    "for key in data_keys:\n",
    "    item = data[key]\n",
    "    row = {\n",
    "        'key': key,   # 对应视频帧\n",
    "        'angle_m': item['angle_m'],  # 头部朝向与运动方向夹角绝对值\n",
    "        'angle_md': item['angle_md'],  # 头部朝向与运动方向夹角\n",
    "        'all_paths':item['all_paths'],  # 完整的骨架信息\n",
    "        'closest_idx':item['closest path'], # 最近的骨架id\n",
    "    }\n",
    "    rows.append(row)\n",
    "df_summary = pd.DataFrame(rows).set_index('key')\n",
    "\n",
    "\n",
    "# 保留中线数据\n",
    "# 保存筛选帧信息：哪些中线的置信度高于阈值\n",
    "df_merged = df_matched.join(df_summary, how='left')\n",
    "df_merged = df_merged.join(df_choose, how='left')   # 根据index合并，df_choose里没有的填none\n",
    "\n",
    "# 中线处理，计算turn\n",
    "# 输出包含‘Time’作为唯一重合列\n",
    "df_turn_curv = get_omega_turn_curvature(df_merged, midline_path, path_ratio_uplim = 1.5, path_uplim=200, dtheta_lim=220,\n",
    "                            cl_turn_size = 50, op_turn_size = 50, visualize_turn=True,\n",
    "                            seg_pixel_step = 13, agl_window = 300, frame_rate=38)\n",
    "\n",
    "# df_merged_2 = df_merged.join(df_turn_curv, how='left')\n",
    "df_merged_2 = pd.merge(df_merged, df_turn_curv, on='Time', how='left')\n",
    "df_merged_2.to_csv(os.path.join(p_f,basename+ '_mot_vid_mid.csv'), index=False)\n",
    "'''文件仍然与视频长度相同，包含所有中线处理信息以及筛选帧信息'''\n",
    "'''写出df_matched: 运动参数;df_merged_2:包含运动参数和所有中线相关信息'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c956d5",
   "metadata": {},
   "source": [
    "##### *中线处理替代（get_omega_turn_curvature）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入中线地址，和运动参数df\n",
    "# 输出包含中线相关参数（以及时间列‘Time’）的df\n",
    "mid_line_path = midline_path\n",
    "df_merged = df_merged\n",
    "path_ratio_uplim = 1.5\n",
    "path_uplim=200\n",
    "dtheta_lim=220\n",
    "cl_turn_size = 50\n",
    "op_turn_size = 50\n",
    "cl_rev_size = 50\n",
    "op_rev_size = 50\n",
    "visualize_turn=True\n",
    "seg_pixel_step = 13\n",
    "agl_window = 300\n",
    "frame_rate=38\n",
    "'''\n",
    "mid_line_path:中线文件路径\n",
    "'''\n",
    "npz_data = np.load(mid_line_path, allow_pickle=True)\n",
    "data = npz_data['arr_0'].item()\n",
    "# data_keys = list(data.keys())\n",
    "paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "closest_idx = {key: value['closest path'] for key, value in data.items()}\n",
    "pos_phr = {key: value['pos_phr'] for key, value in data.items()}\n",
    "# angle_m = {key: value['angle_m'] for key, value in data.items()}\n",
    "vector_h = {key: value['vector_h'] for key, value in data.items()} \n",
    "circular = {key: value['circular'] for key, value in data.items()}\n",
    "branching = {key: value['branching'] for key, value in data.items()}\n",
    "\n",
    "# 前进后退\n",
    "df_mot_slice = df_merged[['Time','X', 'Y',\"x_velocity\",\"y_velocity\", 'angle_m','angle_md']]\n",
    "df_mot_slice = label_smth_forward_reverse(df_mot_slice, close_size = cl_rev_size, open_size = op_rev_size)\n",
    "# 运动方向向量\n",
    "df_mot_slice.loc[:,'moving_vec'] = df_mot_slice.apply(lambda x: (x['x_velocity'], x['y_velocity']),axis = 1)\n",
    "# 头部朝向向量\n",
    "df_mot_slice.loc[:,'heading_vec'] = df_mot_slice.apply(lambda x: rotation_mat_2(x['angle_md'], x['moving_vec']), axis=1)\n",
    "# 求投影头部朝向后的运动向量\n",
    "df_mot_slice.loc[:,'head_moving'] = df_mot_slice.apply(lambda x: project_vector_A_on_B(x['moving_vec'], x['heading_vec']), axis=1)\n",
    "\n",
    "# 骨架拆分\n",
    "seq_paths = {}\n",
    "for key, value in paths.items():\n",
    "    # print(merge_and_find_non_branching(value, quant=3))\n",
    "    seq_path_i, _ = merge_and_find_non_branching(\n",
    "        value,\n",
    "        quant=3   # 若不同段的交点不完全重合，可设 quant=2 或 3 像素，把近点聚成同一节点\n",
    "    )\n",
    "    seq_paths[key] = seq_path_i\n",
    "# 骨架合并，得到最长骨架\n",
    "longest_paths = {}\n",
    "for key, value in seq_paths.items():\n",
    "    longest_coords = longest_path(value)\n",
    "    longest_paths[key] = longest_coords\n",
    "\n",
    "# 对最常骨架重采样计算曲率\n",
    "resampled, dtheta = resample_and_dthatas(longest_paths,step=20)\n",
    "sum_dtheta = {\n",
    "    key: (value['sum_dtheta'] if isinstance(value, dict) and 'sum_dtheta' in value else None)\n",
    "    for key, value in dtheta.items()\n",
    "    }\n",
    "# 曲率计算结果生成df\n",
    "df_sum_dtheta = pd.Series(sum_dtheta, name='sum_dtheta')\n",
    "print(f'df_sum_dtheta的长度{len(df_sum_dtheta)}和最大index{df_sum_dtheta.index.max()}')\n",
    "# 计算路径比例\n",
    "df = path_ratio(paths)\n",
    "df['sum_dtheta'] = df_sum_dtheta\n",
    "# 合并环形和分叉信息\n",
    "df_cir_branching = pd.DataFrame.from_dict(\n",
    "    {\"circular\": circular, \"branching\": branching},\n",
    "    orient=\"index\").T\n",
    "df[[\"circular\", \"branching\"]] = df_cir_branching[[\"circular\", \"branching\"]]\n",
    "\n",
    "# 分类turn\n",
    "df['path_lim'] = df['path_len'].apply(\n",
    "    lambda x: 1 if (isinstance(x, (list, tuple, np.ndarray)) and len(x) > 1 and x[1] >= path_uplim) else 0\n",
    ")\n",
    "df['turn'] = 0\n",
    "df.loc[\n",
    "    (df['circular'] == 1) |\n",
    "    (df['branching'] == 1) |\n",
    "    ((df['path_ratio'] <= path_ratio_uplim)&(df['path_lim'] == 1)) |\n",
    "    (df['sum_dtheta'].abs() >= dtheta_lim)\n",
    "    ,\n",
    "    'turn'] = 1\n",
    "# 平滑turn\n",
    "closed = ndimage.binary_closing(df['turn'].values, structure=np.ones(cl_turn_size))\n",
    "# 再进行开操作（先腐蚀后膨胀）：去除小的噪声点\n",
    "opened = ndimage.binary_opening(closed, structure=np.ones(op_turn_size))\n",
    "# 将结果转换为整数并添加到 DataFrame\n",
    "df['turn_pc'] = opened.astype(int)\n",
    "\n",
    "\n",
    "# 合并前进后退信息\n",
    "df[['Time','forward', 'moving_vec', 'heading_vec', 'head_moving']] = df_mot_slice[['Time', 'forward', 'moving_vec', 'heading_vec', 'head_moving']]\n",
    "df_turn_forw = df[['turn_pc', 'forward']]\n",
    "df[['turn_cor', 'forward_subs_turn']] = get_cor_turn_reversal(df_turn_forw)\n",
    "\n",
    "if visualize_turn:\n",
    "    visualize_turn_labeling(df,path_ratio_uplim,dtheta_lim, cl_turn_size, op_turn_size)\n",
    "    visualize_turn_correction(df, close_size=cl_turn_size, open_size=op_turn_size)\n",
    "\n",
    "# 计算曲率\n",
    "rev_sel_path = select_backbone(paths, closest_idx,pos_phr, longest_paths)\n",
    "curvature = {}\n",
    "head_vector = {}\n",
    "post_phr = {}\n",
    "phr_noses = {}\n",
    "for key, sel in rev_sel_path.items():\n",
    "    pos_phr_i = pos_phr[key]\n",
    "    re_pos_phr_i = [pos_phr_i[1], pos_phr_i[0]]\n",
    "    df_bb_i, curvature_i, head_vector_i, post_phr_i, phr_noses_i = label_segments_curvature(sel,re_pos_phr_i, step=seg_pixel_step)\n",
    "    curvature[key] = curvature_i\n",
    "    head_vector[key] = head_vector_i\n",
    "    post_phr[key] = post_phr_i\n",
    "    phr_noses[key] = phr_noses_i\n",
    "df_cur = pd.Series(curvature, name='curvature')\n",
    "df_head_vec = pd.Series(head_vector, name='head_vector')\n",
    "df_sel = pd.Series(rev_sel_path, name='sel_paths')\n",
    "df['curvature'] = df_cur\n",
    "df['head_vector'] = df_head_vec\n",
    "df['sel_paths'] = df_sel\n",
    "df = cal_head_agl_velocity(df, window_size=agl_window, frame_rate=frame_rate)\n",
    "print(df.columns)\n",
    "# 最后得到的中线df的key和运动相同，并且时间是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41779efc",
   "metadata": {},
   "source": [
    "##### 4. 中线和运动数据裁剪以及与荧光对时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_df_on_laser_t(df,p_f, basename, t_start, t_stop, ext_name):\n",
    "    '''输出df_cut保存csv文件并返回这个df'''\n",
    "    track_start_index = np.searchsorted(df.Time.values, t_start) - 1   # 截取激光照射段的载物台数据时间戳\n",
    "    track_end_index = np.searchsorted(df.Time.values, t_stop)\n",
    "    df_cut = df.loc[track_start_index:track_end_index, :]\n",
    "    df_cut.to_csv(os.path.join(p_f, basename+ext_name), index = False)\n",
    "    return df_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取(在步骤一已读取可省略)\n",
    "basename = os.path.basename(os.path.normpath(p_f))\n",
    "files = [f for f in os.listdir(p_f)]\n",
    "for f in files:\n",
    "    if ('laser-data' in f)or (\"s-data\" in f):\n",
    "        # 读取激光激活数据\n",
    "        print('读取激光照射时间数据并转为s_data')\n",
    "        try:\n",
    "            s_data = pd.read_csv(p_f + '\\laser-data.txt', header=None)  #读取激光激活数据\n",
    "        except FileNotFoundError:\n",
    "            s_data = pd.read_csv(p_f + '\\s-data.txt', header=None)\n",
    "        # 注意：这里s_data已经转换成秒\n",
    "        s_data.loc[:,0] = s_data.loc[:,0]/1000\n",
    "\n",
    "# 步骤五：根据激光开始和结束时间截取荧光拍摄部分的运动数据/时间戳\n",
    "t_frame, t_start, t_stop = get_laser_on_and_off(s_data)\n",
    "df_matched_cut = cut_df_on_laser_t(df_matched,p_f, basename, t_start, t_stop, ext_name='_mot_vid_cut.csv')\n",
    "df_mot_midline_cut = cut_df_on_laser_t(df_merged_2,p_f, basename, t_start, t_stop, ext_name='_mot_midline_cut.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3d210947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_mot_matched_vol_t(df_mat_cut,p_f, df_vol, ext_name = '', write=True):\n",
    "    '''\n",
    "    荧光对时: 根据df_vol_time中'Time'提取df_mat_cut中的数据列并写出\n",
    "    '''\n",
    "    folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "    df_vol = get_nearest_value(df_mat_cut, df_vol, col_1 = 'Time', col_2 = 'Vol_Time', ind_col = 'Nearest_Index', col1_copy = 'Nearest_Time')\n",
    "    # 将df_motionz中的运动参数(有中线加上中线)按照索引对应到视频帧文件的时间戳上\n",
    "    columns_to_add = df_mat_cut.columns\n",
    "    for col in columns_to_add:\n",
    "        df_vol[col] = df_mat_cut.iloc[df_vol['Nearest_Index'].values][col].values\n",
    "        fn_motion_output = folder_name+ext_name\n",
    "    if write:\n",
    "        df_vol.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "    print(f'写出与神经数据时间戳对应的运动/中线参数文件到:{fn_motion_output}')\n",
    "    return df_vol\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b1aa1262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写出与神经数据时间戳对应的运动/中线参数文件到:20250225_0g-ov-24d_004_MotionMidlineMatchVol.csv\n"
     ]
    }
   ],
   "source": [
    "# 步骤六：根据荧光数据时间戳提取对应的运动参数，便于后续合并荧光数据\n",
    "df_vol_time = pd.DataFrame(t_frame, columns=['Vol_Time'])   # 激光照射时间戳要转换单位为s\n",
    "df_vol_time.to_csv(os.path.join(p_f, folder_name+'_VotTime.csv'), index = False)\n",
    "# 有中线, 如果需要mask，write = False\n",
    "df_mot_mid_vol = get_df_mot_matched_vol_t(df_mot_midline_cut,p_f, df_vol_time.copy(),\n",
    "                                           ext_name = '_MotionMidlineMatchVol.csv', write=True)\n",
    "# 如果没有中线\n",
    "# df_mot_vol = get_df_mot_matched_vol_t(df_matched_cut,p_f, df_vol_time, ext_name = '_MotionMatchVol.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237912e",
   "metadata": {},
   "source": [
    "#### 5. 视频检查\n",
    "视频检查应该放在中线和运动数据处理完后，裁剪前完成\n",
    "视频检查的目的是查看turn_pc, turn_cor, reversal标记正确与否\n",
    "以及查看是否有追错等异常帧，标记这些异常片段\n",
    "检查时可以参考前calcium_trace处理部分记录的可能的异常片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b271e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37080716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(points):\n",
    "    \"\"\"确保骨架点是 (N,2) int32 数组\"\"\"\n",
    "    arr = np.array(points, dtype=np.float32)\n",
    "    arr = np.vstack(arr)                # 去掉长度为1的维度\n",
    "\n",
    "    arr = arr.astype(np.int32)           # OpenCV 要 int32\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f91875e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频总帧数23824是否等于df行数: True\n",
      "不存在骨架:frame 20020\n",
      "不存在骨架:frame 20025\n",
      "不存在骨架:frame 20035\n",
      "不存在骨架:frame 20050\n",
      "不存在骨架:frame 20520\n",
      "不存在骨架:frame 20550\n",
      "不存在骨架:frame 20565\n",
      "不存在骨架:frame 20580\n",
      "不存在骨架:frame 20595\n",
      "不存在骨架:frame 20610\n",
      "不存在骨架:frame 20625\n",
      "不存在骨架:frame 20640\n",
      "不存在骨架:frame 20650\n",
      "不存在骨架:frame 20680\n",
      "不存在骨架:frame 20695\n",
      "不存在骨架:frame 20705\n",
      "不存在骨架:frame 20720\n",
      "不存在骨架:frame 20735\n",
      "不存在骨架:frame 21390\n",
      "不存在骨架:frame 21420\n",
      "不存在骨架:frame 21905\n",
      "不存在骨架:frame 21910\n",
      "不存在骨架:frame 21915\n",
      "不存在骨架:frame 21920\n",
      "不存在骨架:frame 21925\n",
      "不存在骨架:frame 21930\n",
      "不存在骨架:frame 21935\n",
      "不存在骨架:frame 21940\n",
      "不存在骨架:frame 21945\n",
      "不存在骨架:frame 21950\n",
      "不存在骨架:frame 21955\n",
      "不存在骨架:frame 21960\n",
      "不存在骨架:frame 21965\n",
      "不存在骨架:frame 21970\n",
      "不存在骨架:frame 21975\n",
      "不存在骨架:frame 21980\n",
      "不存在骨架:frame 21985\n",
      "不存在骨架:frame 21990\n",
      "不存在骨架:frame 21995\n",
      "不存在骨架:frame 22000\n",
      "不存在骨架:frame 22005\n",
      "不存在骨架:frame 22010\n",
      "不存在骨架:frame 22015\n",
      "不存在骨架:frame 22020\n",
      "不存在骨架:frame 22025\n",
      "不存在骨架:frame 22030\n",
      "不存在骨架:frame 22035\n",
      "不存在骨架:frame 22040\n",
      "不存在骨架:frame 22045\n",
      "不存在骨架:frame 22050\n",
      "不存在骨架:frame 22055\n",
      "不存在骨架:frame 22060\n",
      "不存在骨架:frame 22065\n",
      "不存在骨架:frame 22070\n",
      "不存在骨架:frame 22075\n",
      "不存在骨架:frame 22080\n",
      "不存在骨架:frame 22085\n",
      "不存在骨架:frame 22090\n",
      "不存在骨架:frame 22095\n",
      "不存在骨架:frame 22100\n",
      "不存在骨架:frame 22105\n",
      "不存在骨架:frame 22110\n",
      "不存在骨架:frame 22115\n",
      "不存在骨架:frame 22120\n",
      "不存在骨架:frame 22125\n",
      "不存在骨架:frame 22130\n",
      "不存在骨架:frame 22135\n",
      "不存在骨架:frame 22140\n",
      "不存在骨架:frame 22145\n",
      "不存在骨架:frame 22150\n",
      "不存在骨架:frame 22155\n",
      "不存在骨架:frame 22160\n",
      "不存在骨架:frame 22165\n",
      "不存在骨架:frame 22170\n",
      "不存在骨架:frame 22175\n",
      "不存在骨架:frame 22180\n",
      "不存在骨架:frame 22185\n",
      "不存在骨架:frame 22190\n",
      "不存在骨架:frame 22195\n",
      "不存在骨架:frame 22200\n",
      "不存在骨架:frame 22205\n",
      "不存在骨架:frame 22210\n",
      "不存在骨架:frame 22215\n",
      "不存在骨架:frame 22220\n",
      "不存在骨架:frame 22225\n",
      "不存在骨架:frame 22230\n",
      "不存在骨架:frame 22235\n",
      "不存在骨架:frame 22240\n",
      "不存在骨架:frame 22245\n",
      "不存在骨架:frame 22250\n",
      "不存在骨架:frame 22255\n",
      "不存在骨架:frame 22260\n",
      "不存在骨架:frame 22265\n",
      "不存在骨架:frame 22270\n",
      "不存在骨架:frame 22275\n",
      "不存在骨架:frame 22280\n",
      "不存在骨架:frame 22285\n",
      "不存在骨架:frame 22290\n",
      "不存在骨架:frame 22295\n",
      "不存在骨架:frame 22300\n",
      "不存在骨架:frame 22305\n",
      "不存在骨架:frame 22310\n",
      "不存在骨架:frame 22315\n",
      "不存在骨架:frame 22320\n",
      "不存在骨架:frame 22325\n",
      "不存在骨架:frame 22330\n",
      "不存在骨架:frame 22335\n",
      "不存在骨架:frame 22340\n",
      "不存在骨架:frame 22345\n",
      "不存在骨架:frame 22350\n",
      "不存在骨架:frame 22355\n",
      "不存在骨架:frame 22360\n",
      "不存在骨架:frame 22365\n",
      "不存在骨架:frame 22370\n",
      "不存在骨架:frame 22375\n",
      "不存在骨架:frame 22380\n",
      "不存在骨架:frame 22385\n",
      "不存在骨架:frame 22390\n",
      "不存在骨架:frame 22395\n",
      "不存在骨架:frame 22400\n",
      "不存在骨架:frame 22405\n",
      "不存在骨架:frame 22410\n",
      "不存在骨架:frame 22415\n",
      "不存在骨架:frame 22420\n",
      "不存在骨架:frame 22425\n",
      "不存在骨架:frame 22430\n",
      "不存在骨架:frame 22435\n",
      "不存在骨架:frame 22440\n",
      "不存在骨架:frame 22445\n",
      "不存在骨架:frame 22450\n",
      "不存在骨架:frame 22455\n",
      "不存在骨架:frame 22460\n",
      "不存在骨架:frame 22465\n",
      "不存在骨架:frame 22470\n",
      "不存在骨架:frame 22475\n",
      "不存在骨架:frame 22480\n",
      "不存在骨架:frame 22485\n",
      "不存在骨架:frame 22490\n",
      "不存在骨架:frame 22495\n",
      "不存在骨架:frame 22500\n",
      "不存在骨架:frame 22505\n",
      "不存在骨架:frame 22510\n",
      "不存在骨架:frame 22515\n",
      "不存在骨架:frame 22520\n",
      "不存在骨架:frame 22525\n",
      "不存在骨架:frame 22530\n",
      "不存在骨架:frame 22535\n",
      "不存在骨架:frame 22540\n",
      "不存在骨架:frame 22545\n",
      "不存在骨架:frame 22550\n",
      "不存在骨架:frame 22555\n",
      "不存在骨架:frame 22560\n",
      "不存在骨架:frame 22565\n",
      "不存在骨架:frame 22570\n",
      "不存在骨架:frame 22575\n",
      "不存在骨架:frame 22580\n",
      "不存在骨架:frame 22585\n",
      "不存在骨架:frame 22590\n",
      "不存在骨架:frame 22595\n",
      "不存在骨架:frame 22600\n",
      "不存在骨架:frame 22605\n",
      "不存在骨架:frame 22610\n",
      "不存在骨架:frame 22615\n",
      "不存在骨架:frame 22620\n",
      "不存在骨架:frame 22625\n",
      "不存在骨架:frame 22630\n",
      "不存在骨架:frame 22635\n",
      "不存在骨架:frame 22640\n",
      "不存在骨架:frame 22645\n",
      "不存在骨架:frame 22650\n",
      "不存在骨架:frame 22655\n",
      "不存在骨架:frame 22660\n",
      "不存在骨架:frame 22665\n",
      "不存在骨架:frame 22670\n",
      "不存在骨架:frame 22675\n",
      "不存在骨架:frame 22680\n",
      "不存在骨架:frame 22685\n",
      "不存在骨架:frame 22690\n",
      "不存在骨架:frame 22695\n",
      "不存在骨架:frame 22700\n",
      "不存在骨架:frame 22705\n",
      "不存在骨架:frame 22710\n",
      "不存在骨架:frame 22715\n",
      "不存在骨架:frame 22720\n",
      "不存在骨架:frame 22725\n",
      "不存在骨架:frame 22730\n",
      "不存在骨架:frame 22735\n",
      "不存在骨架:frame 22740\n",
      "不存在骨架:frame 22745\n",
      "不存在骨架:frame 22750\n",
      "不存在骨架:frame 22755\n",
      "不存在骨架:frame 22760\n",
      "不存在骨架:frame 22765\n",
      "不存在骨架:frame 22770\n",
      "不存在骨架:frame 22775\n",
      "不存在骨架:frame 22780\n",
      "不存在骨架:frame 22785\n",
      "不存在骨架:frame 22790\n",
      "不存在骨架:frame 22795\n",
      "不存在骨架:frame 22800\n",
      "不存在骨架:frame 22805\n",
      "不存在骨架:frame 22810\n",
      "不存在骨架:frame 22815\n",
      "不存在骨架:frame 22820\n",
      "不存在骨架:frame 22825\n",
      "不存在骨架:frame 22830\n",
      "不存在骨架:frame 22835\n",
      "不存在骨架:frame 22840\n",
      "不存在骨架:frame 22845\n",
      "不存在骨架:frame 22850\n",
      "不存在骨架:frame 22855\n",
      "不存在骨架:frame 22860\n",
      "不存在骨架:frame 22865\n",
      "不存在骨架:frame 22870\n",
      "不存在骨架:frame 22875\n",
      "不存在骨架:frame 22880\n",
      "不存在骨架:frame 22885\n",
      "不存在骨架:frame 22890\n",
      "不存在骨架:frame 22895\n",
      "不存在骨架:frame 22900\n",
      "不存在骨架:frame 22905\n",
      "不存在骨架:frame 22910\n",
      "不存在骨架:frame 22915\n",
      "不存在骨架:frame 22920\n",
      "不存在骨架:frame 22925\n",
      "不存在骨架:frame 22930\n",
      "不存在骨架:frame 22935\n",
      "不存在骨架:frame 22940\n",
      "不存在骨架:frame 22945\n",
      "不存在骨架:frame 22950\n",
      "不存在骨架:frame 22955\n",
      "不存在骨架:frame 22960\n",
      "不存在骨架:frame 22965\n",
      "不存在骨架:frame 22970\n",
      "不存在骨架:frame 22975\n",
      "不存在骨架:frame 22980\n",
      "不存在骨架:frame 22985\n",
      "不存在骨架:frame 22990\n",
      "不存在骨架:frame 22995\n",
      "不存在骨架:frame 23000\n",
      "不存在骨架:frame 23005\n",
      "不存在骨架:frame 23010\n",
      "不存在骨架:frame 23015\n",
      "不存在骨架:frame 23020\n",
      "不存在骨架:frame 23025\n",
      "不存在骨架:frame 23030\n",
      "不存在骨架:frame 23035\n",
      "不存在骨架:frame 23040\n",
      "不存在骨架:frame 23045\n",
      "不存在骨架:frame 23050\n",
      "不存在骨架:frame 23055\n",
      "不存在骨架:frame 23060\n",
      "不存在骨架:frame 23065\n",
      "不存在骨架:frame 23070\n",
      "不存在骨架:frame 23075\n",
      "不存在骨架:frame 23080\n",
      "不存在骨架:frame 23085\n",
      "不存在骨架:frame 23090\n",
      "不存在骨架:frame 23095\n",
      "不存在骨架:frame 23100\n",
      "不存在骨架:frame 23105\n",
      "不存在骨架:frame 23110\n",
      "不存在骨架:frame 23115\n",
      "不存在骨架:frame 23120\n",
      "不存在骨架:frame 23125\n",
      "不存在骨架:frame 23130\n",
      "不存在骨架:frame 23135\n",
      "不存在骨架:frame 23140\n",
      "不存在骨架:frame 23145\n",
      "不存在骨架:frame 23150\n",
      "不存在骨架:frame 23155\n",
      "不存在骨架:frame 23160\n",
      "不存在骨架:frame 23165\n",
      "不存在骨架:frame 23170\n",
      "不存在骨架:frame 23175\n",
      "不存在骨架:frame 23180\n",
      "不存在骨架:frame 23185\n",
      "不存在骨架:frame 23190\n",
      "不存在骨架:frame 23195\n",
      "不存在骨架:frame 23200\n",
      "不存在骨架:frame 23205\n",
      "不存在骨架:frame 23210\n",
      "不存在骨架:frame 23215\n",
      "不存在骨架:frame 23220\n",
      "不存在骨架:frame 23225\n",
      "不存在骨架:frame 23230\n",
      "不存在骨架:frame 23235\n",
      "不存在骨架:frame 23240\n",
      "不存在骨架:frame 23245\n",
      "不存在骨架:frame 23250\n",
      "不存在骨架:frame 23255\n",
      "不存在骨架:frame 23260\n",
      "不存在骨架:frame 23265\n",
      "不存在骨架:frame 23270\n",
      "不存在骨架:frame 23275\n",
      "不存在骨架:frame 23280\n",
      "不存在骨架:frame 23285\n",
      "不存在骨架:frame 23290\n",
      "不存在骨架:frame 23295\n",
      "不存在骨架:frame 23300\n",
      "不存在骨架:frame 23305\n",
      "不存在骨架:frame 23310\n",
      "不存在骨架:frame 23315\n",
      "不存在骨架:frame 23320\n",
      "不存在骨架:frame 23325\n",
      "不存在骨架:frame 23330\n",
      "不存在骨架:frame 23335\n",
      "不存在骨架:frame 23340\n",
      "不存在骨架:frame 23345\n",
      "不存在骨架:frame 23350\n",
      "不存在骨架:frame 23355\n",
      "不存在骨架:frame 23360\n",
      "不存在骨架:frame 23365\n",
      "不存在骨架:frame 23370\n",
      "不存在骨架:frame 23375\n",
      "不存在骨架:frame 23380\n",
      "不存在骨架:frame 23385\n",
      "不存在骨架:frame 23390\n",
      "不存在骨架:frame 23395\n",
      "不存在骨架:frame 23400\n",
      "不存在骨架:frame 23405\n",
      "不存在骨架:frame 23410\n",
      "不存在骨架:frame 23415\n",
      "不存在骨架:frame 23420\n",
      "不存在骨架:frame 23425\n",
      "不存在骨架:frame 23430\n",
      "不存在骨架:frame 23435\n",
      "不存在骨架:frame 23440\n",
      "不存在骨架:frame 23445\n",
      "不存在骨架:frame 23450\n",
      "不存在骨架:frame 23455\n",
      "不存在骨架:frame 23460\n",
      "不存在骨架:frame 23465\n",
      "不存在骨架:frame 23470\n",
      "不存在骨架:frame 23475\n",
      "不存在骨架:frame 23480\n",
      "不存在骨架:frame 23485\n",
      "不存在骨架:frame 23490\n",
      "不存在骨架:frame 23495\n",
      "不存在骨架:frame 23500\n",
      "不存在骨架:frame 23505\n",
      "不存在骨架:frame 23510\n",
      "不存在骨架:frame 23515\n",
      "不存在骨架:frame 23520\n",
      "不存在骨架:frame 23525\n",
      "不存在骨架:frame 23530\n",
      "不存在骨架:frame 23535\n",
      "不存在骨架:frame 23540\n",
      "不存在骨架:frame 23545\n",
      "不存在骨架:frame 23550\n",
      "不存在骨架:frame 23555\n",
      "不存在骨架:frame 23560\n",
      "不存在骨架:frame 23565\n",
      "不存在骨架:frame 23570\n",
      "不存在骨架:frame 23575\n",
      "不存在骨架:frame 23580\n",
      "不存在骨架:frame 23585\n",
      "不存在骨架:frame 23590\n",
      "不存在骨架:frame 23595\n",
      "不存在骨架:frame 23600\n",
      "不存在骨架:frame 23605\n",
      "不存在骨架:frame 23610\n",
      "不存在骨架:frame 23615\n",
      "不存在骨架:frame 23620\n",
      "不存在骨架:frame 23625\n",
      "不存在骨架:frame 23630\n",
      "不存在骨架:frame 23635\n",
      "不存在骨架:frame 23640\n",
      "不存在骨架:frame 23645\n",
      "不存在骨架:frame 23650\n",
      "不存在骨架:frame 23655\n",
      "不存在骨架:frame 23660\n",
      "不存在骨架:frame 23665\n",
      "不存在骨架:frame 23670\n",
      "不存在骨架:frame 23675\n",
      "不存在骨架:frame 23680\n",
      "不存在骨架:frame 23685\n",
      "不存在骨架:frame 23690\n",
      "不存在骨架:frame 23695\n",
      "不存在骨架:frame 23700\n",
      "不存在骨架:frame 23705\n",
      "不存在骨架:frame 23710\n",
      "不存在骨架:frame 23715\n",
      "不存在骨架:frame 23720\n",
      "不存在骨架:frame 23725\n",
      "不存在骨架:frame 23730\n",
      "不存在骨架:frame 23735\n",
      "不存在骨架:frame 23740\n",
      "不存在骨架:frame 23745\n",
      "不存在骨架:frame 23750\n",
      "不存在骨架:frame 23755\n",
      "不存在骨架:frame 23760\n",
      "不存在骨架:frame 23765\n",
      "不存在骨架:frame 23770\n",
      "不存在骨架:frame 23775\n",
      "不存在骨架:frame 23780\n",
      "不存在骨架:frame 23785\n",
      "不存在骨架:frame 23790\n",
      "不存在骨架:frame 23795\n",
      "不存在骨架:frame 23800\n",
      "不存在骨架:frame 23805\n",
      "不存在骨架:frame 23810\n",
      "不存在骨架:frame 23815\n",
      "不存在骨架:frame 23820\n"
     ]
    }
   ],
   "source": [
    "# 读取视频\n",
    "cap = cv2.VideoCapture(os.path.join(p_f, \"c1_onnx.mp4\"))\n",
    "# cap = cv2.VideoCapture(os.path.join(folder_path, \"c1_onnx.mp4\"))\n",
    "df = df_merged_2  \n",
    "# 或者从文件夹中读取，行数与视频帧数相同\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"视频总帧数{total_frames}是否等于df行数:\", total_frames==len(df))\n",
    "# 计算头部朝向时的切割像素长度\n",
    "body_l = 200\n",
    "delay = 50\n",
    "step = 5\n",
    "start_frame = 20000\n",
    "# 结束帧\n",
    "frame_idx_end = total_frames\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "error_ls = []\n",
    "\n",
    "for frame_idx in range(start_frame, total_frames, step):\n",
    "    # 视频定位当前帧\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)  # 定位到 frame_idx\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        continue\n",
    "    if not ret:\n",
    "        break\n",
    "    # 显示帧索引\n",
    "    cv2.putText(frame, f\"Frame: {frame_idx}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    # 画出向量\n",
    "    color_vector_h = (128, 0, 128)  # 紫色\n",
    "    color_vector_m = (0, 165, 255)    # 明黄色\n",
    "    color_vector_mh= (216, 191, 216) # 淡紫色\n",
    "    \n",
    "    row = df.iloc[frame_idx]\n",
    "    # 时间\n",
    "    time_i = row.Time\n",
    "    cv2.putText(frame, f\"Time:{time_i}s\", (10, 50), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    # 读取咽喉点坐标\n",
    "    angle_md = round(row.angle_md,2)\n",
    "    cv2.putText(frame, f\"angle_md:{angle_md}\", (250, 30), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    \n",
    "    # 骨架\n",
    "    sel_path = row.sel_paths\n",
    "   \n",
    "    # 校正turn\n",
    "    if row.turn_cor == 1:\n",
    "        cv2.putText(frame, f\"turn_cor\", (250, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, f\"turn_cor\", (250, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (200, 200, 200), 2)\n",
    "    # 后退\n",
    "    if row.forward == 1:\n",
    "        cv2.putText(frame, f\"reversal\", (250, 70), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, f\"forward\", (250, 70), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (200, 200, 200), 2)\n",
    "    if row.turn_pc == 1:\n",
    "        color_bb = (0,0,255)\n",
    "    else:\n",
    "        color_bb = color_vector_m\n",
    "    if isinstance(sel_path, np.ndarray):\n",
    "        points_raw = sel_path\n",
    "        midline_points = normalize_points(points_raw)  # 返回 (N,2) int32\n",
    "        j = 0\n",
    "        for (x, y) in midline_points:\n",
    "            # 打印第一帧\n",
    "            if j == 0:\n",
    "                cv2.circle(frame, (y,x), 10, (255,0,255), thickness=-1)\n",
    "            cv2.circle(frame, (y,x), 3, color_bb, -1)\n",
    "            j += 1\n",
    "        i += 1\n",
    "    else:\n",
    "        print('不存在骨架:frame',frame_idx)\n",
    "        continue\n",
    "        \n",
    "    cv2.imshow(\"Skeleton Check\", frame)\n",
    "    key = cv2.waitKey(delay) & 0xFF   # 播放速度 (30ms/帧)，也可以改大\n",
    "    if key == ord('q'):   # 按 q 退出\n",
    "        break\n",
    "    elif key == ord(' '): # 按空格暂停\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21339f",
   "metadata": {},
   "source": [
    "#### 6. 对异常数据mask并打印钙信号检查\n",
    "根据可能的追踪异常情况，将4中得到的荧光对时文件打mask，提示异常帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49985cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "df_vol_time.loc[:,'mask'] = 0 \n",
    "df_vol_time.loc[df_vol_time['Vol_Time']>= 685,'mask'] = 1\n",
    "df_vol_time.to_csv(os.path.join(p_f, folder_name+'_VotTime.csv'), index = False)\n",
    "\n",
    "# 有中线的vol_time对齐文件\n",
    "df_mot_mid_vol.loc[:,'mask'] = 0 \n",
    "df_mot_mid_vol.loc[df_mot_mid_vol['Vol_Time']>= 685,'mask'] = 1\n",
    "df_mot_mid_vol.to_csv(os.path.join(p_f, folder_name+'_MotionMidlineMatchVol.csv'), index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06832367",
   "metadata": {},
   "source": [
    "### 单文件处理函数（旧）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc476d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_f = r'Y:\\SZX\\\\2025_wbi_analysis\\\\good_WBI\\\\new_midline_done\\\\20250221_0g-ov-27.5d_6'\n",
    "def single_wbi_data_analysis(p_f, label_corner = True, new_data = True, half_width = 20, x_shift  = 0,\n",
    "                              hlf_spd_inv = 0.5, hlf_agl_spd_inv = 0.2, hlf_agl_inv = 1,\n",
    "                                Pos_CTX_vec = [-1,0]):\n",
    "    \n",
    "    # 检查文件\n",
    "    is_contained, missing_files = is_contained_or_not(p_f, label_corner=label_corner)\n",
    "    if is_contained:\n",
    "        pass\n",
    "    else:\n",
    "        print(f'missing files{missing_files}')\n",
    "        return None\n",
    "\n",
    "    # 数据读取\n",
    "    basename = os.path.basename(os.path.normpath(p_f))\n",
    "    files = [f for f in os.listdir(p_f)]\n",
    "    print(f'\\n\\n=======开始处理{basename}中文件，\\n包含:{files}')\n",
    "    for f in files:\n",
    "        if 'stage_data' in f:\n",
    "            # 使用载物台位置信息计算速度角速度\n",
    "            print('读取载物台位置信息文件')\n",
    "            column_names = ['Time','p_X','p_Y', 'X', 'Y']\n",
    "            df_stage = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None,\n",
    "                                    names=column_names)\n",
    "            columns_to_clean = ['Time', 'p_X', 'p_Y', 'X', 'Y']\n",
    "            for col in columns_to_clean:\n",
    "                df_stage[col] = df_stage[col].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "            df_motion = df_stage[['Time','X','Y']]/1000  # 转换单位为s和mm\n",
    "            # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "            # 保证p_f中有四个顶点文件\n",
    "            if label_corner:\n",
    "                df_label, df_motion = Realign_coordinate(p_f, df_motion)\n",
    "            else:\n",
    "                df_motion = Realign_coordinate_by_edge(p_f, df_motion)\n",
    "        elif 'c1.txt' in f:\n",
    "            # 读取视频帧文件\n",
    "            print('读取视频帧时间戳文件并转为f_data')\n",
    "            f_data = np.loadtxt(p_f + '\\c1.txt', delimiter=',') # 读取相机时间戳\n",
    "            if new_data: \n",
    "                # 202502之后的数据是新数据，视频帧率为30-40Hz\n",
    "                t_c = f_data[f_data[:,3] == 1, 1]/1000\n",
    "            else:\n",
    "                # 旧数据视频帧率150Hz 每隔5帧取一帧（适用于老数据）\n",
    "                indices = np.concatenate(([0], np.arange(5, len(f_data), 5)))\n",
    "                t_c = f_data[indices, 1].astype(int)/1000\n",
    "        elif ('laser-data' in f)or (\"s-data\" in f): \n",
    "            # 读取激光激活数据\n",
    "            print('读取激光照射时间数据并转为s_data')\n",
    "            try:\n",
    "                s_data = pd.read_csv(p_f + '\\laser-data.txt', header=None)  #读取激光激活数据\n",
    "            except FileNotFoundError:\n",
    "                s_data = pd.read_csv(p_f + '\\s-data.txt', header=None)\n",
    "            # 注意：这里s_data已经转换成秒\n",
    "            s_data.loc[:,0] = s_data.loc[:,0]/1000\n",
    "        elif 'output-0516' in f:\n",
    "            # 中线npz数据(* 如果有)\n",
    "            print(f'读取中线文件：{f}')\n",
    "            midline_path = os.path.join(p_f,f)\n",
    "            npz_data = np.load(midline_path, allow_pickle=True)\n",
    "            data = npz_data['arr_0'].item()\n",
    "            # paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "            data_keys = list(data.keys())   # 存储的符合条件的帧数\n",
    "            midline_extract = True   # 是否包含抽取中线后的文件\n",
    "        elif 'iou_results' in f:\n",
    "            print(f'读取中线处理相关文件{f}')\n",
    "            data_iou = np.load(os.path.join(p_f,'iou_results.npz'))\n",
    "        \n",
    "            \n",
    "    # 步骤一：根据载物台运动位置信息抽取运动参数\n",
    "    mean_frame_rate = np.median(1/np.diff(df_motion.Time.values))\n",
    "    print(np.argmin(np.diff(df_motion.Time.values)))\n",
    "    df_motion_als = Sliding_CTX_calation(df_motion, Pos_CTX_vec, hlf_spd_inv, hlf_agl_spd_inv, hlf_agl_inv,\n",
    "                                        window_size=100, frame_rate=mean_frame_rate,X_shift=x_shift)\n",
    "\n",
    "\n",
    "    # 步骤二： 提取视频帧（根据中线处理结果只保留可用帧）对应的运动参数，输出对齐视频可用帧时间后的运动参数文件\n",
    "    df_motion_pt_sorted = df_motion_als.sort_values('Time')  # 确保 df_motion_pt 也是按时间升序\n",
    "    df_t = pd.DataFrame({'Time': t_c})\n",
    "    df_t = df_t.sort_values('Time')  # merge_asof 要求升序\n",
    "    df_motion_pt_sorted = df_motion_pt_sorted.rename(columns={'Time': 'Stage_time'})\n",
    "    # 使用 merge_asof 匹配最近的时间点\n",
    "    df_matched = pd.merge_asof(\n",
    "        df_t,\n",
    "        df_motion_pt_sorted,\n",
    "        left_on='Time',\n",
    "        right_on='Stage_time',\n",
    "        direction='nearest'  # 可选：'backward', 'forward', or 'nearest'\n",
    "    )\n",
    "\n",
    "    # 步骤三：加入condition\n",
    "    folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "    df_matched = add_condition_info(folder_name, df_matched)\n",
    "    # 重置中线位置\n",
    "    df_matched['Disp_to_mid'] = df_matched['X']-half_width\n",
    "    # 保存合并运动参数和视频帧对应时间戳的df数据\n",
    "    df_matched.to_csv(os.path.join(p_f,basename+ '_mot_vid.csv'), index=False)\n",
    "    '''文件长度与视频帧数相同'''\n",
    "\n",
    "    # 步骤四:合并中线数据\n",
    "    if midline_extract:\n",
    "\n",
    "        # 整理choose帧\n",
    "        choose_frame = get_choose_frame(data, data_keys, data_iou)\n",
    "        df_choose = pd.DataFrame(choose_frame, columns=['key'])\n",
    "        df_choose.loc[:,'choose_frame'] = int(1)\n",
    "        df_choose = df_choose.set_index('key')\n",
    "\n",
    "        rows = []\n",
    "        for key in data_keys:\n",
    "            item = data[key]\n",
    "            row = {\n",
    "                'key': key,   # 对应视频帧\n",
    "                'angle_m': item['angle_m'],  # 头部朝向与运动方向夹角绝对值\n",
    "                'angle_md': item['angle_md'],  # 头部朝向与运动方向夹角\n",
    "                'all_paths':item['all_paths'],  # 完整的骨架信息\n",
    "                'closest_idx':item['closest path'], # 最近的骨架id\n",
    "            }\n",
    "            rows.append(row)\n",
    "        # 构建 DataFrame\n",
    "        df_summary = pd.DataFrame(rows).set_index('key')\n",
    "        # 保留所有的处理后的帧信息\n",
    "        df_merged = df_matched.join(df_summary, how='left')\n",
    "\n",
    "        # 保存筛选帧信息：哪些中线的置信度高于阈值\n",
    "        df_merged = df_merged.join(df_choose, how='left')   # 根据index合并，df_choose里没有的填none\n",
    "\n",
    "        # 中线处理，计算turn\n",
    "        df_turn_curv = get_omega_turn_curvature(midline_path, path_ratio_uplim = 1.5, path_uplim=200, dtheta_lim=220,\n",
    "                                cl_turn_size = 50, op_turn_size = 50, visualize_turn=True,\n",
    "                                seg_pixel_step = 13, agl_window = 300, frame_rate=38)\n",
    "        df_merged_2 = df_merged.join(df_turn_curv, how='left')\n",
    "\n",
    "        df_merged_2.to_csv(os.path.join(p_f,basename+ '_mot_vid_mid.csv'), index=False)\n",
    "        '''文件仍然与视频长度相同，包含所有中线处理信息以及筛选帧信息'''\n",
    "\n",
    "    # 步骤五：根据激光开始和结束时间截取荧光拍摄部分的运动数据/时间戳\n",
    "    t_frame, t_start, t_stop = get_laser_on_and_off(s_data)\n",
    "    track_start_index = np.searchsorted(df_matched.Time.values, t_start) - 1   # 截取激光照射段的载物台数据时间戳\n",
    "    track_end_index = np.searchsorted(df_matched.Time.values, t_stop)\n",
    "    df_matched_cut = df_matched.loc[track_start_index:track_end_index, :]\n",
    "    df_vol_time = pd.DataFrame(t_frame, columns=['Vol_Time'])   # 激光照射时间戳要转换单位为s\n",
    "    stage_file = os.path.join(p_f, f's{0+1}.txt')\n",
    "    vol_file = os.path.join(p_f, f't{0+1}.txt')\n",
    "    np.savetxt(stage_file, df_matched_cut[['Time','X_org','Y_org']], fmt='%.4f')  # 存储载物台数据(时间戳是视频帧的)\n",
    "    np.savetxt(vol_file, t_frame, fmt='%.4f')  # 存储荧光每一个volume的数据\n",
    "    df_matched_cut.to_csv(os.path.join(p_f, basename+'_mot_vid_cut.csv'), index = False)\n",
    "    if midline_extract:\n",
    "        df_mot_midline_cut = df_merged_2.loc[track_start_index:track_end_index, :]\n",
    "        df_mot_midline_cut.to_csv(os.path.join(p_f, basename+'_mot_midline_cut.csv'), index = False)\n",
    "\n",
    "    # 步骤六：根据荧光数据时间戳提取对应的运动参数，便于后续合并荧光数据\n",
    "    # 如果有中线数据，就用带中线数据的对齐。否则只用运动参数对齐\n",
    "    if midline_extract:\n",
    "        df_mat_cut = df_mot_midline_cut.copy()\n",
    "        print('有中线数据, 与Volume对齐的运动和中线数据')\n",
    "    else:\n",
    "        df_mat_cut = df_matched_cut.copy()\n",
    "        print('没有中线数据, 输出与Volume时间对齐的运动参数')\n",
    "\n",
    "    df_vol_time = get_nearest_value(df_mat_cut, df_vol_time, col_1 = 'Time', col_2 = 'Vol_Time', ind_col = 'Nearest_Index', col1_copy = 'Nearest_Time')\n",
    "    # 将df_motionz中的运动参数(有中线加上中线)按照索引对应到视频帧文件的时间戳上\n",
    "    columns_to_add = df_mat_cut.columns\n",
    "    for col in columns_to_add:\n",
    "        df_vol_time[col] = df_mat_cut.iloc[df_vol_time['Nearest_Index'].values][col].values\n",
    "    if midline_extract:\n",
    "        fn_motion_output = folder_name+'_MotionMidlineMatchVol.csv'\n",
    "    else:\n",
    "        fn_motion_output = folder_name+'_MotionMatchVol.csv'\n",
    "    df_vol_time.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "    print(f'写出与神经数据时间戳(不包含钙数据)对应的运动参数文件到:{folder_name}/{fn_motion_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数调用\n",
    "file_path_all = r'Y:\\\\SZX\\\\2025_wbi_analysis\\\\good_WBI\\\\no_binary\\\\new'\n",
    "key_word = ''\n",
    "nokey_word = '*'\n",
    "file_paths = [os.path.join(file_path_all,f_p) for f_p in os.listdir(file_path_all) if ('trash' not in f_p)&\n",
    "              ('done' not in f_p)&\n",
    "             os.path.isdir(os.path.join(file_path_all,f_p))&\n",
    "             (key_word in f_p)&(nokey_word not in f_p)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc6623",
   "metadata": {},
   "source": [
    "#### 单文件处理，旧代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1211c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 单文件处理，旧代码\n",
    "p_f = r'Y:\\SZX\\\\2025_wbi_analysis\\\\good_WBI\\\\new_midline_done\\\\20250221_0g-ov-27.5d_6'\n",
    "label_corner = True\n",
    "new_data = True   # 202502之后的数据是新数据，视频帧率为30-40Hz\n",
    "half_width = 20   # 脂片宽度一半\n",
    "x_shift  = 0 \n",
    "hlf_spd_inv = 0.5\n",
    "hlf_agl_spd_inv = 0.2\n",
    "hlf_agl_inv = 1\n",
    "Pos_CTX_vec = [-1,0]\n",
    "t_crit = 3.8\n",
    "min_agl_spd = 75\n",
    "df_motion = {}\n",
    "df_vol_time = {}\n",
    "calcium = True   # 默认有荧光信号的所有文件都包含钙信号\n",
    "\n",
    "is_contained, missing_files = is_contained_or_not(p_f, label_corner=label_corner)\n",
    "if is_contained:\n",
    "    pass\n",
    "else:\n",
    "    print(f'missing files{missing_files}')\n",
    "\n",
    "\n",
    "# 数据读取\n",
    "basename = os.path.basename(os.path.normpath(p_f))\n",
    "files = [f for f in os.listdir(p_f)]\n",
    "print(f'\\n\\n=======开始处理{basename}中文件，\\n包含:{files}')\n",
    "for f in files:\n",
    "    if 'stage_data' in f:\n",
    "        # 使用载物台位置信息计算速度角速度\n",
    "        print('读取载物台位置信息文件')\n",
    "        column_names = ['Time','p_X','p_Y', 'X', 'Y']\n",
    "        df_stage = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None,\n",
    "                                names=column_names)\n",
    "        columns_to_clean = ['Time', 'p_X', 'p_Y', 'X', 'Y']\n",
    "        for col in columns_to_clean:\n",
    "            df_stage[col] = df_stage[col].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "        df_motion = df_stage[['Time','X','Y']]/1000  # 转换单位为s和mm\n",
    "        # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "        # 保证p_f中有四个顶点文件\n",
    "        if label_corner:\n",
    "            df_label, df_motion = Realign_coordinate(p_f, df_motion)\n",
    "        else:\n",
    "            df_motion = Realign_coordinate_by_edge(p_f, df_motion)\n",
    "\n",
    "    elif 'c1.txt' in f:\n",
    "        # 读取视频帧文件\n",
    "        print('读取视频帧时间戳文件并转为f_data')\n",
    "        f_data = np.loadtxt(p_f + '\\c1.txt', delimiter=',') # 读取相机时间戳\n",
    "        if new_data: \n",
    "            # 202502之后的数据是新数据，视频帧率为30-40Hz\n",
    "            t_c = f_data[f_data[:,3] == 1, 1]/1000\n",
    "        else:\n",
    "            # 旧数据视频帧率150Hz 每隔5帧取一帧（适用于老数据）\n",
    "            indices = np.concatenate(([0], np.arange(5, len(f_data), 5)))\n",
    "            t_c = f_data[indices, 1].astype(int)/1000\n",
    "    elif ('laser-data' in f)or (\"s-data\" in f):\n",
    "        # 读取激光激活数据\n",
    "        print('读取激光照射时间数据并转为s_data')\n",
    "        try:\n",
    "            s_data = pd.read_csv(p_f + '\\laser-data.txt', header=None)  #读取激光激活数据\n",
    "        except FileNotFoundError:\n",
    "            s_data = pd.read_csv(p_f + '\\s-data.txt', header=None)\n",
    "        # 注意：这里s_data已经转换成秒\n",
    "        s_data.loc[:,0] = s_data.loc[:,0]/1000\n",
    "    elif 'calcium_intensity.npy' in f:\n",
    "        # 钙信号数据(* 如果有)\n",
    "        print(f'读取钙信号数据：{f}')\n",
    "        Cal_data_path = os.path.join(p_f, f)\n",
    "        calcium_intensity= np.load(Cal_data_path)\n",
    "        Calcium = True    # 包含钙信号数据\n",
    "    elif 'output-0516' in f:\n",
    "        # 中线npz数据(* 如果有)\n",
    "        print(f'读取中线文件：{f}')\n",
    "        midline_path = os.path.join(p_f,f)\n",
    "        npz_data = np.load(midline_path, allow_pickle=True)\n",
    "        data = npz_data['arr_0'].item()\n",
    "        # paths= {key: value['all_paths'] for key, value in data.items()}\n",
    "        data_keys = list(data.keys())   # 存储的符合条件的帧数\n",
    "        midline_extract = True   # 是否包含抽取中线后的文件\n",
    "    elif 'iou_results' in f:\n",
    "        print(f'读取中线处理相关文件{f}')\n",
    "        data_iou = np.load(os.path.join(p_f,'iou_results.npz'))\n",
    "    \n",
    "        \n",
    "# 步骤一：根据载物台运动位置信息抽取运动参数\n",
    "mean_frame_rate = np.median(1/np.diff(df_motion.Time.values))\n",
    "print(np.argmin(np.diff(df_motion.Time.values)))\n",
    "df_motion_als = Sliding_CTX_calation(df_motion, Pos_CTX_vec, hlf_spd_inv, hlf_agl_spd_inv, hlf_agl_inv,\n",
    "                                    window_size=100, frame_rate=mean_frame_rate,X_shift=x_shift)\n",
    "\n",
    "\n",
    "# 步骤二： 提取视频帧（根据中线处理结果只保留可用帧）对应的运动参数，输出对齐视频可用帧时间后的运动参数文件\n",
    "df_motion_pt_sorted = df_motion_als.sort_values('Time')  # 确保 df_motion_pt 也是按时间升序\n",
    "df_t = pd.DataFrame({'Time': t_c})\n",
    "df_t = df_t.sort_values('Time')  # merge_asof 要求升序\n",
    "df_motion_pt_sorted = df_motion_pt_sorted.rename(columns={'Time': 'Stage_time'})\n",
    "# 使用 merge_asof 匹配最近的时间点\n",
    "df_matched = pd.merge_asof(\n",
    "    df_t,\n",
    "    df_motion_pt_sorted,\n",
    "    left_on='Time',\n",
    "    right_on='Stage_time',\n",
    "    direction='nearest'  # 可选：'backward', 'forward', or 'nearest'\n",
    ")\n",
    "\n",
    "# 步骤三：加入condition\n",
    "folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "df_matched = add_condition_info(folder_name, df_matched)\n",
    "# 重置中线位置\n",
    "df_matched['Disp_to_mid'] = df_matched['X']-half_width\n",
    "# 保存合并运动参数和视频帧对应时间戳的df数据\n",
    "df_matched.to_csv(os.path.join(p_f,basename+ '_mot_vid.csv'), index=False)\n",
    "'''文件长度与视频帧数相同'''\n",
    "\n",
    "# 步骤四:合并中线数据\n",
    "if midline_extract:\n",
    "\n",
    "    # 整理choose帧\n",
    "    choose_frame = get_choose_frame(data, data_keys, data_iou)\n",
    "    df_choose = pd.DataFrame(choose_frame, columns=['key'])\n",
    "    df_choose.loc[:,'choose_frame'] = int(1)\n",
    "    df_choose = df_choose.set_index('key')\n",
    "\n",
    "    rows = []\n",
    "    for key in data_keys:\n",
    "        item = data[key]\n",
    "        row = {\n",
    "            'key': key,   # 对应视频帧\n",
    "            'angle_m': item['angle_m'],  # 头部朝向与运动方向夹角绝对值\n",
    "            'angle_md': item['angle_md'],  # 头部朝向与运动方向夹角\n",
    "            'all_paths':item['all_paths'],  # 完整的骨架信息\n",
    "            'closest_idx':item['closest path'], # 最近的骨架id\n",
    "        }\n",
    "        rows.append(row)\n",
    "    # 构建 DataFrame\n",
    "    df_summary = pd.DataFrame(rows).set_index('key')\n",
    "    # 保留所有的处理后的帧信息\n",
    "    df_merged = df_matched.join(df_summary, how='left')\n",
    "\n",
    "    # 保存筛选帧信息：哪些中线的置信度高于阈值\n",
    "    df_merged = df_merged.join(df_choose, how='left')   # 根据index合并，df_choose里没有的填none\n",
    "\n",
    "    # 中线处理，计算turn\n",
    "    df_turn_curv = get_omega_turn_curvature(midline_path, path_ratio_uplim = 1.5, path_uplim=200, dtheta_lim=220,\n",
    "                              cl_turn_size = 50, op_turn_size = 50, visualize_turn=True,\n",
    "                              seg_pixel_step = 13, agl_window = 300, frame_rate=38)\n",
    "    df_merged_2 = df_merged.join(df_turn_curv, how='left')\n",
    "\n",
    "    df_merged_2.to_csv(os.path.join(p_f,basename+ '_mot_vid_mid.csv'), index=False)\n",
    "    '''文件仍然与视频长度相同，包含所有中线处理信息以及筛选帧信息'''\n",
    "\n",
    "# 步骤五：根据激光开始和结束时间截取荧光拍摄部分的运动数据/时间戳\n",
    "t_frame, t_start, t_stop = get_laser_on_and_off(s_data)\n",
    "track_start_index = np.searchsorted(df_matched.Time.values, t_start) - 1   # 截取激光照射段的载物台数据时间戳\n",
    "track_end_index = np.searchsorted(df_matched.Time.values, t_stop)\n",
    "df_matched_cut = df_matched.loc[track_start_index:track_end_index, :]\n",
    "df_vol_time = pd.DataFrame(t_frame, columns=['Vol_Time'])   # 激光照射时间戳要转换单位为s\n",
    "stage_file = os.path.join(p_f, f's{0+1}.txt')\n",
    "vol_file = os.path.join(p_f, f't{0+1}.txt')\n",
    "np.savetxt(stage_file, df_matched_cut[['Time','X_org','Y_org']], fmt='%.4f')  # 存储载物台数据(时间戳是视频帧的)\n",
    "np.savetxt(vol_file, t_frame, fmt='%.4f')  # 存储荧光每一个volume的数据\n",
    "df_matched_cut.to_csv(os.path.join(p_f, basename+'_mot_vid_cut.csv'), index = False)\n",
    "if midline_extract:\n",
    "    df_mot_midline_cut = df_merged_2.loc[track_start_index:track_end_index, :]\n",
    "    df_mot_midline_cut.to_csv(os.path.join(p_f, basename+'_mot_midline_cut.csv'), index = False)\n",
    "\n",
    "# 步骤六：根据荧光数据时间戳提取对应的运动参数，便于后续合并荧光数据\n",
    "# 如果有中线数据，就用带中线数据的对齐。否则只用运动参数对齐\n",
    "if midline_extract:\n",
    "    df_mat_cut = df_mot_midline_cut.copy()\n",
    "    print('有中线数据, 与Volume对齐的运动和中线数据')\n",
    "else:\n",
    "    df_mat_cut = df_matched_cut.copy()\n",
    "    print('没有中线数据, 输出与Volume时间对齐的运动参数')\n",
    "\n",
    "df_vol_time = get_nearest_value(df_mat_cut, df_vol_time, col_1 = 'Time', col_2 = 'Vol_Time', ind_col = 'Nearest_Index', col1_copy = 'Nearest_Time')\n",
    "# 将df_motionz中的运动参数(有中线加上中线)按照索引对应到视频帧文件的时间戳上\n",
    "columns_to_add = df_mat_cut.columns\n",
    "for col in columns_to_add:\n",
    "    df_vol_time[col] = df_mat_cut.iloc[df_vol_time['Nearest_Index'].values][col].values\n",
    "if midline_extract:\n",
    "    fn_motion_output = folder_name+'_MotionMidlineMatchVol.csv'\n",
    "else:\n",
    "    fn_motion_output = folder_name+'_MotionMatchVol.csv'\n",
    "df_vol_time.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "print(f'写出与神经数据时间戳(不包含钙数据)对应的运动参数文件到:{folder_name}/{fn_motion_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff4bf5",
   "metadata": {},
   "source": [
    "# 主函数：批量计算运动参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入每一对txt文件所在的的文件夹\n",
    "\n",
    "file_path_all = r'Y:\\\\SZX\\\\2025_wbi_analysis\\\\good_WBI\\\\no_binary\\\\new'\n",
    "\n",
    "# 包含所有荧光数据的文件夹（没有则不加）\n",
    "folder_beh_label = ''    # 行为标记文件夹名称\n",
    "file_path_beh_label = os.path.join(file_path_all, folder_beh_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07b16b",
   "metadata": {},
   "source": [
    "根据行为数据记录原始文件，依次进行：\n",
    "+ 对齐stage, camera, fluorescent imaging时间轴\n",
    "+ 根据位置坐标序列抽取运动参数，输出MotionExtract文件（待裁剪/拼接）\n",
    "+ 根据volume timestamp对齐神经数据时间和位置序列时间，将行为参数列和从calcium_trace.npy的神经活动迹合并到单个文件NeuralAlignMotion中\n",
    "    (如果暂时没有calcium可以选择不合并)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 放入所有包含单个实验文件的文件夹\n",
    "\n",
    "label_corner = True\n",
    "new_data = True # 2025.2之后的数据，c1中1为视频存储帧\n",
    "x_shift  = 0     # mm 中线相对于梯度中线的偏离\n",
    "key_word = ''\n",
    "nokey_word = '*'\n",
    "hlf_spd_inv = 0.5\n",
    "hlf_agl_spd_inv = 0.2\n",
    "hlf_agl_inv = 1\n",
    "Pos_CTX_vec = [-1,0]\n",
    "t_crit = 3.8\n",
    "min_agl_spd = 75\n",
    "cut_window = 2000\n",
    "half_width = 20  #mm 琼脂片宽度的一半\n",
    "Calcium = False\n",
    "midline_extract = True\n",
    "\n",
    "if label_corner:\n",
    "    required_files = ['stage_data.txt', 'c1.txt',\n",
    "                     'upper_left.txt', 'lower_left.txt', 'upper_right.txt',\n",
    "                     'lower_right.txt']   # 数据处理需要的文件\n",
    "else:\n",
    "    required_files = ['stage_data.txt', 'c1.txt',\n",
    "                     'edge0.txt', 'edge45.txt']   # 数据处理需要的文件\n",
    "required_files_set = set(required_files)\n",
    "# p_f_test = r'Z:\\data space+\\C. elegans chemotaxis\\2025_WBI\\good_WBI\\test'  # 用于储存临时输出的文件夹\n",
    "file_paths = [os.path.join(file_path_all,f_p) for f_p in os.listdir(file_path_all) if ('trash' not in f_p)&\n",
    "              ('done' not in f_p)&\n",
    "             os.path.isdir(os.path.join(file_path_all,f_p))&\n",
    "             (key_word in f_p)&(nokey_word not in f_p)]\n",
    "\n",
    "for i, p_f in enumerate(file_paths):\n",
    "    df_motion = {}\n",
    "    df_vol_time = {}\n",
    "    calcium = True   # 默认有荧光信号的所有文件都包含钙信号\n",
    "    files = [f for f in os.listdir(p_f) if '.txt' in f ]\n",
    "    # 转化为集合进行比较\n",
    "    files_set = set(files)\n",
    "    # 判断文件夹中的文件是否完全包含自定义列表中的元素\n",
    "    is_contained = required_files_set.issubset(files_set)\n",
    "    # 找出missing_files\n",
    "    missing_files = [file for file in required_files if file not in files]\n",
    "    \n",
    "    if is_contained:\n",
    "        basename = os.path.basename(os.path.normpath(p_f))\n",
    "        print(f'\\n\\n=======开始处理{basename}中文件，\\n包含:{files}')\n",
    "        \n",
    "        files = [f for f in os.listdir(p_f)]\n",
    "        for f in files:\n",
    "            if 'stage_data' in f:\n",
    "                # 使用载物台位置信息计算速度角速度\n",
    "                print('读取载物台位置信息文件')\n",
    "                column_names = ['Time','p_X','p_Y', 'X', 'Y']\n",
    "                df_stage = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None,\n",
    "                                        names=column_names)\n",
    "                columns_to_clean = ['Time', 'p_X', 'p_Y', 'X', 'Y']\n",
    "                for col in columns_to_clean:\n",
    "                    df_stage[col] = df_stage[col].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "                df_motion = df_stage[['Time','X','Y']]/1000  # 转换单位为s和mm\n",
    "                # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "                # 保证p_f中有四个顶点文件\n",
    "                if label_corner:\n",
    "                    df_label, df_motion = Realign_coordinate(p_f, df_motion)\n",
    "                else:\n",
    "                    df_motion = Realign_coordinate_by_edge(p_f, df_motion)\n",
    "                # 打印轨迹\n",
    "            elif 'c1.txt' in f:\n",
    "                # 读取视频帧文件\n",
    "                print('读取视频帧时间戳文件并转为f_data')\n",
    "                f_data = np.loadtxt(p_f + '\\c1.txt', delimiter=',') # 读取相机时间戳\n",
    "                if new_data: \n",
    "                    # 202502之后的数据是新数据，视频帧率为30-40Hz\n",
    "                    t_c = f_data[f_data[:,3] == 1, 1]/1000\n",
    "                else:\n",
    "                    # 旧数据视频帧率150Hz 每隔5帧取一帧（适用于老数据）\n",
    "                    indices = np.concatenate(([0], np.arange(5, len(f_data), 5)))\n",
    "                    t_c = f_data[indices, 1].astype(int)/1000\n",
    "            elif ('laser-data' in f)or (\"s-data\" in f):\n",
    "                # 读取激光激活数据\n",
    "                print('读取激光照射时间数据并转为s_data')\n",
    "                try:\n",
    "                    s_data = pd.read_csv(p_f + '\\laser-data.txt', header=None)  #读取激光激活数据\n",
    "                except FileNotFoundError:\n",
    "                    s_data = pd.read_csv(p_f + '\\s-data.txt', header=None)\n",
    "                # 注意：这里s_data已经转换成秒\n",
    "                s_data.loc[:,0] = s_data.loc[:,0]/1000\n",
    "            elif 'calcium_intensity.npy' in f:\n",
    "                # 钙信号数据(* 如果有)\n",
    "                print(f'读取钙信号数据：{f}')\n",
    "                Cal_data_path = os.path.join(p_f, f)\n",
    "                calcium_intensity= np.load(Cal_data_path)\n",
    "                Calcium = True    # 包含钙信号数据\n",
    "            elif 'output-0516' in f:\n",
    "                # 中线npz数据(* 如果有)\n",
    "                print(f'读取中线文件：{f}')\n",
    "                midline_path = os.path.join(p_f,f)\n",
    "                npz_data = np.load(midline_path, allow_pickle=True)\n",
    "                data = npz_data['arr_0'].item()\n",
    "                data_keys = list(data.keys())   # 存储的符合条件的帧数\n",
    "                midline_extract = True   # 是否包含抽取中线后的文件\n",
    "            elif 'iou_results' in f:\n",
    "                print(f'读取中线处理相关文件{f}')\n",
    "                data_iou = np.load(os.path.join(p_f,'iou_results.npz'))\n",
    "        if midline_extract:\n",
    "            # 步骤零点五：根据骨架处理结果筛选符合条件的帧\n",
    "            # 读取骨架npz中的数据，排除异常帧\n",
    "    #         path_summary = {key: value['path_summary'] for key, value in data.items()} # 骨架分析summary\n",
    "            # paths = {key: value['all_paths'] for key, value in data.items()} # 骨架坐标\n",
    "            # vector_h = {key: value['vector_h'] for key, value in data.items()} # 头部方向\n",
    "            # vector_m = {key: value['vector_m'] for key, value in data.items()} # 运动方向\n",
    "            # pos_phr = {key: value['pos_phr'] for key, value in data.items()} # 咽喉坐标\n",
    "            # data_keys = list(data.keys())   # 存储的处理后符合条件的视频帧数\n",
    "            # angle_m = {key: value['angle_m'] for key, value in data.items()} # 前进方向与身体方向角度\n",
    "            # closest_idx = {key: value['closest path'] for key, value in data.items()}\n",
    "            # circle_dict = {key: value['circular'] for key, value in data.items()}\n",
    "            # branching_dict = {key: value['branching'] for key, value in data.items()}\n",
    "            # 识别卷曲动作区域\n",
    "            circle = np.array(list({key: value['circular'] for key, value in data.items()}.values())) \n",
    "            branching = np.array(list({key: value['branching'] for key, value in data.items()}.values())) \n",
    "            choose = np.array(list({key: value['choose_frame'] for key, value in data.items()}.values())) \n",
    "            circular_frames = np.array(data_keys)[circle == 1]\n",
    "            branching_frames = np.array(data_keys)[branching == 1]\n",
    "            merged = list(heapq.merge(circular_frames, branching_frames))\n",
    "            choose_frame = np.array(data_keys)[choose == 1]\n",
    "            # 识别面积抖动区间\n",
    "            # iou = np.load(os.path.join(folder_path,'iou_results.npz'))['iou']\n",
    "            iou = data_iou[data_iou.files[0]] \n",
    "            ano_f = np.vstack([[1, 1], iou]) < 0.7\n",
    "            pre_f= np.where(ano_f[:,0] == True)[0]\n",
    "            post_f= np.where(ano_f[:,1] == True)[0]\n",
    "            ano_merged = list(heapq.merge(pre_f, post_f))\n",
    "            # 排除IOU异常帧\n",
    "            choose_frame = np.setdiff1d(choose_frame, ano_merged)\n",
    "                \n",
    "        # 步骤一：根据载物台运动位置信息抽取运动参数\n",
    "        mean_frame_rate = np.median(1/np.diff(df_motion.Time.values))\n",
    "        print(np.argmin(np.diff(df_motion.Time.values)))\n",
    "        df_motion_als = Sliding_CTX_calation(df_motion, Pos_CTX_vec, hlf_spd_inv, hlf_agl_spd_inv, hlf_agl_inv,\n",
    "                                          window_size=100, frame_rate=mean_frame_rate,X_shift=x_shift)\n",
    "        # df_motion_pt = Label_Pirouettes_by_id(df_motion_als, t_crit=t_crit, min_agl_spd=min_agl_spd)\n",
    "#         visualization check\n",
    "        # Scatter_Pirouette(df_motion_pt, folder=p_f+'\\\\')\n",
    "        # Scatter_Pos_Grad_Dir(df_motion_pt, Pos_CTX_vec, folder=p_f+'\\\\')\n",
    "#         df_motion_pt.to_csv(os.path.join(p_f_test,'df_motion_pt.csv'), index=False)\n",
    "        \n",
    "        \n",
    "        # 步骤二： 提取视频帧（根据中线处理结果只保留可用帧）对应的运动参数，输出对齐视频可用帧时间后的运动参数文件\n",
    "        # 确保 df_motion_pt 也是按时间升序\n",
    "        df_motion_pt_sorted = df_motion_als.sort_values('Time')\n",
    "#         df_t['Time'] = pd.to_datetime(df_t['Time'])\n",
    "#         df_motion_pt_sorted['Time'] = pd.to_datetime(df_motion_pt_sorted['Time'])\n",
    "        df_t = pd.DataFrame({'Time': t_c})\n",
    "        df_t = df_t.sort_values('Time')  # merge_asof 要求升序\n",
    "#         df_motion_pt_sorted.to_csv(os.path.join(p_f_test,'df_motion_pt_sorted.csv'), index=False)\n",
    "        # 无论有没有中线\n",
    "        df_motion_pt_sorted = df_motion_pt_sorted.rename(columns={'Time': 'Stage_time'})\n",
    "\n",
    "        # 使用 merge_asof 匹配最近的时间点\n",
    "        df_matched = pd.merge_asof(\n",
    "            df_t,\n",
    "            df_motion_pt_sorted,\n",
    "            left_on='Time',\n",
    "            right_on='Stage_time',\n",
    "            direction='nearest'  # 可选：'backward', 'forward', or 'nearest'\n",
    "        )\n",
    "        \n",
    "        # 加入condition\n",
    "        folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "        features = folder_name.split('_')\n",
    "        if len(features) <3:\n",
    "            print('文件名异常，未检测到Condition信息')\n",
    "        else:\n",
    "            # 如果Condition信息存在，在df_vol_time中加入condition信息\n",
    "            df_matched['Date'] = features[0]\n",
    "            df_matched['Rep_id'] = features[-1]\n",
    "            condition_ls = features[-2].split('-')\n",
    "            print(f'日期:{features[0]}, 编号：{features[-1]}\\n Conditions:{condition_ls}')\n",
    "            for i, c in enumerate(condition_ls):\n",
    "                if len(condition_ls) > 1:\n",
    "                    con_col = 'Condition'+str(i)\n",
    "                    df_matched[con_col] = c\n",
    "                else:\n",
    "                    df_matched['Condition'] = features[-2]\n",
    "            if len(condition_ls) > 1:\n",
    "                df_matched['Date_Con_Rep'] = features[-2]\n",
    "            df_matched['Disp_to_mid'] = df_matched['X']-half_width\n",
    "        \n",
    "        \n",
    "        # 保存合并运动参数和视频帧对应时间戳的df数据\n",
    "        df_matched.to_csv(os.path.join(p_f,basename+ '_mot_vid.csv'), index=False)\n",
    "        '''文件长度与视频帧数相同'''\n",
    "        \n",
    "        if midline_extract:\n",
    "            # 读取npz中的数据\n",
    "            \"\"\"\n",
    "                t_stage:载物台移动数据，原始帧率200，降采样到40fps，提供位置信息\n",
    "                t_c:追踪视频帧时间戳，帧率30，提供线虫中线等信息  \n",
    "                data_keys：基于追踪视频进行处理后的视频帧。\n",
    "            \"\"\"\n",
    "            rows = []\n",
    "            for key in data_keys:\n",
    "                item = data[key]\n",
    "                row = {\n",
    "                    'key': key,   # 对应视频帧\n",
    "                    'angle_m': item['angle_m'],  # 头部朝向与运动方向夹角绝对值\n",
    "                    'angle_md': item['angle_md'],  # 头部朝向与运动方向夹角\n",
    "                    'all_paths':item['all_paths'],  # 完整的骨架信息\n",
    "                    'closest_idx':item['closest path'], # 最近的骨架id\n",
    "                    # 'circular':item[\"circular\"],  # 成环\n",
    "                    # 'branching':item['branching'],  # 分叉\n",
    "                    # 'vector_h':item['vector_h'],   # 头部朝向\n",
    "                    # 'vector_m':item['vector_m'],  # 运动方向\n",
    "                    # 'pos_phr':item['pos_phr'],    # 咽部位置\n",
    "                }\n",
    "                rows.append(row)\n",
    "                \n",
    "            # 构建 DataFrame\n",
    "            df_summary = pd.DataFrame(rows).set_index('key')\n",
    "            # 保留所有的处理后的帧信息\n",
    "            df_merged = df_matched.join(df_summary, how='left')\n",
    "            # 保存筛选帧信息：哪些中线的置信度高于阈值\n",
    "            # choose_frame是一个列表，包含了被选择的中线的索引\n",
    "            df_choose = pd.DataFrame(choose_frame, columns=['key'])\n",
    "            df_choose.loc[:,'choose_frame'] = int(1)\n",
    "            df_choose = df_choose.set_index('key')\n",
    "            df_merged_2 = df_merged.join(df_choose, how='left')   # 根据index合并，df_choose里没有的填none\n",
    "            df_merged_2.to_csv(os.path.join(p_f,basename+ '_mot_vid_mid.csv'), index=False)\n",
    "            '''文件仍然与视频长度相同，包含所有中线处理信息以及筛选帧信息'''\n",
    "\n",
    "        # 步骤三：根据激光开始和结束时间截取荧光拍摄部分的运动数据/时间戳     \n",
    "        # 提取每个激光照射起始和终止时刻\n",
    "        s_start = s_data[s_data[1] == 2].index\n",
    "        s_stop = s_data[s_data[1] == 3].index\n",
    "        if s_stop.empty:\n",
    "            s_s = [len(s_data[1])]\n",
    "        else:\n",
    "            s_s = []\n",
    "            for m in s_start:\n",
    "                # 如果有多个照射片段，分别找到结束的时间装入s_s\n",
    "                data_1 = (s_stop >= m)\n",
    "                s_s.append(s_stop[data_1 == 1][0])\n",
    "        \n",
    "        i=0  # 不考虑其它段，只考虑第一段\n",
    "        #         t_l = int(round((s_data[0][s_start[i] + 1 : s_s[i] - 1]).diff().sum(skipna=True) / (s_s[i] - s_start[i] - 2)))\n",
    "        t_l = (s_data[0][s_start[i] + 1 : s_s[i] - 1]).diff().sum(skipna=True) / (s_s[i] - s_start[i] - 2)\n",
    "        print('时间间隔t_l',t_l)\n",
    "        t = s_data[0].values\n",
    "        # 取从激光照射开始前2帧到激光照射结束的数据\n",
    "        t_frame = np.concatenate([[t[s_start[i]] - t_l *2, t[s_start[i]] - t_l], t[s_start[i]:s_s[i]]])\n",
    "        t_start = s_data[0][s_start[i]] - t_l *2\n",
    "        t_stop = s_data[0][s_s[i] - 1] + t_l\n",
    "\n",
    "        # 截取激光照射段的载物台数据时间戳\n",
    "        track_start_index = np.searchsorted(df_matched.Time.values, t_start) - 1\n",
    "        track_end_index = np.searchsorted(df_matched.Time.values, t_stop)\n",
    "\n",
    "        df_matched_cut = df_matched.loc[track_start_index:track_end_index, :]\n",
    "        df_vol_time = pd.DataFrame(t_frame, columns=['Vol_Time'])   # 激光照射时间戳要转换单位为s\n",
    "\n",
    "        stage_file = os.path.join(p_f, f's{i+1}.txt')\n",
    "        vol_file = os.path.join(p_f, f't{i+1}.txt')\n",
    "\n",
    "        # 保存文件\n",
    "        np.savetxt(stage_file, df_matched_cut[['Time','X_org','Y_org']], fmt='%.4f')  # 存储载物台数据(时间戳是视频帧的)\n",
    "        np.savetxt(vol_file, t_frame, fmt='%.4f')  # 存储荧光每一个volume的数据\n",
    "        df_matched_cut.to_csv(os.path.join(p_f, basename+'_mot_vid_cut.csv'), index = False)\n",
    "        if midline_extract:\n",
    "            df_mot_midline_cut = df_merged_2.loc[track_start_index:track_end_index, :]\n",
    "            df_mot_midline_cut.to_csv(os.path.join(p_f, basename+'_mot_midline_cut.csv'), index = False)\n",
    "        \n",
    "        # 步骤四：根据荧光数据时间戳提取对应的运动参数，便于后续合并荧光数据\n",
    "        # 如果有中线数据，就用带中线数据的对齐。否则只用运动参数对齐\n",
    "        if midline_extract:\n",
    "            df_mat_cut = df_mot_midline_cut.copy()\n",
    "            print('有中线数据，输出与Volume对齐的运动和中线数据')\n",
    "        else:\n",
    "            df_mat_cut = df_matched_cut.copy()\n",
    "            print('没有中线数据，只输出与Volume时间对齐的运动参数')\n",
    "        \n",
    "        nearest_times = []\n",
    "        nearest_indices = []\n",
    "        for vol_time in df_vol_time['Vol_Time']:\n",
    "            # 计算时间差的绝对值\n",
    "            differences = np.abs(df_mat_cut['Time'] - vol_time)\n",
    "            # 找到最小差值的索引\n",
    "            nearest_idx = np.argmin(differences)\n",
    "            # 保存最近时间值和索引\n",
    "            nearest_times.append(df_mat_cut.iloc[nearest_idx, df_mat_cut.columns.get_loc('Time')])\n",
    "            nearest_indices.append(nearest_idx)\n",
    "\n",
    "        # 将结果存入 DataFrame\n",
    "        df_vol_time['Nearest_Time'] = nearest_times\n",
    "        df_vol_time['Nearest_Index'] = nearest_indices\n",
    "\n",
    "        # 将df_motionz中的运动参数(有中线加上中线)按照索引对应到视频帧文件的时间戳上\n",
    "        if midline_extract:\n",
    "#             columns_to_add = ['X', 'Y', 'speed','x_velocity','y_velocity', 'agl_speed', 'CTX_left', 'Event', 'Reorientation', 'angle_m','all_paths','choose_frame'],  # 头部朝向与运动方向夹角\n",
    "            columns_to_add = df_mat_cut.columns\n",
    "        else:\n",
    "            columns_to_add = df_mat_cut.columns\n",
    "#             columns_to_add = ['X', 'Y', 'speed','x_velocity','y_velocity', 'agl_speed', 'CTX_left', 'Event', 'Reorientation']\n",
    "#         for col in columns_to_add:\n",
    "#             df_vol_time[col] = df_vol_time['Nearest_Index'].map(df_mat_cut[col])\n",
    "        for col in columns_to_add:\n",
    "            df_vol_time[col] = df_mat_cut.iloc[df_vol_time['Nearest_Index'].values][col].values\n",
    "        \n",
    "        # 输出与神经数据时间戳对齐的运动数据\n",
    "        if midline_extract:\n",
    "            fn_motion_output = folder_name+'_MotionMidlineMatchVol.csv'\n",
    "        else:\n",
    "            fn_motion_output = folder_name+'_MotionMatchVol.csv'\n",
    "        df_vol_time.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "        print(f'写出与神经数据时间戳(不包含钙数据)对应的运动参数文件到:{folder_name}/{fn_motion_output}')\n",
    "        \n",
    "        if Calcium:\n",
    "            print('处理前文件大小:neuron*timestamp',calcium_intensity.shape)\n",
    "            # 将神经数据与行为数据合并\n",
    "            calcium_intensity_T = calcium_intensity.T\n",
    "            # 创建列名\n",
    "            col_neuron_names = [f\"neuron{i+1}\" for i in range(calcium_intensity_T.shape[1])]\n",
    "            df_calcium = pd.DataFrame(calcium_intensity_T, columns=col_neuron_names)\n",
    "            # 按照行索引合并\n",
    "            df_neu_motion = pd.concat([df_calcium, df_vol_time], axis=1)\n",
    "            \n",
    "            if midline_extract:\n",
    "                fn_nt_output = folder_name+'_MotionMidlineMatchNeural.csv'\n",
    "                df_neu_motion.to_csv(os.path.join(p_f, fn_nt_output), index = False)\n",
    "                print(f'写出与神经匹配的运动信息(包含中线和钙数据)到:{folder_name}/{fn_nt_output}')\n",
    "            else:\n",
    "                fn_nt_output = folder_name+'_MotionMatchNeural.csv'\n",
    "                df_neu_motion.to_csv(os.path.join(p_f, fn_nt_output), index = False)\n",
    "                print(f'写出与神经匹配的运动信息(只钙数据无中线)到:{folder_name}/{fn_nt_output}')\n",
    "\n",
    "    else:\n",
    "        print(f'{os.path.basename(os.path.normpath(p_f))}中缺少数据处理必须的文件如下：\\n' + \"\\n\".join(missing_files))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6ca60",
   "metadata": {},
   "source": [
    "### 数据裁剪\n",
    "+ 对于追丢的数据，需要裁剪追踪丢失时间点前的数据作为最终数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b11e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 包含同一天数据的大文件夹\n",
    "file_path_beh = r'Z:\\data space+\\C. elegans chemotaxis\\20250410_WBI\\track'\n",
    "# # 包含所有荧光数据的文件夹\n",
    "# file_path_fluo = r'D:\\data analysis\\fluorescent'\n",
    "# 裁剪文件夹名称和裁剪时段(s)按顺序摆放\n",
    "file_to_slice = ['20250410_1g-ov-24d_001'\n",
    "                ]\n",
    "idx_end = [4250]\n",
    "\n",
    "write = True\n",
    "\n",
    "print('检查文件数量和idx_ls数量是否相同：', len(idx_end)==len(file_to_slice))\n",
    "for i in range(len(idx_end)):\n",
    "    print(f\"{file_to_slice[i]}:{idx_end[i]}\\n\")\n",
    "for i,f in enumerate(file_to_slice):\n",
    "    print(f'=======开始处理文件{f}====================')\n",
    "    f_p = os.path.join(file_path_beh, f)\n",
    "    \n",
    "    f_mot = f+'_MotionExtract.csv'\n",
    "    df_mot = pd.read_csv(os.path.join(f_p, f_mot))\n",
    "    file_contain_cal = [file for file in os.listdir(os.path.join(file_path_beh, f)) if '_NeuralAlignMotionCal' in file]\n",
    "    if len(file_contain_cal):\n",
    "        f_neu = f+'_NeuralAlignMotionCal.csv'\n",
    "    else:\n",
    "        f_neu = f+'_NeuralAlignMotion.csv'\n",
    "    df_neu = pd.read_csv(os.path.join(f_p, f_neu))\n",
    "    print(f'原本NeuralAlignMotion文件的时间跨度为:{df_neu.Vol_Time.min()}~{df_neu.Vol_Time.max()}')\n",
    "    df_neu_slice = df_neu.loc[:idx_end[i], :]\n",
    "    print(f'裁剪后NeuralAlignMotion文件的时间跨度：{df_neu_slice.Vol_Time.min()}~{df_neu_slice.Vol_Time.max()}')\n",
    "    nearest_time = df_neu.loc[idx_end[i], 'Nearest_Time']\n",
    "    print(f'原本MotionExtract文件的时间跨度为:{df_mot.Time.min()}~{df_mot.Time.max()}')\n",
    "    df_mot_slice = df_mot[df_mot.Time <= nearest_time]\n",
    "    print(f'裁剪后MotionExtract文件的时间跨度：{df_mot_slice.Time.min()}~{df_mot_slice.Time.max()}')\n",
    "    \n",
    "#     # 用于寻找神经数据\n",
    "#     f_decom_name = f.split(\"_\")\n",
    "#     f_date = f_decom_name[0]\n",
    "#     f_idx  = f_decom_name[-1]\n",
    "    \n",
    "#     cal_path = os.path.join(file_path_fluo, f_date, f_idx)\n",
    "#     cal_int= np.load(os.path.join(cal_path, 'calcium_intensity.npy'))\n",
    "#     print('处理前文件大小:neuron*timestamp',cal_int.shape)\n",
    "#     cal_int_slice = cal_int[:,:idx_end[i]]\n",
    "#     print('处理后:neuron*timestamp',cal_int_slice.shape)\n",
    "    \n",
    "    if write:\n",
    "          f_mot_new = f+'_MotionExtract_slice.csv'\n",
    "          df_mot_slice.to_csv(os.path.join(f_p, f_mot_new), index = False)\n",
    "          f_neu_new = f+'_NeuralAlignMotion_slice.csv'\n",
    "          df_neu_slice.to_csv(os.path.join(f_p, f_neu_new), index = False)\n",
    "print('裁剪处理完成>.<\\n======================================')\n",
    "#           cal_new =\" calcium_intensity_slice.npy\"\n",
    "#           np.save(os.path.join(cal_path,cal_new), cal_int_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b45f4",
   "metadata": {},
   "source": [
    "### 数据拼接\n",
    "+ 对于行为状态良好，但是意外追丢的线虫可能会重开继续追，将多段数据根据时间关系拼接在一起\n",
    "+ 先抽运动参数再拼接\n",
    "+ 思路：根据两个运动数据文件的创建时间差加上后一个文件的原时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08923c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大文件夹装有待拼接的文件\n",
    "file_path_all = r'Z:\\data space+\\C. elegans chemotaxis\\20250409_WBI'\n",
    "# 根据时序关系放置\n",
    "file_to_concat = ['20250409_1g-ov-24d_012', '20250409_1g-ov-24d_013']\n",
    "\n",
    "write = True\n",
    "\n",
    "ctimes = []\n",
    "for i, f_n in enumerate(file_to_concat):\n",
    "    # 获取每个文件夹中state_data的创建时间\n",
    "    f_p = os.path.join(file_path_all,f_n)\n",
    "    stage_data_i = os.path.join(f_p, 'stage_data.txt')\n",
    "    ctimes.append(os.stat(stage_data_i).st_ctime)    # 创建时间（s）\n",
    "    \n",
    "# 计算每个数据与第一个数据stage_data创建时间的时间差\n",
    "# 计算时间差，保留两位小数（0.01秒）\n",
    "time_diff_ls  = []\n",
    "for i, f_n in enumerate(file_to_concat):\n",
    "    print(f_n)\n",
    "    time_dff_i = round(abs(ctimes[i] - ctimes[0]), 3)\n",
    "    time_diff_ls.append(time_dff_i)\n",
    "    f_path = os.path.join(file_path_all, f_n)\n",
    "    f_slice_ls = [f for f in os.listdir(f_path) if ('_slice.csv' in f)]\n",
    "    if len(f_slice_ls):\n",
    "        # 如果有切片文件，使用切片文件\n",
    "        f_mot = [f for f in os.listdir(f_path) if ('MotionExtract_slice.csv' in f)][0]\n",
    "        f_neu = [f for f in os.listdir(f_path) if ('NeuralAlignMotion_slice.csv' in f)][0]\n",
    "    else:\n",
    "        f_mot = [f for f in os.listdir(f_path) if ('MotionExtract.csv' in f)][0]\n",
    "        f_neu = [f for f in os.listdir(f_path) if ('NeuralAlignMotion.csv' in f)][0]\n",
    "\n",
    "    df_mot = pd.read_csv(os.path.join(f_path, f_mot))\n",
    "    df_neu = pd.read_csv(os.path.join(f_path, f_neu))\n",
    "\n",
    "    df_mot['Time'] = df_mot['Time'] + time_dff_i\n",
    "    df_neu['Vol_Time'] = df_neu['Vol_Time']+ time_dff_i\n",
    "    df_neu['Nearest_Time'] = df_neu['Nearest_Time']+ time_dff_i\n",
    "\n",
    "    if write:\n",
    "        print('写出文件')\n",
    "        # 写出运动参数文件\n",
    "        mot_name_new = f_mot.replace(\"MotionExtract\", 'MotionExtract_mod')\n",
    "        df_mot.to_csv(os.path.join(f_path, mot_name_new), index = False)\n",
    "        # 写出对齐神经数据时间戳的运动参数文件\n",
    "        neu_name_new = f_neu.replace(\"NeuralAlignMotion\", 'NeuralAlignMotion_mod')\n",
    "        df_neu.to_csv(os.path.join(f_path, neu_name_new), index = False)\n",
    "\n",
    "print(f\"验证文件创建时间差:{time_diff_ls}\")\n",
    "print(f'=====拼接多个文件======')\n",
    "\n",
    "folder_output = os.path.join(file_path_all, file_to_concat[0])    # 选择拼接文件输出的文件夹，一般设置为第一个片段\n",
    "\n",
    "# 运动数据\n",
    "key_word = 'MotionExtract_mod.csv'\n",
    "df_to_concat = []\n",
    "rep_ids = []\n",
    "for i, f_p in enumerate(file_to_concat):\n",
    "    f_path = os.path.join(file_path_all,f_p)\n",
    "    # 收集所有需要concat的数据路径\n",
    "    f_path_to_concat_i  = [os.path.join(f_path, f) for f in os.listdir(f_path) if key_word in f][0]\n",
    "    df_i = pd.read_csv(f_path_to_concat_i)\n",
    "#     if i == 0:\n",
    "    # 取第一个文件的编号作为rep_id\n",
    "    rep_id_i = df_i.Rep_id.unique()[0]\n",
    "    rep_ids.append(rep_id_i)\n",
    "    df_to_concat.append(df_i)\n",
    "df_concat = pd.concat(df_to_concat, ignore_index=True)\n",
    "df_concat['Rep_id'] = rep_ids[0]\n",
    "print(f'拼接后使用同一个Rep_id:{rep_ids[0]}')\n",
    "if write:\n",
    "    idx_exp_first = '-'.join(map(str, rep_ids[1:]))\n",
    "    concat_name = file_to_concat[0]+'-'+idx_exp_first+'_MotionExtract_concat.csv'\n",
    "    df_concat.to_csv(os.path.join(folder_output,concat_name), index = False)\n",
    "    \n",
    "# 神经配准数据\n",
    "key_word = 'NeuralAlignMotion_mod.csv'\n",
    "df_to_concat = []\n",
    "rep_ids = []\n",
    "for i, f_p in enumerate(file_to_concat):\n",
    "    f_path = os.path.join(file_path_all,f_p)\n",
    "    # 收集所有需要concat的数据路径\n",
    "    f_path_to_concat_i  = [os.path.join(f_path, f) for f in os.listdir(f_path) if key_word in f][0]\n",
    "    df_i = pd.read_csv(f_path_to_concat_i)\n",
    "    df_i['Old_index'] = df_i.index\n",
    "    rep_id_i = str(df_i.Rep_id.unique()[0])\n",
    "    rep_ids.append(rep_id_i)\n",
    "    df_to_concat.append(df_i)\n",
    "    \n",
    "# 拼接时保留index\n",
    "df_concat = pd.concat(df_to_concat, ignore_index = False)\n",
    "\n",
    "# 取第一个文件的编号作为rep_id\n",
    "df_concat['Rep_id'] = rep_ids[0]\n",
    "print(f'拼接后使用同一个Rep_id:{rep_ids[0]}')\n",
    "if write:\n",
    "    # 修改部分\n",
    "    idx_exp_first = '-'.join(map(str, rep_ids[1:]))  # 将 rep_ids 中的元素转换为字符串类型=\n",
    "    concat_name = file_to_concat[0]+'-'+idx_exp_first+'_NeuralAlignMotion_concat.csv'\n",
    "    df_concat.to_csv(os.path.join(folder_output,concat_name), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b67fa",
   "metadata": {},
   "source": [
    "### 特殊数据处理\n",
    "+ 对于缺少标定点的文件，可跳过对齐坐标轴的步骤只处理文件本身\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf4006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 特殊文件文件夹地址\n",
    "p_f = r'Z:\\data space+\\C. elegans chemotaxis\\20250312_WBI\\20250312_4.5g-24d-4.5h_006'\n",
    "\n",
    "\n",
    "position_file_names  = ['s1']\n",
    "vol_time_file_names = ['t1']\n",
    "\n",
    "required_files = ['stage_data.txt', 'c1.txt','s-data.txt',\n",
    "                 ]   # 数据处理需要的文件\n",
    "required_files_set = set(required_files)\n",
    "write = True    # 是否写出文件\n",
    "hlf_spd_inv = 0.5\n",
    "hlf_agl_spd_inv = 0.2\n",
    "hlf_agl_inv = 1\n",
    "Pos_CTX_vec = [-1,0]\n",
    "t_crit = 3.8\n",
    "min_agl_spd = 75\n",
    "cut_window = 2000\n",
    "half_width = 20  #mm 琼脂片宽度的一半\n",
    "\n",
    "\n",
    "calcium = True   # 默认有荧光信号的所有文件都包含钙信号\n",
    "files = [f for f in os.listdir(p_f) if '.txt' in f ]\n",
    "# 转化为集合进行比较\n",
    "files_set = set(files)\n",
    "# 判断文件夹中的文件是否完全包含自定义列表中的元素\n",
    "is_contained = required_files_set.issubset(files_set)\n",
    "# 找出missing_files\n",
    "missing_files = [file for file in required_files if file not in files]\n",
    "\n",
    "if is_contained:\n",
    "    print(f'\\n\\n=======开始处理{os.path.basename(os.path.normpath(p_f))}中文件，\\n包含:{files}')\n",
    "    # 预处理：时间戳对齐\n",
    "    Align_timestamp(p_f)\n",
    "    for f in files:\n",
    "        if (len(position_file_names)==1):\n",
    "            if position_file_names[0] in f:\n",
    "                # 运动参数位置文件\n",
    "                print('读取运动参数文件')\n",
    "                column_names = ['Time', 'X', 'Y']\n",
    "                print(os.path.join(p_f, f))\n",
    "                df_motion = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None,\n",
    "                                        names=column_names)\n",
    "                df_motion = df_motion.astype(float)\n",
    "\n",
    "                # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "                df_label, df_motion = Realign_coordinate_false(p_f, df_motion)\n",
    "                # 打印轨迹\n",
    "            elif vol_time_file_names[0] in f:\n",
    "                # volume时间戳文件\n",
    "                print('读取volume时间戳文件')\n",
    "                column_names = ['Vol_Time']\n",
    "                print(os.path.join(p_f, f))\n",
    "                df_vol_time = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None, names=column_names)\n",
    "    # 查找在motion.Time中最近值和索引，加入df_vol_time中\n",
    "    nearest_times = []\n",
    "    nearest_indices = []\n",
    "    for vol_time in df_vol_time['Vol_Time']:\n",
    "        # 计算时间差的绝对值\n",
    "        differences = np.abs(df_motion['Time'] - vol_time)\n",
    "        # 找到最小差值的索引\n",
    "        nearest_idx = np.argmin(differences)\n",
    "        # 保存最近时间值和索引\n",
    "        nearest_times.append(df_motion.loc[nearest_idx, 'Time'])\n",
    "        nearest_indices.append(nearest_idx)\n",
    "\n",
    "    # 将结果存入 DataFrame\n",
    "    df_vol_time['Nearest_Time'] = nearest_times\n",
    "    df_vol_time['Nearest_Index'] = nearest_indices\n",
    "\n",
    "\n",
    "    # 根据运动位置信息抽取运动参数\n",
    "\n",
    "    mean_frame_rate = np.mean(1/np.diff(df_motion.Time.values))\n",
    "    df_motion_als = Sliding_CTX_calation(df_motion, Pos_CTX_vec, hlf_spd_inv, hlf_agl_spd_inv, hlf_agl_inv,\n",
    "                                      window_size=100, frame_rate=mean_frame_rate)\n",
    "    df_motion_pt = Label_Pirouettes_by_id(df_motion_als, t_crit=t_crit, min_agl_spd=min_agl_spd)\n",
    "    # visualization check\n",
    "    Scatter_Pirouette(df_motion_pt, folder=p_f+'\\\\')\n",
    "    Scatter_Pos_Grad_Dir(df_motion_pt, Pos_CTX_vec, folder=p_f+'\\\\')\n",
    "#     cut_and_draw(df_motion_pt, n = cut_window)\n",
    "\n",
    "    # 将df_motionz中的运动参数按照索引对应到神经数据时间戳上\n",
    "    columns_to_add = ['X', 'Y', 'speed', 'agl_speed', 'CTX_left', 'Event', 'Reorientation']\n",
    "    for col in columns_to_add:\n",
    "        df_vol_time[col] = df_vol_time['Nearest_Index'].map(df_motion_pt[col])\n",
    "\n",
    "    # 加入condition\n",
    "    folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "    features = folder_name.split('_')\n",
    "    df_motion_pt['Date'] = features[0]\n",
    "    df_vol_time['Date'] = features[0]\n",
    "    df_motion_pt['Rep_id'] = features[-1]\n",
    "    df_vol_time['Rep_id'] = features[-1]\n",
    "    condition_ls = features[-2].split('-')\n",
    "    print(f'日期:{features[0]}, 编号：{features[-1]}\\n Conditions:{condition_ls}')\n",
    "    for i, c in enumerate(condition_ls):\n",
    "        if len(condition_ls) > 1:\n",
    "            con_col = 'Condition'+str(i)\n",
    "            df_motion_pt[con_col] = c\n",
    "            df_vol_time[con_col] = c\n",
    "        else:\n",
    "            df_motion_pt['Condition'] = features[-2]\n",
    "            df_vol_time['Condition'] = features[-2]\n",
    "    if len(condition_ls) > 1:\n",
    "        df_vol_time['Date_Con_Rep'] = features[-2]\n",
    "\n",
    "    df_motion_pt['Disp_to_mid'] = df_motion_pt['X']-half_width\n",
    "    df_vol_time['Disp_to_mid'] = df_vol_time['X']-half_width\n",
    "    if file_path_fluo:\n",
    "        # 如果有神经数据的地址，就加上\n",
    "        # 加入神经数据\n",
    "        # 用于寻找神经数据\n",
    "        f = os.path.basename(p_f)\n",
    "        f_decom_name = f.split(\"_\")\n",
    "        f_date = f_decom_name[0]\n",
    "        f_idx  = f_decom_name[-1]\n",
    "\n",
    "        cal_path = os.path.join(file_path_fluo, f_idx)\n",
    "        files = list(Path(cal_path).rglob(\"calcium_intensity.npy\"))\n",
    "        if not files:\n",
    "            calcium = False\n",
    "            print(\"未找到任何 calcium_trace.npy 文件。\")\n",
    "        elif len(files)>1:\n",
    "\n",
    "            print(\"找到以下文件：\")\n",
    "            for i, file in enumerate(files):\n",
    "                print(f\"{i}: {file}\")\n",
    "\n",
    "            index = int(input(f\"请输入要读取的文件索引 (0~{len(files) - 1}): \"))\n",
    "\n",
    "            if 0 <= index < len(files):\n",
    "                selected_file = files[index]\n",
    "                calcium_intensity = np.load(selected_file)\n",
    "                print(f\"成功加载: {selected_file}\")\n",
    "            else:\n",
    "                print(\"输入索引无效！\")\n",
    "        else:\n",
    "            calcium_intensity= np.load(files[0])\n",
    "        if calcium:\n",
    "            print('处理前文件大小:neuron*timestamp',calcium_intensity.shape)\n",
    "            # 将神经数据与行为数据合并\n",
    "            calcium_intensity_T = calcium_intensity.T\n",
    "            # 创建列名\n",
    "            col_neuron_names = [f\"{i+1}\" for i in range(calcium_intensity_T.shape[1])]\n",
    "            df_calcium = pd.DataFrame(calcium_intensity_T, columns=col_neuron_names)\n",
    "            # 按照行索引合并\n",
    "            df_cal_motion = pd.concat([df_calcium, df_vol_time], axis=1)\n",
    "\n",
    "            # 写出文件\n",
    "            if write:\n",
    "                # folder_name为当前数据日期批次\n",
    "                fn_motion_output = folder_name+'_MotionExtract.csv'\n",
    "                df_motion_pt.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "                print(f'写出抽取的运动参数文件到:{folder_name}/{fn_motion_output}')\n",
    "                fn_nt_output = folder_name+'_NeuralAlignMotionCal.csv'\n",
    "                df_cal_motion.to_csv(os.path.join(p_f, fn_nt_output), index = False)\n",
    "                print(f'写出与神经数据时间戳(包含钙数据)对应的运动参数文件到:{folder_name}/{fn_nt_output}')\n",
    "        else:\n",
    "            # 没有找到钙信号\n",
    "            if write:\n",
    "                # folder_name为当前数据日期批次\n",
    "                fn_motion_output = folder_name+'_MotionExtract.csv'\n",
    "                df_motion_pt.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "                print(f'写出抽取的运动参数文件到:{folder_name}/{fn_motion_output}')\n",
    "                fn_nt_output = folder_name+'_NeuralAlignMotion.csv'\n",
    "                df_vol_time.to_csv(os.path.join(p_f, fn_nt_output), index = False)\n",
    "                print(f'写出与神经数据时间戳(不包含钙数据)对应的运动参数文件到:{folder_name}/{fn_nt_output}')\n",
    "    else:\n",
    "        # 写出文件\n",
    "        if write:\n",
    "            # folder_name为当前数据日期批次\n",
    "            fn_motion_output = folder_name+'_MotionExtract.csv'\n",
    "            df_motion_pt.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "            print(f'写出抽取的运动参数文件到:{folder_name}/{fn_motion_output}')\n",
    "            fn_nt_output = folder_name+'_NeuralAlignMotion.csv'\n",
    "            df_vol_time.to_csv(os.path.join(p_f, fn_nt_output), index = False)\n",
    "            print(f'写出与神经数据时间戳(不包含钙数据)对应的运动参数文件到:{folder_name}/{fn_nt_output}')\n",
    "else:\n",
    "    print(f'{os.path.basename(os.path.normpath(p_f))}中缺少数据处理必须的文件如下：\\n' + \"\\n\".join(missing_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddb70e",
   "metadata": {},
   "source": [
    "可视化手动标注行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71dc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 提取所有行为文件csv，进行时间戳对齐\n",
    "f_b_l_ls = [f for f in os.listdir(file_path_beh_label) if '_beh' in f]\n",
    "for f in f_b_l_ls:\n",
    "    df_b = pd.read_csv(os.path.join(file_path_beh_label, f))\n",
    "    f_name = os.path.basename(f)\n",
    "    f_num = f_name.split('_')[0]\n",
    "    print(f'处理文件编号：{f_num}')\n",
    "    \n",
    "    # 在主文件夹中找到对应编号子文件夹，提取tracking文件创建时间\n",
    "    p_track_f = [os.path.join(file_path_all, f) for f in os.listdir(file_path_all) if\n",
    "                 (f_num in f)&(os.path.isdir(os.path.join(file_path_all,f)))]\n",
    "    c_time = os.path.getctime(p_track_f[0])\n",
    "    file_creation_datetime = pd.to_datetime(datetime.fromtimestamp(c_time))\n",
    "    df_b['Time'] = pd.to_datetime(df_b['Time'])\n",
    "    df_b['Time_diff'] = df_b['Time'] - file_creation_datetime\n",
    "    df_b['Time_diff_sec'] = df_b['Time_diff'].dt.total_seconds()\n",
    "    print(f'文件{f_name}的首个记录时间为{df_b.loc[0,\"Time\"]},\\n减去stage_data创建时间{file_creation_datetime}为{df_b.loc[0,\"Time_diff_sec\"]}')\n",
    "    # 写入文件编号为新列\n",
    "    df_b['Rep_id'] = f_num\n",
    "    df_b['Rep_id'] = df_b['Rep_id'].astype(str)\n",
    "    new_name = f_num+'_align.csv'\n",
    "    df_b.to_csv(os.path.join(file_path_beh_label, new_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有对齐后的行为文件csv\n",
    "df_ls = []\n",
    "f_b_align = [f for f in os.listdir(file_path_beh_label) if '_align' in f]\n",
    "for f in f_b_align:\n",
    "    df_i = pd.read_csv(os.path.join(file_path_beh_label, f),dtype={'Rep_id': str})\n",
    "    df_ls.append(df_i)\n",
    "df = pd.concat(df_ls, ignore_index=True)\n",
    "num_id = df.Rep_id.unique()\n",
    "# 定义行为模式映射\n",
    "behavior_map = {'None':0, 'Forward': 1, 'Reverse': 2, 'Turn': 3, 'Coil': 4, 'Pause':5}\n",
    "# colors = ['blue', 'red', 'green', 'orange', 'grey']  # 颜色对应行为\n",
    "# colors = ['#D1D6B6', '#F2E6CE', '#70635F','#C77D7D', '#D4A07B']\n",
    "# colors = ['#008BA1', '#BF0079', '#75245C','#005CA3', '#4D157D']\n",
    "# colors = ['#F2E77E', '#5D72A8', '#65CF61','#336131', '#1D3980']\n",
    "colors = ['#889A9C','#0742D9', '#fdca4e', '#EB1717','#73DE00', '#F5EBD7']\n",
    "cmap = ListedColormap(colors)\n",
    "behavior_col = ['Forward', 'Reverse', 'Turn', 'Coil', 'Pause']\n",
    "color_dict = {i:colors[i] for i in range(len(colors))}\n",
    "\n",
    "# 生成大图\n",
    "width = 10\n",
    "height= 0.5\n",
    "fig, ax = plt.subplots(len(num_id), 1, figsize=(width, height*len(num_id)))\n",
    "fig.suptitle(\"Behavior over Time\", fontsize = width*2, y = 1.25)\n",
    "for i, (idx, df_i) in enumerate(df.groupby('Rep_id')):\n",
    "    \n",
    "    # 找到每个时间点的行为模式（只含1的列）\n",
    "    df_i['Behavior'] = df_i[behavior_col].apply(lambda row: row.idxmax() if row.sum() >0 else 'None', axis=1)  # 获取行为模式列名\n",
    "    df_i['Behavior_idx'] = df_i['Behavior'].map(behavior_map)  # 转换为数值索引\n",
    "    df_i['Beh_color'] = df_i['Behavior_idx'].map(color_dict)   # 映射颜色方便之后作轨迹图\n",
    "    \n",
    "    df_i.to_csv(os.path.join(file_path_beh_label, str(idx)+'beh_label.csv'))\n",
    "    \n",
    "    # 画图\n",
    "    vectors = df_i['Behavior_idx'].values[np.newaxis, :]\n",
    "    # 生成颜色列表\n",
    "    color_vec = [color_dict.get(key, None) for key in df_i['Behavior_idx'].values]\n",
    "    bound = np.arange(-0.5, 6.5, 1)\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    sc = ax[i].imshow(vectors, \n",
    "                   aspect='auto', cmap=cmap, norm = norm, extent=[df_i['Time_diff_sec'].min(), df_i['Time_diff_sec'].max(), 0, 1])\n",
    "\n",
    "    # 设置横轴\n",
    "    ax[i].set_xlim([0, 30*60])\n",
    "    ax[i].tick_params(axis = 'x', labelsize = width*1.5)\n",
    "    ax[i].set_yticks([])  # 隐藏 y 轴\n",
    "    if i < len(num_id)-1:\n",
    "        ax[i].set_xticks([])\n",
    "    # xlabel标记编号\n",
    "    ax[i].set_ylabel(idx, fontsize = width*1.75, labelpad = width*2, rotation=0)\n",
    "#     ax[i].grid(True, axis = 'x', color='grey', alpha = 0.35, ls = 'dashed')\n",
    "    if i==0:\n",
    "        # 添加图例\n",
    "        # Create legend manually for color mapping\n",
    "        legend_labels = behavior_map.keys()\n",
    "        legend_handles = [mpatches.Patch(color=color, label=label) for color, label in zip(colors, legend_labels)]\n",
    "\n",
    "        # Add the legend with desired properties\n",
    "        ax[i].legend(handles=legend_handles, loc='upper center', bbox_to_anchor=(0.5, 2.3), \n",
    "                  ncol=len(legend_labels), frameon=False, markerscale=0.5, fontsize=width*1.25)\n",
    "\n",
    "        # Remove the axes' outer border\n",
    "    ax[i].spines['top'].set_visible(False)\n",
    "    ax[i].spines['right'].set_visible(False)\n",
    "    ax[i].spines['bottom'].set_visible(False)\n",
    "    ax[i].spines['left'].set_visible(False)\n",
    "    if i == len(num_id)-1:\n",
    "        ax[i].set_xlabel(\"Time (s)\", fontsize = width*2)\n",
    "        ax[i].spines['bottom'].set_visible(True)\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09486490",
   "metadata": {},
   "source": [
    "根据注释行为作轨迹图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afdb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # behavior_map = {'None':0, 'Forward': 1, 'Reverse': 2, 'Turn': 3, 'Coil': 4, 'Pause':5}\n",
    "# # colors = ['#889A9C','#0742D9', '#fdca4e', '#EB1717','#73DE00', '#F5EBD7']\n",
    "# # color_dict = {i:colors[i] for i in range(len(colors))}\n",
    "\n",
    "# f_b_anot = [f for f in os.listdir(file_path_beh_label) if 'beh_label' in f]\n",
    "# for f in f_b_anot:\n",
    "#     df_i = pd.read_csv(os.path.join(file_path_beh_label, f),dtype={'Rep_id': str})\n",
    "#     f_num = f.split('beh')[0]\n",
    "#     p_track_folder = [os.path.join(file_path_all, f) for f in os.listdir(file_path_all) if\n",
    "#                  (f_num in f)&(os.path.isdir(os.path.join(file_path_all,f)))][0]\n",
    "#     track_f = [os.path.join(p_track_folder,f) for f in os.listdir(p_track_folder) if 'MotionExtract' in f][0]\n",
    "#     df_motion = pd.read_csv(track_f)\n",
    "    \n",
    "#     for t in df_motion['Time']:\n",
    "#         diff_time = np.abs(df_i['Time_diff_sec']-t)\n",
    "#         nearest_idx = np.argmin(diff_time)\n",
    "#         df_motion['Nearest_idx'] = nearest_idx\n",
    "#         df_motion['Beh_color'] = df_i.loc[nearest_idx,'Beh_color']\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.scatter(df_motion['X'], df_motion['Y'], c = df_motion['Beh_color'], s = 0.8)\n",
    "#     ax.set_aspect(1)\n",
    "#     ax.set_title(f)\n",
    "#     plt.show()\n",
    "        \n",
    "f_b_anot = [f for f in os.listdir(file_path_beh_label) if 'beh_label' in f]\n",
    "\n",
    "for f in f_b_anot:\n",
    "    # 读取行为标签文件\n",
    "    df_i = pd.read_csv(os.path.join(file_path_beh_label, f), dtype={'Rep_id': str})\n",
    "    f_num = f.split('beh')[0]\n",
    "\n",
    "    # 获取对应的轨迹文件夹\n",
    "    p_track_folder = next(os.path.join(file_path_all, f) for f in os.listdir(file_path_all) \n",
    "                          if (f_num in f) and os.path.isdir(os.path.join(file_path_all, f)))\n",
    "    \n",
    "    # 读取运动轨迹文件\n",
    "    track_f = next(os.path.join(p_track_folder, f) for f in os.listdir(p_track_folder) if 'MotionExtract' in f)\n",
    "    print(track_f)\n",
    "    df_motion = pd.read_csv(track_f)\n",
    "\n",
    "    # **使用 `merge_asof()` 进行最近时间点匹配，提高效率**\n",
    "    df_i = df_i.sort_values('Time_diff_sec')  # 先按时间排序\n",
    "    df_motion = df_motion.sort_values('Time')  # 轨迹数据按时间排序\n",
    "\n",
    "    # **最近匹配最近的行为**\n",
    "    df_motion = pd.merge_asof(df_motion, df_i[['Time_diff_sec', 'Beh_color']], \n",
    "                              left_on='Time', right_on='Time_diff_sec', direction='nearest')\n",
    "\n",
    "    # 绘制轨迹图\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(df_motion['X'], df_motion['Y'], c=df_motion['Beh_color'], s=0.8)\n",
    "    ax.legend(handles=legend_handles, loc='upper center', bbox_to_anchor=(1.2, 0.9), \n",
    "                  ncol=1, frameon=False, markerscale=0.5, fontsize=width*1.25)\n",
    "\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_title(f)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd256b1",
   "metadata": {},
   "source": [
    "# （*）多文件可视化行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e70eb5",
   "metadata": {},
   "source": [
    "## 函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入所有一级文件夹列表，合并所有csv数据\n",
    "def combine_csv_to_csv(folder_ls, key_1='_mot_vid.csv', key_2='', key_3='', nokey_1='*', nokey_2='*', columns=[], t_inv_draw=15, d_inv_draw=5):\n",
    "    '''\n",
    "    合并指定文件夹中所有的子文件夹中的指定csv文件\n",
    "    输入folder_ls包含所有感兴趣的文件夹路径(一般每个路径是单天实验)\n",
    "    '''\n",
    "    p_f_ls = []\n",
    "    for folder in folder_ls:\n",
    "        # 对于所有1级文件夹地址，收起其中所有文件夹地址\n",
    "        p_f_ls += [os.path.join(folder, p_f) for p_f in os.listdir(folder) if \n",
    "                   (os.path.isdir(os.path.join(folder, p_f))&('trash' not in p_f)&(key_2 in p_f)&(key_3 in p_f)\n",
    "                    &(nokey_1 not in p_f)&(nokey_2 not in p_f))]\n",
    "        print(p_f_ls)\n",
    "    data_frames = []\n",
    "    for p_f in p_f_ls:\n",
    "        files =  [\n",
    "            f for f in os.listdir(p_f)\n",
    "            if f.endswith('.csv') and key_1 in f ]\n",
    "        if len(files) > 1:\n",
    "            # 说明可能是拼接文件，选择有concat的，否则跳过\n",
    "            file_sel_ls = [f for f in files if 'concat' in f]\n",
    "            if len(file_sel_ls)==0:\n",
    "                print(os.path.basename(p_f),'中没有符合条件的文件')\n",
    "                continue\n",
    "            else:\n",
    "                file_sel = file_sel_ls[0]\n",
    "        elif len(files) == 0:\n",
    "            print(os.path.basename(p_f)+'中无符合条件的文件')\n",
    "            continue\n",
    "        else:\n",
    "            file_sel = files[0]\n",
    "        print(file_sel,'\\n')\n",
    "        df = pd.read_csv(os.path.join(p_f, file_sel))\n",
    "        data_frames.append(df)\n",
    "    df_als = pd.concat(data_frames, ignore_index=True)\n",
    "    if 'Date_Con_Rep' in df_als.columns:\n",
    "        df_als = df_als.drop(['Date_Con_Rep'], axis=1)\n",
    "    # 合并Date, condition, rep_id以便于分组单个数据\n",
    "    df_als['Date_Con_Rep'] = df_als['Date'].astype(str)+'_'\n",
    "    for col in df_als.columns:\n",
    "        if 'Condition' in col:\n",
    "            if '0' in col:\n",
    "                df_als['Date_Con_Rep'] += df_als[col].astype(str)\n",
    "            else:\n",
    "                df_als['Date_Con_Rep']    += '-'+df_als[col].astype(str)\n",
    "\n",
    "    df_als['Date_Con_Rep'] += ('_'+df_als['Rep_id'].astype(str))\n",
    "        \n",
    "    return df_als"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_by_GroupCol(df, group_col, group_col_ls):\n",
    "    '''\n",
    "    根据group_col_ls排序group_col\n",
    "    '''\n",
    "    df[group_col] = pd.Categorical(df[group_col], categories=group_col_ls, ordered=True)\n",
    "    df.sort_values(by = [group_col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497f78e",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f16370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_ls = [r'Y:\\SZX\\BehaviorData\\20250823_WBI']\n",
    "\n",
    "key_word_2 = '1gNa'\n",
    "df_als = combine_csv_to_csv(folder_ls, key_2=key_word_2)\n",
    "\n",
    "# 设置一个存放所有输出图片的文件夹\n",
    "folder_fig_name = 'BehAnalysis'\n",
    "p_f_fig = os.path.join(folder_ls[0], folder_fig_name)\n",
    "if not os.path.isdir(p_f_fig):\n",
    "    os.mkdir(p_f_fig)\n",
    "save = 1\n",
    "# 排序\n",
    "# 排序\n",
    "# sort_col = 'Condition0'\n",
    "# df[sort_col] = pd.Categorical(df[sort_col], ordered=True)\n",
    "# df_als = df_als.sort_values(by = [sort_col,'Condition1', 'Vol_Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de78c2",
   "metadata": {},
   "source": [
    "## Visualization of Trajectory distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a9b37",
   "metadata": {},
   "source": [
    "### 轨迹分布大图(所有数据画在一个图上)\n",
    "+ 将所有包含单次数据的文件夹放在一个总文件夹中，file_path_all\n",
    "+ 依次读出所有单次数据，不合并为一个df作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f0743",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df_als.copy()\n",
    "fs = 20\n",
    "group_col = 'Date_Con_Rep'\n",
    "kw = key_word_2\n",
    "sort = False\n",
    "sort_col = 'Condition3'\n",
    "# 可视化所有轨迹\n",
    "figure_0,ax = plt.subplots(figsize = (10,10))\n",
    "# 定义要使用的 sequential colormaps\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'BuPu',\n",
    "             'YlOrBr', 'PuRd', 'YlOrRd', 'OrRd', 'RdPu',\n",
    "             'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "if sort:\n",
    "    # 排序输出，美观\n",
    "    df[sort_col] = pd.Categorical(df[sort_col], ordered=True)\n",
    "    df = df.sort_values(by = [sort_col,'Condition1', 'Vol_Time'])\n",
    "\n",
    "for i,(g,df_g) in enumerate(df.groupby(group_col, sort = False)):\n",
    "    \n",
    "    # 设置每个数据的colormap\n",
    "    orig_cmap = mpl.colormaps[colormaps[i%len(colormaps)]]\n",
    "    # 重新缩放 colormap，使最浅部分变深\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        \"truncated_cmap\", orig_cmap(np.linspace(0.3, 1, 256))  # 从 30% 处开始\n",
    "    )\n",
    "\n",
    "    ax.scatter(df_g['X'], df_g['Y'], c = df_g['Time'], cmap = new_cmap, \n",
    "               s = 1)\n",
    "    first_valid_idx = df_g['X'].first_valid_index()\n",
    "    \n",
    "    ax.scatter(df_g.loc[first_valid_idx, 'X'], df_g.loc[first_valid_idx, 'Y'], \n",
    "              s=30, c = 'r', marker='*')\n",
    "    legend_color = new_cmap(0.5)\n",
    "    ax.scatter([], [], color=legend_color, label=g, s=5, marker='o')  # 仅用于图例\n",
    "ax.axvline(x=32, color = 'grey', ls = 'dashed', alpha = 0.8, lw = 2)\n",
    "ax.axvline(x=8, color = 'red', ls = 'dashed', alpha = 0.8, lw = 2, label='border')\n",
    "# 定义图例的颜色\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), markerscale=2, fontsize=20)\n",
    "ax.set_title('Trajectories', fontsize = fs*1.5, pad= fs)\n",
    "ax.tick_params(axis = 'both', labelsize = fs)\n",
    "ax.set_xlabel('mm', fontsize = fs)\n",
    "ax.set_ylabel('mm', fontsize = fs)\n",
    "ax.set_xlim([-1,41])\n",
    "ax.set_aspect('equal')\n",
    "if save:\n",
    "    figure_0.savefig(os.path.join(p_f_fig, f'Trajectories of all data{kw}.png'), \n",
    "                    bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64d4c1",
   "metadata": {},
   "source": [
    "分条件画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a66906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_als.Rep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_als['Condition3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fec63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_als[(df_als['Condition3']=='8h')&(df_als['Rep_id']!=15)]\n",
    "fs = 20\n",
    "group_col = 'Date_Con_Rep'\n",
    "kw = key_word_2\n",
    "sort = False\n",
    "sort_col = 'Condition3'\n",
    "# 可视化所有轨迹\n",
    "figure_0,ax = plt.subplots(figsize = (10,10))\n",
    "# 定义要使用的 sequential colormaps\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'BuPu',\n",
    "             'YlOrBr', 'PuRd', 'YlOrRd', 'OrRd', 'RdPu',\n",
    "             'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "if sort:\n",
    "    # 排序输出，美观\n",
    "    df[sort_col] = pd.Categorical(df[sort_col], ordered=True)\n",
    "    df = df.sort_values(by = [sort_col,'Condition1', 'Vol_Time'])\n",
    "\n",
    "for i,(g,df_g) in enumerate(df.groupby(group_col, sort = False)):\n",
    "    \n",
    "    # 设置每个数据的colormap\n",
    "    orig_cmap = mpl.colormaps[colormaps[i%len(colormaps)]]\n",
    "    # 重新缩放 colormap，使最浅部分变深\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        \"truncated_cmap\", orig_cmap(np.linspace(0.3, 1, 256))  # 从 30% 处开始\n",
    "    )\n",
    "\n",
    "    ax.scatter(df_g['X'], df_g['Y'], c = df_g['Time'], cmap = new_cmap, \n",
    "               s = 1)\n",
    "    first_valid_idx = df_g['X'].first_valid_index()\n",
    "    \n",
    "    ax.scatter(df_g.loc[first_valid_idx, 'X'], df_g.loc[first_valid_idx, 'Y'], \n",
    "              s=30, c = 'r', marker='*')\n",
    "    legend_color = new_cmap(0.5)\n",
    "    ax.scatter([], [], color=legend_color, label=g, s=5, marker='o')  # 仅用于图例\n",
    "ax.axvline(x=32, color = 'grey', ls = 'dashed', alpha = 0.8, lw = 2)\n",
    "ax.axvline(x=8, color = 'red', ls = 'dashed', alpha = 0.8, lw = 2, label='border')\n",
    "# 定义图例的颜色\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), markerscale=2, fontsize=20)\n",
    "ax.set_title('Trajectories', fontsize = fs*1.5, pad= fs)\n",
    "ax.tick_params(axis = 'both', labelsize = fs)\n",
    "ax.set_xlabel('mm', fontsize = fs)\n",
    "ax.set_ylabel('mm', fontsize = fs)\n",
    "ax.set_xlim([-1,41])\n",
    "ax.set_aspect('equal')\n",
    "if save:\n",
    "    figure_0.savefig(os.path.join(p_f_fig, f'Trajectories of all data{kw}.png'), \n",
    "                    bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb915ad",
   "metadata": {},
   "source": [
    "#### 分子图画轨迹\n",
    "+ 将所有包含单次数据的文件夹放在一个总文件夹中，file_path_all\n",
    "+ 依次读出所有单次数据，不合并为一个df作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93490cc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df_als.copy()\n",
    "fs = 20\n",
    "kw = key_word_2\n",
    "group_col = 'Date_Con_Rep'\n",
    "sort_col = 'Condition0'\n",
    "interval = 300   # s\n",
    "sort = False\n",
    "# 定义要使用的 sequential colormaps, 获取颜色列表\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'BuPu',\n",
    "             'YlOrBr', 'PuRd', 'YlOrRd', 'OrRd', 'RdPu',\n",
    "             'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "if sort:\n",
    "    # 排序\n",
    "    df[sort_col] = pd.Categorical(df[sort_col], ordered=True)\n",
    "    df = df.sort_values(by = [sort_col,'Condition1', 'Time'])\n",
    "\n",
    "# 计算每个子图的高度比例\n",
    "height_ratios = []\n",
    "\n",
    "for g, df_g in df.groupby(group_col, sort=False):\n",
    "    y_min = df_g['Y'].min()\n",
    "    y_max = df_g['Y'].max()\n",
    "    y_range = y_max - y_min\n",
    "\n",
    "    # 增加 10% 的缓冲区\n",
    "    padding = 0.1 * y_range\n",
    "    y_min -= padding\n",
    "    y_max += padding\n",
    "    y_range_with_padding = y_max - y_min\n",
    "\n",
    "    height_ratios.append(y_range_with_padding)\n",
    "\n",
    "# 创建大图\n",
    "group_num = df[group_col].unique()\n",
    "figure_0, ax = plt.subplots(\n",
    "    len(group_num), 1, \n",
    "    figsize=(10, 3 * len(group_num)), \n",
    "    sharex=True,  # 共享 x 轴\n",
    "    gridspec_kw={'height_ratios': height_ratios}  # 子图高度自适应\n",
    ")\n",
    "\n",
    "# 找到所有子图中 Vol_Time 的最大值，作为 colorbar 的统一上限\n",
    "vmax = df.Time.max()\n",
    "\n",
    "# 绘制每个子图\n",
    "for i,(g,df_g) in enumerate(df.groupby(group_col, sort = False)):\n",
    "            \n",
    "    # 设置colormap \n",
    "    # 设置截断colormap\n",
    "    orig_cmap = mpl.colormaps[colormaps[i%len(colormaps)]]\n",
    "    # 重新缩放 colormap，使最浅部分变深\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        \"truncated_cmap\", orig_cmap(np.linspace(0.3, 1, 256))  # 从 30% 处开始\n",
    "    )\n",
    "    \n",
    "    # 绘制散点图\n",
    "    scatter = ax[i].scatter(df_g['X'], df_g['Y'], c=df_g['Time'], \n",
    "                            cmap=new_cmap, s=1, vmin=0, vmax=vmax)  # 统一 colorbar 上限\n",
    "    ax[i].axvline(x=20, ls='dashed', c='r')  # 添加红色虚线\n",
    "    \n",
    "    # 标记起始位置\n",
    "    first_valid_idx = df_g['X'].first_valid_index()\n",
    "    \n",
    "    ax[i].scatter(df_g.loc[first_valid_idx, 'X'], df_g.loc[first_valid_idx, 'Y'], c = 'r', marker = '*', s = 25)\n",
    "    \n",
    "    # 间隔指定时间打印时间节点文字\n",
    "    df_g_dn = df_g.dropna(subset=['X', 'Y', 'Time']).reset_index(drop = True)  # 删除包含 NaN 的行\n",
    "    \n",
    "    # 最后打印的时间序列元素是原始时间\n",
    "    time_series = np.arange(df_g_dn.Time.min(), df_g_dn.Time.max(), interval)\n",
    "    # 转为字符串，便于打印\n",
    "    time_list = list(np.int32(time_series))\n",
    "    time_list_str = list(map(str, time_list))\n",
    "    # 取完整时间序列，扩展成矩阵\n",
    "    time_vec = df_g_dn.Time\n",
    "    mat_time = np.tile(time_vec, (len(time_series), 1))\n",
    "    # 使用广播机制，将矩阵每一行减去时间序列对应元素\n",
    "    mat_time_subtracted = mat_time-time_series[:,np.newaxis]\n",
    "    # 返回每一行最小值的索引\n",
    "#     min_diff_idx = np.argmin(np.abs(mat_time_subtracted), axis=1)\n",
    "    min_diff_idx = np.abs(mat_time_subtracted).argmin(axis=1)\n",
    "#     ax[i].text(df_motion.loc[min_diff_idx, 'X'],df_motion.loc[min_diff_idx, 'Y'], time_list_str, c = 'r', fontsize = 5)\n",
    "    # zip将两个列表打包\n",
    "    for idx, time_str in zip(min_diff_idx, time_list_str):\n",
    "        ax[i].text(df_g_dn.loc[idx, 'X'], \n",
    "                   df_g_dn.loc[idx, 'Y'], \n",
    "                   time_str+'s', c='r', fontsize=10)\n",
    "\n",
    "    # 设置标题\n",
    "    ax[i].set_title(g, fontsize=20)\n",
    "    ax[i].set_xlim([-1,41])\n",
    "    # 设置 y 轴范围（增加缓冲区）\n",
    "    y_min = df_g['Y'].min()\n",
    "    y_max = df_g['Y'].max()\n",
    "    padding = 0.1 * (y_max - y_min)  # 10% 的缓冲区\n",
    "    ax[i].set_ylim(y_min - padding, y_max + padding)\n",
    "    ax[i].tick_params(axis = 'both', labelsize = 15)\n",
    "    # 设置子图宽高比\n",
    "    ax[i].set_aspect('equal')\n",
    "    ax[i].set_ylabel('mm', fontsize = 15)\n",
    "    if i == len(group_num)-1:\n",
    "        \n",
    "        ax[i].set_xlabel('mm', fontsize = 15)\n",
    "    # 为每个子图添加 colorbar\n",
    "    divider = make_axes_locatable(ax[i])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)  # colorbar 宽度为 5%，间距为 0.1\n",
    "    cbar = plt.colorbar(scatter, cax=cax)\n",
    "    cbar.set_label('Time(s)', fontsize = 15)\n",
    "\n",
    "# 调整子图间距\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "if save:\n",
    "    figure_0.savefig(os.path.join(p_f_fig, f'Trajectory of each data{kw}.png'), \n",
    "                    bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8e74a",
   "metadata": {},
   "source": [
    "#### 分子图画轨迹分布\n",
    "+ 根据时间段出图，每一行为单个重复组的数据，每一列表示时间窗\n",
    "+ 将所有包含单次数据的文件夹放在一个总文件夹中，file_path_all\n",
    "+ 依次读出所有单次数据，不合并为一个df作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数生成指定colormap指定位置的颜色的RGB和16进制代码列表\n",
    "def trans_colormap_to_RGB(colormaps, ratio):\n",
    "    '''\n",
    "    根据一组colormap序列得到指定位置的单一颜色，如ratio=0.5时取中位颜色\n",
    "    '''\n",
    "    # 用于存储中位颜色的 RGB 和 16进制代码\n",
    "    RGB_ls = []\n",
    "    hex_ls = []\n",
    "    # 遍历每个 colormap，计算中位颜色\n",
    "    for cmap_name in colormaps:\n",
    "        cmap = mpl.colormaps[cmap_name]\n",
    "        # 获取 colormap 中的颜色值（在 0 到 1 之间）\n",
    "        colors = cmap(np.linspace(0, 1, 256))  # 256 表示 colormap 中的颜色分辨率\n",
    "        # 计算中位数索引\n",
    "        idx = int(len(colors)*ratio)\n",
    "        rgb = colors[idx, :3]  # 获取 RGB (去除 alpha)\n",
    "\n",
    "        # 将 RGB 转换为 16 进制代码\n",
    "        hex_color = '#{:02x}{:02x}{:02x}'.format(int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))\n",
    "        \n",
    "        RGB_ls.append(rgb)\n",
    "        hex_ls.append(hex_color)\n",
    "        \n",
    "    return RGB_ls, hex_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画组间比较位置分布\n",
    "\n",
    "df = df_als.copy()\n",
    "fs = 20\n",
    "kw = key_word_2\n",
    "group_col = 'Date_Con_Rep'\n",
    "sort = False\n",
    "sort_col = 'Condition1'\n",
    "interval_min = 5  # min\n",
    "interval_s = interval_min*60 # s\n",
    "time_series = []\n",
    "bin_size = 2.5 # mm\n",
    "x_min, x_max = 0, 40  # 设定 X 的范围\n",
    "\n",
    "if sort:\n",
    "    # 排序\n",
    "    df[sort_col] = pd.Categorical(df[sort_col], ordered=True)\n",
    "    df = df.sort_values(by = [sort_col, 'Vol_Time'])\n",
    "\n",
    "# 定义要使用的 sequential colormaps\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'BuPu',\n",
    "             'YlOrBr', 'PuRd', 'YlOrRd', 'OrRd', 'RdPu',\n",
    "             'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "rgb_ls, hex_ls = trans_colormap_to_RGB(colormaps, 0.55)\n",
    "num_colormap = len(colormaps)\n",
    "fs = 25\n",
    "\n",
    "# 数据分组数量\n",
    "group_num = df[group_col].unique()\n",
    "\n",
    "# 找出最大时间\n",
    "max_time = df['Vol_Time'].max()\n",
    "print(f'所有数据中的最大时间{max_time}')\n",
    "\n",
    "# 根据最大时间分时间段间隔\n",
    "if not len(time_series):\n",
    "    time_series = np.arange(0, max_time, interval_s)\n",
    "# 分割X区间\n",
    "\n",
    "bins = list(np.arange(x_min, x_max + bin_size, bin_size))  # 创建区间边界\n",
    "bins_array = np.array(bins)\n",
    "labels = list((bins_array[:-1]+bins_array[1:])/2)\n",
    "# 使用 pd.cut() 将 X 分割成区间\n",
    "df['X_bin'] = pd.cut(df['X'], bins=bins,labels=labels, right=False)  # 右开区间 \n",
    "\n",
    "# 创建大图\n",
    "figure_0, ax = plt.subplots(\n",
    "    len(group_num), len(time_series)-1, \n",
    "    figsize=(5*(len(time_series)-1), 2.5 * len(group_num)), \n",
    "    sharex=True,  # 共享 x 轴\n",
    "    sharey=True  \n",
    ")\n",
    "\n",
    "\n",
    "# 绘制每个子图\n",
    "for i,(g,df_g) in enumerate(df.groupby(group_col)):\n",
    "    \n",
    "    for j in range(len(time_series)-1):\n",
    "        \n",
    "        df_g_t = df_g[(df_g['Vol_Time']>=time_series[j])&(df_g['Vol_Time']<time_series[j+1])]\n",
    "    \n",
    "        # 绘制柱状图\n",
    "        sum_timepoint = df_g_t['X_bin'].notna().count()\n",
    "        df_gb_xbin = df_g_t.groupby('X_bin').agg({'Vol_Time':'count'}).reset_index()\n",
    "        df_gb_xbin = df_gb_xbin.rename(columns={'Vol_Time':'count'})\n",
    "        df_gb_xbin['ratio_count'] = df_gb_xbin['count']/sum_timepoint\n",
    "        ax[i][j].bar(df_gb_xbin['X_bin'],df_gb_xbin['ratio_count'], width=2,color = rgb_ls[i%num_colormap])\n",
    "#         ax[i].axvline(x=20, ls='dashed', c='r')  # 添加红色虚线\n",
    "        ax[i][j].tick_params(axis = 'both', labelsize = fs)\n",
    "        if i == 0:\n",
    "            # 设置标题\n",
    "            begin_min = int(time_series[j]/60)\n",
    "            end_min = int(time_series[j+1]/60)\n",
    "            ax[i][j].set_title(f'{str(begin_min)}~{str(end_min)}min', fontsize=fs*1.25)\n",
    "        if j == 0:\n",
    "            y_label = '\\n'.join(g.split('_'))\n",
    "            ax[i][j].set_ylabel(y_label, fontsize = fs, rotation=60, labelpad = fs*1.5)\n",
    "        # 设置x轴刻度范围\n",
    "        # 设置 X 轴刻度间隔\n",
    "        ax[i][j].set_xticks(np.arange(x_min, x_max+1, 10))  # 设置刻度间隔为 5mm\n",
    "        ax[i][j].grid(True, color='grey', alpha = 0.5, ls = 'dashed')\n",
    "        \n",
    "        if i == len(group_num)-1:\n",
    "            ax[i][j].set_xlabel('mm', fontsize = fs, labelpad = fs*0.25)\n",
    "\n",
    "# 调整子图间距\n",
    "plt.subplots_adjust(hspace=0.2, wspace = 0.2)\n",
    "if save:\n",
    "    figure_0.savefig(os.path.join(p_f_fig, f'Pos Distribution of each worm{kw}.png'), \n",
    "                    bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b9bfb",
   "metadata": {},
   "source": [
    "## CTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe98bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度角速度随时间的变化\n",
    "\n",
    "\n",
    "df = df_als.copy()\n",
    "kw = key_word_2\n",
    "group_col = 'Condition2'\n",
    "# group_col = 'Date_Con_Rep'\n",
    "value_col = ['speed', 'agl_speed']\n",
    "group_col_ls=[]\n",
    "color_dict={}\n",
    "color_map = 'plasma'\n",
    "# t_inv_overtime = [0,30]\n",
    "t_max = 30    # min\n",
    "time_inv = 5  # min\n",
    "n_col = 2\n",
    "sharey=True\n",
    "fs=25\n",
    "y_lim=[]\n",
    "frame_rate = 20\n",
    "lw= 2\n",
    "folder=None\n",
    "'''\n",
    "比较value_col的均值随时间变化的组内差异（按照group_col分组，组内可能按照group_id不同比较）\n",
    "group_cols:用于分组的列的列表（可排序后再输入），相当于原本按照condition分组的conds\n",
    "group_col_ls用于排序\n",
    "value_col：指定可视化的参数列，比如CTX_left\n",
    "n_col: 自适应子图数量，n_col指定列数\n",
    "sharey=True\n",
    "'''\n",
    "\n",
    "# 根据所有group_col的数量生成一张大图\n",
    "if not len(group_col_ls):\n",
    "    # 如果指定了组列表，直接使用\n",
    "    group_col_ls = df[group_col].unique()\n",
    "\n",
    "# 去除turn\n",
    "df_run = df[df.Event==0]\n",
    "# 颜色\n",
    "if not color_dict:\n",
    "    color_dict  = map_color_dict(group_col_ls, color_map)\n",
    "    \n",
    "# 设置时间区间\n",
    "df_run,_ = set_time_intervals(df_run, t_max*60, time_inv*60)   \n",
    "df_time = df_run.copy()\n",
    "df_time['Period_label'] = df_time['Period_label'].astype(float)\n",
    "# df_time_p = df_time[(df_time['Period_label']>=t_inv_overtime[0]) & (df_time['Period_label']<=t_inv_overtime[1])]\n",
    "\n",
    "# 根据指定的group_col_ls排序\n",
    "if len(group_col_ls):\n",
    "    df_time = Sort_by_GroupCol(df_time, group_col,group_col_ls)\n",
    "# 可视化\n",
    "n_row = len(value_col)   # 每一行表示不同的可视化量\n",
    "n_col = len(group_col_ls)  # 每一列表示不同的分组\n",
    "\n",
    "fig, ax = plt.subplots(n_row, n_col, figsize=(5.5 * n_col, 5 * n_row), sharex=True, sharey=False)\n",
    "\n",
    "# 计算每一行(y变量)的全局y轴范围\n",
    "y_limits = {}\n",
    "for n, v_col in enumerate(value_col):\n",
    "    y_min, y_max = float('inf'), float('-inf')\n",
    "    for g, df_f in df_time.groupby(group_col):\n",
    "        df_mean_ctx_by_g = df_f.groupby(['Date_Con_Rep', 'Period_label']).agg({v_col: 'mean'}).reset_index()\n",
    "        y_min = min(y_min, df_mean_ctx_by_g[v_col].min())\n",
    "        y_max = max(y_max, df_mean_ctx_by_g[v_col].max())\n",
    "    y_limits[v_col] = (y_min-(y_max-y_min)*0.1, y_max+(y_max-y_min)*0.1)\n",
    "\n",
    "# 绘制图像\n",
    "for n, v_col in enumerate(value_col):\n",
    "    for i, (g, df_f) in enumerate(df_time.groupby(group_col)):\n",
    "\n",
    "        df_mean_ctx_by_g = df_f.groupby(['Date_Con_Rep', 'Period_label']).agg({v_col: 'mean'}).reset_index()\n",
    "\n",
    "        sns.lineplot(df_mean_ctx_by_g, x='Period_label', y=v_col, ax=ax[n][i],\n",
    "                     color=color_dict[g], markers=True, style='Date_Con_Rep',\n",
    "                     linewidth=lw)\n",
    "\n",
    "        ax[n][i].set_title(g, fontsize=fs * 1.25, pad=fs * 0.5)\n",
    "        ax[n][i].set_xlabel('Time(s)', fontsize=fs, labelpad=fs * 0.5)\n",
    "\n",
    "        # 统一每一行的 y 轴范围\n",
    "        ax[n][i].set_ylim(y_limits[v_col])\n",
    "\n",
    "        if n == 1:\n",
    "            ax[n][i].legend(loc='upper center', bbox_to_anchor=(0.5, -0.3),\n",
    "                            fontsize=fs * 0.85, ncol=1, frameon=False)\n",
    "        else:\n",
    "            ax[n][i].get_legend().remove()  # 移除 legend\n",
    "\n",
    "        ax[n][i].grid(True, linestyle=':', color='grey', alpha=0.6)\n",
    "        ax[n][i].tick_params(labelsize=fs * 0.85)\n",
    "\n",
    "        # 画 y=0\n",
    "        ax[n][i].axhline(y=0, color='r', linestyle='dashed', alpha=0.5)\n",
    "\n",
    "        # 共享 y 轴\n",
    "        if i > 0:\n",
    "            ax[n][i].sharey(ax[n][0])\n",
    "            ax[n][i].set_ylabel('')\n",
    "        else:\n",
    "            ax[n][i].set_ylabel(v_col, fontsize=fs, labelpad=fs * 0.5)\n",
    "\n",
    "if save:\n",
    "    # 保存图片\n",
    "    values = '_'.join(value_col)\n",
    "    title_name = f\"{values}_OverTimeWihtinG_{kw}.png\"\n",
    "    file_path = os.path.join(p_f_fig, title_name)\n",
    "    fig.savefig(file_path, bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861a6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CTX随时间变化\n",
    "\n",
    "df = df_als.copy()\n",
    "kw = key_word_2\n",
    "group_col = 'Condition2'\n",
    "# group_col = 'Date_Con_Rep'\n",
    "value_col = 'CTX_left'\n",
    "group_col_ls=[]\n",
    "color_dict={}\n",
    "color_map = 'plasma'\n",
    "# t_inv_overtime = [0,30]\n",
    "t_max = 30    # min\n",
    "time_inv = 5  # min\n",
    "n_col = 2\n",
    "sharey=True\n",
    "fs=25\n",
    "y_lim=[]\n",
    "frame_rate = 20\n",
    "lw= 2\n",
    "folder=None\n",
    "'''\n",
    "比较value_col的均值随时间变化的组内差异（按照group_col分组，组内可能按照group_id不同比较）\n",
    "group_cols:用于分组的列的列表（可排序后再输入），相当于原本按照condition分组的conds\n",
    "group_col_ls用于排序\n",
    "value_col：指定可视化的参数列，比如CTX_left\n",
    "n_col: 自适应子图数量，n_col指定列数\n",
    "sharey=True\n",
    "'''\n",
    "\n",
    "# 根据所有group_col的数量生成一张大图\n",
    "if not len(group_col_ls):\n",
    "    # 如果指定了组列表，直接使用\n",
    "    group_col_ls = df[group_col].unique()\n",
    "    \n",
    "# 子图数量与分组数量相同\n",
    "num_fig = len(group_col_ls)\n",
    "\n",
    "# 去除turn\n",
    "df_run = df[df.Event==0]\n",
    "# 颜色\n",
    "if not color_dict:\n",
    "    color_dict  = map_color_dict(group_col_ls, color_map)\n",
    "    \n",
    "# 设置时间区间\n",
    "df_run,_ = set_time_intervals(df_run, t_max*60, time_inv*60)   \n",
    "df_time = df_run.copy()\n",
    "df_time['Period_label'] = df_time['Period_label'].astype(float)\n",
    "# df_time_p = df_time[(df_time['Period_label']>=t_inv_overtime[0]) & (df_time['Period_label']<=t_inv_overtime[1])]\n",
    "\n",
    "# 根据指定的group_col_ls排序\n",
    "if len(group_col_ls):\n",
    "    df_time = Sort_by_GroupCol(df_time, group_col,group_col_ls)\n",
    "\n",
    "# 可视化\n",
    "n_row = 1\n",
    "n_col = num_fig\n",
    "fig,axes = plt.subplots(n_row, n_col, figsize = (5.5*n_col,5*n_row), sharex = True, sharey = sharey)\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i, (g, df_f) in enumerate(df_time.groupby(group_col)):\n",
    " \n",
    "    df_mean_ctx_by_g = df_f.groupby(['Date_Con_Rep', 'Period_label']).agg({value_col:'mean'}).reset_index()\n",
    "\n",
    "    sns.lineplot(df_mean_ctx_by_g,x='Period_label',\n",
    "                y=value_col, ax = ax[i],\n",
    "                color=color_dict[g], markers=True, style='Date_Con_Rep',\n",
    "                 linewidth=lw\n",
    "                )\n",
    "    ax[i].set_title(g, fontsize=fs*1.25, pad = fs)\n",
    "    ax[i].set_xlabel('Time(s)', fontsize=fs, labelpad = fs*0.5)\n",
    "    ax[i].legend(loc='upper center', bbox_to_anchor=(0.5, -0.3),fontsize=fs*0.85, ncol=1, frameon=False)\n",
    "    ax[i].grid(True,linestyle=':',color='grey',alpha=0.6)\n",
    "    ax[i].tick_params(labelsize=fs*0.85)\n",
    "    # 画y=0\n",
    "    ax[i].axhline(y=0, color='r', linestyle='dashed', alpha = 0.5)\n",
    "    ax[i].set_ylabel(value_col, fontsize=fs, labelpad = fs*0.5)\n",
    "    if y_lim:\n",
    "        ax[i].set_ylim(y_lim[0], y_lim[1])\n",
    "\n",
    "if save:\n",
    "#  保存图片\n",
    "    title_name = f'{kw}_CTX-leftOverTimeWihtinG.png'\n",
    "    file_path = os.path.join(p_f_fig, title_name)\n",
    "    plt.savefig(file_path,bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30b513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "half_width = 20  #mm\n",
    "df_als['Disp_to_mid'] = df_als['X']-half_width\n",
    "x_min, x_max = -20, 20\n",
    "dist_inv = 2.5   # mm\n",
    "dist_vec = np.arange(x_min,x_max,dist_inv)\n",
    "labels = [(dist_vec[i] + dist_vec[i + 1]) / 2 for i in range(len(dist_vec) - 1)]\n",
    "df_als['Disp_label'] = pd.cut(df_als['Disp_to_mid'], bins=dist_vec, labels=labels, right=False, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff783a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 距中线的平均距离随时间变化\n",
    "df = df_als.copy()\n",
    "kw = key_word_2\n",
    "group_col = 'Condition2'\n",
    "# group_col = 'Date_Con_Rep'\n",
    "value_col = 'Disp_to_mid'\n",
    "group_col_ls=[]\n",
    "color_dict={}\n",
    "color_map = 'plasma'\n",
    "# t_inv_overtime = [0,30]\n",
    "t_max = 30    # min\n",
    "time_inv = 5  # min\n",
    "n_col = 2\n",
    "sharey=True\n",
    "fs=25\n",
    "y_lim=[]\n",
    "frame_rate = 20\n",
    "lw= 2\n",
    "folder=None\n",
    "'''\n",
    "比较value_col的均值随时间变化的组内差异（按照group_col分组，组内可能按照group_id不同比较）\n",
    "group_cols:用于分组的列的列表（可排序后再输入），相当于原本按照condition分组的conds\n",
    "group_col_ls用于排序\n",
    "value_col：指定可视化的参数列，比如CTX_left\n",
    "n_col: 自适应子图数量，n_col指定列数\n",
    "sharey=True\n",
    "'''\n",
    "\n",
    "# 根据所有group_col的数量生成一张大图\n",
    "if not len(group_col_ls):\n",
    "    # 如果指定了组列表，直接使用\n",
    "    group_col_ls = df[group_col].unique()\n",
    "    \n",
    "# 子图数量与分组数量相同\n",
    "num_fig = len(group_col_ls)\n",
    "\n",
    "# 去除turn\n",
    "df_run = df[df.Event==0]\n",
    "# 颜色\n",
    "if not color_dict:\n",
    "    color_dict  = map_color_dict(group_col_ls, color_map)\n",
    "    \n",
    "# 设置时间区间\n",
    "df_run,_ = set_time_intervals(df_run, t_max*60, time_inv*60)   \n",
    "df_time = df_run.copy()\n",
    "df_time['Period_label'] = df_time['Period_label'].astype(float)\n",
    "# df_time_p = df_time[(df_time['Period_label']>=t_inv_overtime[0]) & (df_time['Period_label']<=t_inv_overtime[1])]\n",
    "\n",
    "# 根据指定的group_col_ls排序\n",
    "if len(group_col_ls):\n",
    "    df_time = Sort_by_GroupCol(df_time, group_col,group_col_ls)\n",
    "\n",
    "# 可视化\n",
    "n_row = 1\n",
    "n_col = num_fig\n",
    "fig,axes = plt.subplots(n_row, n_col, figsize = (5.5*n_col,5*n_row), sharex = True, sharey = sharey)\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i, (g, df_f) in enumerate(df_time.groupby(group_col)):\n",
    " \n",
    "    df_mean_ctx_by_g = df_f.groupby(['Date_Con_Rep', 'Period_label']).agg({value_col:'mean'}).reset_index()\n",
    "\n",
    "    sns.lineplot(df_mean_ctx_by_g,x='Period_label',\n",
    "                y=value_col, ax = ax[i],\n",
    "                color=color_dict[g], markers=True, style='Date_Con_Rep',\n",
    "                 linewidth=lw\n",
    "                )\n",
    "    ax[i].set_title(g, fontsize=fs*1.25, pad = fs)\n",
    "    ax[i].set_xlabel('Time(s)', fontsize=fs, labelpad = fs*0.5)\n",
    "    ax[i].legend(loc='upper center', bbox_to_anchor=(0.5, -0.3),fontsize=fs*0.85, ncol=1, frameon=False)\n",
    "    ax[i].grid(True,linestyle=':',color='grey',alpha=0.6)\n",
    "    ax[i].tick_params(labelsize=fs*0.85)\n",
    "    # 画y=0\n",
    "    ax[i].axhline(y=0, color='r', linestyle='dashed', alpha = 0.5)\n",
    "    ax[i].set_ylabel(value_col, fontsize=fs, labelpad = fs*0.5)\n",
    "    if y_lim:\n",
    "        ax[i].set_ylim(y_lim[0], y_lim[1])\n",
    "\n",
    "if save:\n",
    "#  保存图片\n",
    "    title_name = f'{kw}_CTX-leftOverTimeWihtinG.png'\n",
    "    file_path = os.path.join(p_f_fig, title_name)\n",
    "    plt.savefig(file_path,bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0033df",
   "metadata": {},
   "source": [
    "# 神经数据清洗\n",
    "1. 通过可视化神经trace裁标记明显异常的区间\n",
    "+ Z轴严重偏离\n",
    "+ 追丢\n",
    "+ 线虫瘫痪，运动状态异常\n",
    "\n",
    "2. 标记信号异常的神经元\n",
    "+ 无信号神经元\n",
    "+ 假信号(信号波动过于剧烈)的神经元\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595156a",
   "metadata": {},
   "source": [
    "## 可视化Calcium traces/Behavior选择异常时间区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_calcium_curve(\n",
    "    calcium_intensity, \n",
    "    smooth_kernel=None, \n",
    "    save=True, \n",
    "    scale=1, \n",
    "    percentile_threshold=None  # 用 99 分位筛选阈值\n",
    "):\n",
    "    colors = ['orangered', 'blue', 'limegreen', 'purple', 'gold', 'cyan', 'magenta', 'coral', 'skyblue', 'orange']\n",
    "    import cv2\n",
    "\n",
    "    plot_idx = 0  # 紧凑排列\n",
    "    yticks_labels = []  # 原 neuron ID\n",
    "\n",
    "    # 先统计要画的 neuron 索引\n",
    "    keep_indices = []\n",
    "    for i in range(calcium_intensity.shape[0]):\n",
    "        line_data = calcium_intensity[i] * scale\n",
    "        if percentile_threshold is not None:\n",
    "            perc99 = np.percentile(line_data, 99)\n",
    "            if perc99 <= percentile_threshold:\n",
    "                continue\n",
    "        keep_indices.append(i)\n",
    "\n",
    "    # 根据保留数量动态计算画布大小\n",
    "    num_to_plot = len(keep_indices)\n",
    "    fig_h = int(10 * round(num_to_plot / 10)) if num_to_plot > 0 else 5\n",
    "    fig_w = int(0.7 * round(calcium_intensity.shape[1] / 100))\n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "\n",
    "    # 再画图\n",
    "    for i in keep_indices:\n",
    "        line_data = calcium_intensity[i] * scale\n",
    "        color = colors[plot_idx % len(colors)]\n",
    "\n",
    "        if smooth_kernel:\n",
    "            smooth_line_data = cv2.blur(calcium_intensity[i],  (1,smooth_kernel)) * scale\n",
    "        else:\n",
    "            smooth_line_data = cv2.blur(calcium_intensity[i], (1, 7)) * scale\n",
    "\n",
    "        plt.plot(smooth_line_data + plot_idx, color=color, linestyle='-', label='A' if plot_idx == 0 else \"\", linewidth=1)\n",
    "        plt.scatter(\n",
    "            x=np.arange(line_data.shape[0]), \n",
    "            y=line_data + plot_idx, \n",
    "            color=color, s=1, \n",
    "            label='A' if plot_idx == 0 else \"\"\n",
    "        )\n",
    "\n",
    "        # if i in bound:\n",
    "        #     plt.axhline(y=plot_idx - 0.2, color='r', linestyle='dashdot', linewidth=4)\n",
    "\n",
    "        yticks_labels.append(i)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 坐标轴设置\n",
    "    plt.xlabel(\"Time point\", fontsize=20)\n",
    "    plt.ylabel(\"Intensity\", fontsize=20)\n",
    "    plt.ylim(bottom=-1, top=plot_idx + 1)\n",
    "    plt.yticks(np.arange(0, plot_idx, 1), labels=yticks_labels, fontsize=20)\n",
    "    plt.xticks(np.arange(0, calcium_intensity.shape[1] + 1, 200), fontsize=20, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    print(plot_idx)\n",
    "    # 保存文件\n",
    "    if save:\n",
    "        filename = \"cluster_calcium_curve\"\n",
    "        if smooth_kernel:\n",
    "            filename += f\"(smooth_{smooth_kernel})\"\n",
    "        if percentile_threshold is not None:\n",
    "            filename += f\"_p99_{percentile_threshold}\"\n",
    "        plt.savefig(f\"{signal_save_path}/{filename}.png\")\n",
    "        \n",
    "    return keep_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f8d0f",
   "metadata": {},
   "source": [
    "# 标记前进后退和Turning行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035aff5b",
   "metadata": {},
   "source": [
    "## 函数定义\n",
    "+ 找事件起始点\n",
    "+ 开闭操作\n",
    "+ 保留正常的事件epoch\n",
    "+ 可视化事件和behavioral traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d70c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_points(forward_reversal_series):\n",
    "    # 创建一个 DataFrame，只包含原始列，方便处理\n",
    "    df = pd.DataFrame({'forward_reversal_series': forward_reversal_series})\n",
    "#     df['rev_start'] = 0\n",
    "\n",
    "    # 差分找变化点\n",
    "    df['diff_qr'] = df['forward_reversal_series'].diff()\n",
    "#     plt.plot(diff_qr)\n",
    "#     print(len(df.loc[diff_qr == 1, 'rev_start']), len(df.loc[diff_qr == 1, 'rev_start'])/len(df))\n",
    "    # 标记起始和结束\n",
    "    df.loc[:,'rev_start'] = np.nan\n",
    "    df.loc[df['diff_qr'] == 1, 'rev_start'] = 1\n",
    "    df.loc[df['diff_qr'] == -2, 'rev_start'] = 2    # 从nan转到reverse\n",
    "    df.loc[df['diff_qr'] == -1, 'rev_start'] = 3\n",
    "    df.loc[df['diff_qr'] == -3, 'rev_start'] = 4    # 从nan转到forward\n",
    "    # 特殊情况处理：开头或结尾就是 1\n",
    "#     if df['quick_reversal'].iloc[0] == 1:\n",
    "#         df.loc[df.index[0], 'quick_reversal_point'] = 1\n",
    "#     if df['quick_reversal'].iloc[-1] == 1:\n",
    "#         df.loc[df.index[-1], 'quick_reversal_point'] = 2\n",
    "\n",
    "    return df['rev_start'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e94838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Opening_Closing(vector,structure_size_open=10, structure_size_close=5):\n",
    "    '''利用开操作删除和合并后退\n",
    "    vector: 一维向量\n",
    "    structure_size的第一和第二个值分别控制开和闭的大小'''\n",
    "    # 可视化重定向的位置，找到索引\n",
    "#     vector = df_PCA_motion.forward.values\n",
    "\n",
    "    # 将一维向量扩展为二维（例如 1 行或 1 列）\n",
    "    heatmap_data = vector[np.newaxis, :]  # 变为 1 行 N 列\n",
    "    print(heatmap_data)\n",
    "    # Define two colors\n",
    "    colors = ['blue', 'yellow']\n",
    "    # Create a ListedColormap\n",
    "    two_color_cmap = ListedColormap(colors)\n",
    "\n",
    "    # Define the size of the structuring element\n",
    "#     structure_size_open = 10\n",
    "    # Perform opening\n",
    "    '''这里opening的效果是合并！'''\n",
    "    opened_vector = grey_opening(heatmap_data, size=structure_size_open)\n",
    "\n",
    "    # Perform closing\n",
    "    '''这里的close的效果是擦除！'''\n",
    "    vector = grey_closing(opened_vector, size=structure_size_close)\n",
    "\n",
    "#     plt.figure(figsize=(20, 1))  # 设置热图大小\n",
    "#     plt.imshow(vector, cmap=two_color_cmap, aspect='auto')  # 选择 colormap\n",
    "#     plt.colorbar(label='Value')  # 添加颜色条\n",
    "#     plt.title('Removed small turnings')\n",
    "#     plt.xticks()  # 设置x轴刻度\n",
    "#     plt.yticks([])  # 隐藏y轴刻度\n",
    "#     # plt.title(\"Heatmap of 1D Vector\")\n",
    "#     plt.show()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_event_segments(df, start_indices, event_col = \"forward_opcl\",\n",
    "                          accept_ab_end = True,\n",
    "                          output_col='forward_sel'):\n",
    "    \"\"\"\n",
    "    根据开始和结束标记对DataFrame进行段落标注。\n",
    "    注意这个代码需要匹配对于forward-reverse transition的四种情况的标记语法\n",
    "\n",
    "    参数：\n",
    "    - start_flags：开始的标记值列表\n",
    "    - accept_ab_end: 接收异常结尾\n",
    "    - output_col：新列的名字\n",
    "    - debug：是否打印调试信息\n",
    "    \"\"\"\n",
    "    # 初始化输出列\n",
    "    df[output_col] = 0\n",
    "\n",
    "    # 获取 end 索引及标签\n",
    "\n",
    "    end_nor_indices = df[df['rev_start_opcl']==3].index\n",
    "    end_abnor_indices = df[df['rev_start_opcl']==4].index\n",
    "    \n",
    "    rev_end_idx = []\n",
    "    for s_idx in start_indices:\n",
    "        nearest_nor_end = end_nor_indices[bisect.bisect_left(end_nor_indices, s_idx)]\n",
    "        nearest_abnor_end = end_abnor_indices[bisect.bisect_left(end_abnor_indices, s_idx)]\n",
    "        print('起始index：',s_idx)\n",
    "#         if not end_candidates:\n",
    "#             if debug:\n",
    "#                 print(f'⚠ No end found for start at {s_idx}')\n",
    "#             continue\n",
    "        if nearest_abnor_end <= nearest_nor_end:\n",
    "            print(f'abnormal{nearest_abnor_end} < normal:{nearest_nor_end}')\n",
    "            # 异常结束\n",
    "            if accept_ab_end:\n",
    "                df.loc[s_idx+1:nearest_abnor_end,output_col] = 1\n",
    "                rev_end_idx.append(nearest_abnor_end)\n",
    "                print('保存abnormal起始结束：',s_idx, nearest_abnor_end)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            df.loc[s_idx+1:nearest_nor_end,output_col] = 1\n",
    "            rev_end_idx.append(nearest_nor_end)\n",
    "            print('保存normal起始结束:', s_idx, nearest_nor_end)\n",
    "    return df, rev_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_custom_timeseries(\n",
    "    df,\n",
    "    time_col,\n",
    "    line_cols=None,\n",
    "    line_color_by_col=None,\n",
    "    scatter_col=None,\n",
    "    colorbar1_col=None,\n",
    "    colorbar2_col=None,\n",
    "    nanbar1_col=None,\n",
    "    vertical_line_col=None,\n",
    "    triangle_times=None,\n",
    "    plot_line=None,\n",
    "    plot_scatter=None,\n",
    "    plot_colorbar1=None,\n",
    "    plot_colorbar2=None,\n",
    "    plot_colorbar3=None,\n",
    "    plot_vertical_lines=None,\n",
    "    plot_triangles=None,\n",
    "    figsize=(20, 4),\n",
    "    title=None,\n",
    "    time_range=None\n",
    "):\n",
    "    def col_should_plot(col, flag):\n",
    "        if flag is None:\n",
    "            return col is not None and col in df.columns\n",
    "        return flag\n",
    "\n",
    "    if time_range:\n",
    "        start, end = time_range\n",
    "        df = df[(df[time_col] >= start) & (df[time_col] <= end)].reset_index(drop=True)\n",
    "\n",
    "    draw_bar1 = col_should_plot(colorbar1_col, plot_colorbar1)\n",
    "    draw_bar2 = col_should_plot(colorbar2_col, plot_colorbar2)\n",
    "    draw_bar3 = col_should_plot(nanbar1_col, plot_colorbar3)\n",
    "    num_bars = int(draw_bar1) + int(draw_bar2) + int(draw_bar3)\n",
    "\n",
    "#     fig, ax_main = plt.subplots(figsize=figsize, constrained_layout=True)\n",
    "    fig,ax = plt.subplots(figsize=figsize)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "#     if num_bars > 0:\n",
    "    fig.set_constrained_layout(False)\n",
    "    gs = GridSpec(len(line_cols)+1, 1, height_ratios=[1.0]*len(line_cols)+[0.3 * num_bars], wspace=0.3, hspace=0.25, figure=fig)\n",
    "    # 可能画多个折线图\n",
    "    ax_bar = fig.add_subplot(gs[-1], sharex=ax)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.set_yticks([])                  # 去除刻度\n",
    "    ax.set_yticklabels([])            # 去除刻度标签\n",
    "    ax.tick_params(axis='y', left=False)  # 去除刻度线\n",
    "    # Plot line\n",
    "    i = 0\n",
    "    for line_col in line_cols:\n",
    "        ax_main = fig.add_subplot(gs[i])\n",
    "        if isinstance(line_col, str):\n",
    "            line_col_list = [line_col]\n",
    "        else:\n",
    "            line_col_list = line_col\n",
    "\n",
    "        for col in line_col_list:\n",
    "            if line_color_by_col and line_color_by_col in df.columns:\n",
    "                \n",
    "                # 分段上色\n",
    "                segment_time, segment_value, current_color = [], [], None\n",
    "                handles_drawn = {'red': False, 'blue': False}\n",
    "\n",
    "                for t, y, c in zip(df[time_col], df[col], df[line_color_by_col]):\n",
    "                    if current_color is None:\n",
    "                        current_color = c\n",
    "                    if c != current_color:\n",
    "                        if segment_time:\n",
    "                            color = 'red' if current_color == 1 else 'blue'\n",
    "                            label = 'reversal' if color == 'red' and not handles_drawn['red'] else None\n",
    "                            ax_main.plot(segment_time, segment_value, color=color)\n",
    "                            handles_drawn[color] = True\n",
    "                        segment_time, segment_value = [t], [y]\n",
    "                        current_color = c\n",
    "                    else:\n",
    "                        segment_time.append(t)\n",
    "                        segment_value.append(y)\n",
    "                if segment_time:\n",
    "                    color = 'red' if current_color == 1 else 'blue'\n",
    "                    label = 'reversal' if color == 'red' and not handles_drawn['red'] else None\n",
    "                    ax_main.plot(segment_time, segment_value, color=color)\n",
    "            else:\n",
    "                ax_main.plot(df[time_col], df[col])\n",
    "            if \"smoothed_\" in col:\n",
    "                col = col.replace(\"smoothed_\",\"\")\n",
    "            ax_main.set_ylabel('\\n'.join(col.split('_')), rotation=0,labelpad = 15, va='center')\n",
    "            if time_range:\n",
    "                ax_main.set_xlim(time_range)\n",
    "            else:\n",
    "#                 print(df[time_col].min(), df[time_col].max())\n",
    "                ax_main.set_xlim(df[time_col].min(), df[time_col].max())\n",
    "            ax_main.set_xticks([])\n",
    "            \n",
    "            # Triangle markers\n",
    "            if len(triangle_times) and (plot_triangles is None or plot_triangles):\n",
    "                y_min, y_max = ax_main.get_ylim()\n",
    "                triangle_y = y_max + 0.05 * (y_max - y_min)\n",
    "                for t in triangle_times:\n",
    "                    ax_main.plot(df.loc[t,time_col], triangle_y, marker='v', color='purple', markersize=8)\n",
    "                    ax_main.axvline(x=df.loc[t,time_col], color='black', linestyle='--', linewidth=1)\n",
    "                ax_main.set_ylim(y_min, triangle_y + 0.1 * (y_max - y_min))\n",
    "        i += 1\n",
    "\n",
    "    # Plot scatter\n",
    "    if col_should_plot(scatter_col, plot_scatter):\n",
    "        ax_main.scatter(df[time_col], df[scatter_col], color='r', s=2, label=scatter_col)\n",
    "\n",
    "    # Vertical lines\n",
    "    if col_should_plot(vertical_line_col, plot_vertical_lines):\n",
    "        for vt in df[df[vertical_line_col] == 1][time_col]:\n",
    "            ax_main.axvline(x=vt, color='green', linestyle='--', linewidth=1)\n",
    "    # Colorbar\n",
    "    handles, labels = ax_main.get_legend_handles_labels()\n",
    "    if ax_bar:\n",
    "        bar_data = []\n",
    "        if draw_bar3:\n",
    "            nan_mask = df[nanbar1_col].isna().values\n",
    "            bar_data.append((np.where(nan_mask, 1, 0)[np.newaxis, :], mcolors.ListedColormap(['black', 'red'])))\n",
    "        if draw_bar2:\n",
    "            vals2 = df[colorbar2_col].fillna(0).astype(int).values[np.newaxis, :]\n",
    "            bar_data.append((vals2, mcolors.ListedColormap(['black', 'yellow'])))\n",
    "        if draw_bar1:\n",
    "            vals1 = df[colorbar1_col].fillna(0).astype(int).values[np.newaxis, :]\n",
    "            bar_data.append((vals1, mcolors.ListedColormap(['black', 'yellow', 'red'])))\n",
    "\n",
    "        bar_height = 1.0 / num_bars\n",
    "        for i, (vals, cmap) in enumerate(bar_data):\n",
    "            extent = [df[time_col].iloc[0], df[time_col].iloc[-1], i * bar_height, (i + 1) * bar_height]\n",
    "            ax_bar.imshow(vals, aspect='auto', cmap=cmap, extent=extent)\n",
    "\n",
    "        bar_labels = []\n",
    "        if draw_bar3: bar_labels.append('untrust')\n",
    "        if draw_bar2: bar_labels.append('reversal(after)')\n",
    "        if draw_bar1: bar_labels.append('reversal(before)')\n",
    "\n",
    "        ax_bar.set_yticks([(i + 0.5) / num_bars for i in range(num_bars)])\n",
    "        ax_bar.set_yticklabels(bar_labels)\n",
    "        ax_bar.set_ylim(0, 1)\n",
    "        ax_bar.tick_params(left=False)\n",
    "        ax_bar.set_xlabel('Time')\n",
    "\n",
    "        # Legend patch\n",
    "        patches = []\n",
    "        if draw_bar1: \n",
    "            patches.append(Patch(color='yellow', label='reversal'))\n",
    "        if draw_bar2: patches.append(Patch(color='red', label='untrusted frames'))\n",
    "        handles += patches\n",
    "        labels += [p.get_label() for p in patches]\n",
    "\n",
    "    if handles:\n",
    "#         ax_main.legend(handles=handles, labels=labels, loc='upper right')\n",
    "        ax_main.legend(loc='upper left',handles=handles, labels=labels, bbox_to_anchor=(1.01, 0.85), borderaxespad=0.)\n",
    "\n",
    "#     ax_main.set_ylabel('Value')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    return fig, ax_main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea016708",
   "metadata": {},
   "source": [
    "## 数据导入与行为事件计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b6464",
   "metadata": {},
   "source": [
    "### Import data and smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ebd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_f = r'Z:\\data space+\\C. elegans chemotaxis\\2025_WBI\\good_WBI\\no_binary\\20250116_4.5g-ov_05'\n",
    "\n",
    "# 行为数据导入\n",
    "'''激光段的行为数据，帧率为30Hz左右'''\n",
    "# if not midline info, use mot_vid_cut\n",
    "mot_mid_cut = [f for f in os.listdir(p_f) if 'mot_midline_cut' in f elif 'mot_vid_cut' in f][0]\n",
    "df_motion_cut = pd.read_csv(os.path.join(p_f, mot_mid_cut))\n",
    "# 行为数据进行平滑和分割前进后退\n",
    "# 对数据进行平滑\n",
    "# 平滑速度\n",
    "window_size = 15\n",
    "# 计算移动平均值\n",
    "df_motion_cut['smoothed_speed'] = df_motion_cut['speed'].rolling(window=window_size, min_periods=1).mean()\n",
    "# 平滑角速度\n",
    "window_size = 15\n",
    "# 计算移动平均值\n",
    "df_motion_cut['smoothed_agl_speed'] = df_motion_cut['agl_speed'].rolling(window=window_size, min_periods=1).mean()\n",
    "# 平滑ctx\n",
    "window_size = 15\n",
    "# 计算移动平均值\n",
    "df_motion_cut['smoothed_CTX'] = df_motion_cut['CTX_left'].rolling(window=window_size, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd361f7",
   "metadata": {},
   "source": [
    "### Calculate curvature using backbone info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f23884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算头部曲率head curvature\n",
    "\n",
    "# df_motion_cut['all_paths'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4aaa0b",
   "metadata": {},
   "source": [
    "### visualize angle_m and separate forward and reverse by threshold\n",
    "+ separate forward and reverse\n",
    "+ calculate heading velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1948bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据角速度分隔前进后退状态\n",
    "agl_threshold = 100\n",
    "df_motion_cut.loc[:,'forward'] = 0\n",
    "df_motion_cut.loc[df_motion_cut['angle_m']>=agl_threshold,'forward'] = 1\n",
    "df_motion_cut.loc[df_motion_cut['angle_m'].isna(),'forward'] = np.nan\n",
    "# df_PCA_motion.loc[df_PCA_motion['choose_frame']!=1,'forward'] = 3\n",
    "label_rev = get_start_points(df_motion_cut['forward'].values)\n",
    "df_motion_cut['rev_start'] = label_rev\n",
    "# 计算前向速度\n",
    "df_motion_cut['cos_m'] = np.cos(np.radians(df_motion_cut['angle_m']))\n",
    "df_motion_cut['head_velocity'] = df_motion_cut['cos_m']*df_motion_cut['smoothed_speed']\n",
    "df_motion_cut['head_speed'] = df_motion_cut['head_velocity'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对行为数据分割的前进后退进行开闭操作\n",
    "# 开操作：去除噪声，structure_size = 0.65s * 30 = 19.5 ~ 20\n",
    "# 闭操作：合并两个 reversal: structure_size = 40\n",
    "structure = np.ones(50)\n",
    "opened_first = binary_opening(df_motion_cut['forward'].astype(bool), structure=structure).astype(int)\n",
    "\n",
    "structure = np.ones(40)\n",
    "closed = binary_closing(opened_first, structure=structure).astype(int)\n",
    "\n",
    "# opened = binary_opening(closed, structure=structure).astype(int)\n",
    "df_motion_cut['forward_opcl'] = closed\n",
    "df_motion_cut.loc[df_motion_cut['angle_m'].isna(),'forward_opcl'] = 3  # 还是将没有通过筛选的处理为3，灰色\n",
    "# 开闭后对节点进行检查\n",
    "label_rev = get_start_points(df_motion_cut['forward_opcl'].values)\n",
    "df_motion_cut['rev_start_opcl'] = label_rev\n",
    "rev_start_idx = df_motion_cut[df_motion_cut['rev_start_opcl']==1].index\n",
    "rev_end_idx = df_motion_cut[df_motion_cut['rev_start_opcl']==3].index\n",
    "\n",
    "event_idx_ls = rev_start_idx\n",
    "draw_window = 100  # 在节点前后画图的时间窗\n",
    "\n",
    "for i, idx in enumerate(event_idx_ls):\n",
    "    start_idx = idx-draw_window\n",
    "    end_idx = idx+draw_window\n",
    "    df_i = df_motion_cut.loc[start_idx:end_idx, :]\n",
    "    plt.figure()\n",
    "    plt.grid(ls='dashed', c = 'grey', alpha = 0.5)\n",
    "    plt.axis('equal')\n",
    "    plt.title('event_idx:'+str(idx))\n",
    "    plt.scatter(df_i.loc[idx,'X'], df_i.loc[idx,'Y'], s = 100, marker = '*', color='green')\n",
    "    colors = df_i['forward_opcl'].map({0: 'blue', 1: 'red', 3:'grey'})\n",
    "    plt.scatter(df_i['X'], df_i['Y'],c = colors ,s=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a380561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rev_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95246303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0116-05 start point删除: 8788,21257,22302,23661,27812\n",
    "rev_start_idx = [ 846,  2876,  3863,  4627,  7388,  8386,  9327, 11892, 12063,\n",
    "       13455, 16554, 19578, 20896, 23113, 26328, 27482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e6809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_motion_cut_sel, rev_end_idx = label_event_segments(df_motion_cut, rev_start_idx, event_col = \"forward_opcl\",\n",
    "                          accept_ab_end = True,\n",
    "                          output_col='forward_sel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化行为数据与前进后退分割(30Hz)\n",
    "rev_end_idx = df_motion_cut[df_motion_cut['rev_start_opcl']==3].index\n",
    "rev_start_idx = df_motion_cut[df_motion_cut['rev_start_opcl']==1].index\n",
    "\n",
    "df_motion_cut_reset = df_motion_cut.reset_index().rename(columns={'index': 'Index'})\n",
    "\n",
    "fig, ax = plot_custom_timeseries(\n",
    "    df=df_motion_cut_reset,\n",
    "    time_col='Index',\n",
    "    line_cols=[\"smoothed_CTX\", \"smoothed_agl_speed\", \"smoothed_speed\",'head_velocity'],\n",
    "    colorbar1_col='forward_opcl',\n",
    "    colorbar2_col='forward_sel',\n",
    "    nanbar1_col='angle_m',\n",
    "#     time_range=(750,900),\n",
    "    triangle_times=rev_start_idx,       # 指定几个时间点画三角形\n",
    "    title='Motion(30Hz) Labeled with Reversal End'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929509e",
   "metadata": {},
   "source": [
    "# 备用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a871ffc",
   "metadata": {},
   "source": [
    "特殊文件单独处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205f6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 单独处理：没有标记四个顶点坐标\n",
    "# # 输入每一对txt文件所在的的文件夹\n",
    "# # file_paths = [r'Z:\\data space+\\C. elegans chemotaxis\\20250219_WBI\\20250219-1']\n",
    "# file_paths = [r'Z:\\data space+\\C. elegans chemotaxis\\20250219_WBI\\trash\\20250219_4.5g-ov-nan_3']\n",
    "# # 放入所有包含单个实验文件的文件夹\n",
    "\n",
    "# position_file_names  = ['s1']\n",
    "# vol_time_file_names = ['t1']\n",
    "\n",
    "# # position_file_names = ['s1', 's1', 's2', 's1', 's1']\n",
    "# # vol_time_file_names = ['t1','t1','t2','t1','t1'] \n",
    "\n",
    "# required_files = ['stage_data.txt', 'c1.txt','s-data.txt']   # 数据处理需要的文件\n",
    "# required_files_set = set(required_files)\n",
    "# write = True    # 是否写出文件\n",
    "# hlf_spd_inv = 0.5\n",
    "# hlf_agl_spd_inv = 0.2\n",
    "# hlf_agl_inv = 1\n",
    "# Pos_CTX_vec = [-1,0]\n",
    "# t_crit = 3.8\n",
    "# min_agl_spd = 75\n",
    "# cut_window = 2000\n",
    "# half_width = 20  #mm 琼脂片宽度的一半\n",
    "\n",
    "\n",
    "# for i, p_f in enumerate(file_paths):\n",
    "    \n",
    "#     files = [f for f in os.listdir(p_f) if '.txt' in f ]\n",
    "#     # 转化为集合进行比较\n",
    "#     files_set = set(files)\n",
    "#     # 判断文件夹中的文件是否完全包含自定义列表中的元素\n",
    "#     is_contained = required_files_set.issubset(files_set)\n",
    "#     # 找出missing_files\n",
    "#     missing_files = [file for file in required_files if file not in files]\n",
    "    \n",
    "#     if is_contained:\n",
    "#         print(f'\\n\\n=======开始处理{os.path.basename(os.path.normpath(p_f))}中文件，\\n包含:{files}')\n",
    "#         # 预处理：时间戳对齐\n",
    "#         Align_timestamp(p_f)\n",
    "#         for f in files:\n",
    "#             if (len(position_file_names)==1):\n",
    "#                 if position_file_names[0] in f:\n",
    "#                     # 运动参数位置文件\n",
    "#                     column_names = ['Time', 'X', 'Y']\n",
    "#                     df_motion = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None,\n",
    "#                                             names=column_names)\n",
    "#                     df_motion = df_motion.astype(float)\n",
    "\n",
    "# #                     # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "# #                     df_label, df_motion = Realign_coordinate(p_f, df_motion)\n",
    "# #                     # 打印轨迹\n",
    "\n",
    "\n",
    "#                 elif vol_time_file_names[0] in f:\n",
    "#                     # volume时间戳文件\n",
    "#                     column_names = ['Vol_Time']\n",
    "#                     df_vol_time = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None, names=column_names)\n",
    "\n",
    "#             elif (len(position_file_names)>1):\n",
    "#                 if position_file_names[i] in f:\n",
    "#                     # 运动参数位置文件\n",
    "#                     column_names = ['Time', 'X', 'Y']\n",
    "#                     df_motion = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None, names=column_names)\n",
    "#                     df_motion = df_motion.astype(float)\n",
    "\n",
    "#                     # 转换坐标，统一x轴坐标起点为0，终点为4.5\n",
    "#                     df_label, df_motion = Realign_coordinate(p_f, df_motion)\n",
    "#                     # 打印轨迹\n",
    "\n",
    "\n",
    "#                 elif vol_time_file_names[i] in f:\n",
    "#                     # volume时间戳文件\n",
    "#                     column_names = ['Vol_Time']\n",
    "#                     df_vol_time = pd.read_csv(os.path.join(p_f, f),sep=r'\\s+', header=None, names=column_names)\n",
    "\n",
    "\n",
    "#         # 查找在motion.Time中最近值和索引，加入df_vol_time中\n",
    "#         nearest_times = []\n",
    "#         nearest_indices = []\n",
    "#         for vol_time in df_vol_time['Vol_Time']:\n",
    "#             # 计算时间差的绝对值\n",
    "#             differences = np.abs(df_motion['Time'] - vol_time)\n",
    "#             # 找到最小差值的索引\n",
    "#             nearest_idx = np.argmin(differences)\n",
    "#             # 保存最近时间值和索引\n",
    "#             nearest_times.append(df_motion.loc[nearest_idx, 'Time'])\n",
    "#             nearest_indices.append(nearest_idx)\n",
    "\n",
    "#         # 将结果存入 DataFrame\n",
    "#         df_vol_time['Nearest_Time'] = nearest_times\n",
    "#         df_vol_time['Nearest_Index'] = nearest_indices\n",
    "\n",
    "\n",
    "#         # 根据运动位置信息抽取运动参数\n",
    "\n",
    "#         mean_frame_rate = np.mean(1/np.diff(df_motion.Time.values))\n",
    "#         df_motion_als = Sliding_CTX_calation(df_motion, Pos_CTX_vec, hlf_spd_inv, hlf_agl_spd_inv, hlf_agl_inv,\n",
    "#                                           window_size=100, frame_rate=mean_frame_rate)\n",
    "#         df_motion_pt = Label_Pirouettes_by_id(df_motion_als, t_crit=t_crit, min_agl_spd=min_agl_spd)\n",
    "#         # visualization check\n",
    "#         Scatter_Pirouette(df_motion_pt, folder=p_f+'\\\\', x_lim = [0,40])\n",
    "#         Scatter_Pos_Grad_Dir(df_motion_pt, Pos_CTX_vec, folder=p_f+'\\\\', x_lim = [0,40])\n",
    "#     #     cut_and_draw(df_motion_pt, n = cut_window)\n",
    "\n",
    "#         # 将df_motionz中的运动参数按照索引对应到神经数据时间戳上\n",
    "#         columns_to_add = ['X', 'Y', 'speed', 'agl_speed', 'CTX_left', 'Event', 'Reorientation']\n",
    "#         for col in columns_to_add:\n",
    "#             df_vol_time[col] = df_vol_time['Nearest_Index'].map(df_motion_pt[col])\n",
    "\n",
    "#         # 加入condition\n",
    "#         folder_name = os.path.basename(os.path.normpath(p_f))\n",
    "#         features = folder_name.split('_')\n",
    "#         df_motion_pt['Date'] = features[0]\n",
    "#         df_vol_time['Date'] = features[0]\n",
    "#         df_motion_pt['Rep_id'] = features[-1]\n",
    "#         df_vol_time['Rep_id'] = features[-1]\n",
    "#         condition_ls = features[-2].split('-')\n",
    "#         print(f'日期:{features[0]}, 编号：{features[-1]}\\n Conditions:{condition_ls}')\n",
    "#         for i, c in enumerate(condition_ls):\n",
    "#             if len(condition_ls) > 1:\n",
    "#                 con_col = 'Condition'+str(i)\n",
    "#                 df_motion_pt[con_col] = c\n",
    "#                 df_vol_time[con_col] = c\n",
    "#             else:\n",
    "#                 df_motion_pt['Condition'] = features[-2]\n",
    "#                 df_vol_time['Condition'] = features[-2]\n",
    "#         if len(condition_ls) > 1:\n",
    "#             df_vol_time['Date_Con_Rep'] = features[-2]\n",
    "\n",
    "#         df_motion_pt['Disp_to_mid'] = df_motion_pt['X']-half_width\n",
    "#         df_vol_time['Disp_to_mid'] = df_vol_time['X']-half_width\n",
    "#         # 写出文件\n",
    "#         if write:\n",
    "#             # folder_name为当前数据日期批次\n",
    "#             fn_motion_output = folder_name+'_MotionExtract.csv'\n",
    "#             df_motion_pt.to_csv(os.path.join(p_f, fn_motion_output), index = False)\n",
    "#             print(f'写出抽取的运动参数文件到:{folder_name}/{fn_motion_output}')\n",
    "#             fn_nt_output = folder_name+'_NeuralAlignMotion.csv'\n",
    "#             df_vol_time.to_csv(os.path.join(p_f, fn_nt_output), index = False)\n",
    "#             print(f'写出与神经数据时间戳对应的运动参数文件到:{folder_name}/{fn_nt_output}')\n",
    "#     else:\n",
    "#         print(f'{os.path.basename(os.path.normpath(p_f))}中缺少数据处理必须的文件如下：\\n' + \"\\n\".join(missing_files))\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7da7c6",
   "metadata": {},
   "source": [
    "以不合并所有数据的方式作图，优点是节省内存，方便，缺点是不利于排序和组间比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_subfolders(file_path_all, nokey_1 = 'trash', nokey_2 ='*'):\n",
    "    '''\n",
    "    file_path_all：输入包含所有一级文件夹的地址的list\n",
    "    （每个一级文件夹中包含单个日期下的所有数据文件的子文件夹）\n",
    "    '''\n",
    "    file_paths = []\n",
    "    for fir_p_f in file_path_all:\n",
    "        file_paths += [os.path.join(fir_p_f,f_p) for f_p in os.listdir(fir_p_f) if\n",
    "                      (os.path.isdir(os.path.join(fir_p_f, f_p))) & (nokey_1 not in f_p) & (nokey_2 not in f_p)]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b666d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_all = [r'Z:\\data space+\\C. elegans chemotaxis\\20250221_WBI', r'Z:\\data space+\\C. elegans chemotaxis\\20250219_WBI']\n",
    "file_paths = get_data_subfolders(file_path_all)\n",
    "print('所有用于作图的数据:\\n'+\"\\n\".join(file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d54cdd",
   "metadata": {},
   "source": [
    "## 轨迹分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe14f5",
   "metadata": {},
   "source": [
    "### 轨迹分布大图(所有数据画在一个图上)\n",
    "+ 将所有包含单次数据的文件夹放在一个总文件夹中，file_path_all\n",
    "+ 依次读出所有单次数据，不合并为一个df作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67e102",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fs = 20\n",
    "# 可视化所有轨迹\n",
    "figure_0,ax = plt.subplots(figsize = (10,10))\n",
    "# 定义要使用的 sequential colormaps\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "             'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "             'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "for i, p_f in enumerate(file_paths):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 设置截断colormap\n",
    "    orig_cmap = mpl.colormaps[colormaps[i%len(colormaps)]]\n",
    "    # 重新缩放 colormap，使最浅部分变深\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        \"truncated_cmap\", orig_cmap(np.linspace(0.3, 1, 256))  # 从 30% 处开始\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    for f in os.listdir(p_f):\n",
    "        if 'NeuralAlignMotion' in f:\n",
    "            df_motion = pd.read_csv(os.path.join(p_f, f))\n",
    "    base_name = os.path.basename(p_f)\n",
    "#     name,_= os.path.splitext(base_name)\n",
    "    ax.scatter(df_motion['X'], df_motion['Y'], c = df_motion['Time'], cmap = new_cmap, \n",
    "               s = 1)\n",
    "    first_valid_idx = df_motion['X'].first_valid_index()\n",
    "    \n",
    "    ax.scatter(df_motion.loc[first_valid_idx, 'X'], df_motion.loc[first_valid_idx, 'Y'], \n",
    "              s=30, c = 'r', marker='*')\n",
    "    ax.scatter([], [], color=legend_color, label=base_name, s=5, marker='o')  # 仅用于图例\n",
    "    ax.axvline(x=20, color = 'grey', ls = 'dashed', alpha = 0.2, lw = 2)\n",
    "    # 定义图例的颜色\n",
    "    legend_color = new_cmap(0.7)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), markerscale=2, fontsize=20)\n",
    "    ax.set_title('Trajectories', fontsize = fs*1.5)\n",
    "    ax.tick_params(axis = 'both', labelsize = fs)\n",
    "    ax.set_xlabel('mm', fontsize = fs)\n",
    "    ax.set_ylabel('mm', fontsize = fs)\n",
    "    ax.set_xlim([-1,41])\n",
    "    ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fe148",
   "metadata": {},
   "source": [
    "#### 分子图画轨迹\n",
    "+ 将所有包含单次数据的文件夹放在一个总文件夹中，file_path_all\n",
    "+ 依次读出所有单次数据，不合并为一个df作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17fc29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# file_path_all = r'Z:\\data space+\\C. elegans chemotaxis\\20250221_WBI'\n",
    "# file_paths = [os.path.join(file_path_all,f_p) for f_p in os.listdir(file_path_all)]\n",
    "\n",
    "# 定义要使用的 sequential colormaps\n",
    "colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "             'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "             'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "# 计算每个子图的高度比例\n",
    "height_ratios = []\n",
    "\n",
    "\n",
    "for p_f in file_paths:\n",
    "    for f in os.listdir(p_f):\n",
    "        if 'NeuralAlignMotion' in f:\n",
    "            df_motion = pd.read_csv(os.path.join(p_f, f))\n",
    "            y_min = df_motion['Y'].min()\n",
    "            y_max = df_motion['Y'].max()\n",
    "            y_range = y_max - y_min\n",
    "            \n",
    "            # 增加 10% 的缓冲区\n",
    "            padding = 0.1 * y_range\n",
    "            y_min -= padding\n",
    "            y_max += padding\n",
    "            y_range_with_padding = y_max - y_min\n",
    "            \n",
    "            height_ratios.append(y_range_with_padding)\n",
    "\n",
    "# 创建大图\n",
    "figure_0, ax = plt.subplots(\n",
    "    len(file_paths), 1, \n",
    "    figsize=(10, 3 * len(file_paths)), \n",
    "    sharex=True,  # 共享 x 轴\n",
    "    gridspec_kw={'height_ratios': height_ratios}  # 子图高度自适应\n",
    ")\n",
    "\n",
    "# 找到所有子图中 Vol_Time 的最大值，作为 colorbar 的统一上限\n",
    "vmax = max(\n",
    "    pd.read_csv(os.path.join(p_f, f))['Vol_Time'].max()\n",
    "    for p_f in file_paths\n",
    "    for f in os.listdir(p_f)\n",
    "    if 'NeuralAlignMotion' in f\n",
    ")\n",
    "\n",
    "# 绘制每个子图\n",
    "for i, p_f in enumerate(file_paths):\n",
    "    for f in os.listdir(p_f):\n",
    "        if 'NeuralAlignMotion' in f:\n",
    "            df_motion = pd.read_csv(os.path.join(p_f, f))\n",
    "            \n",
    "    # 设置colormap \n",
    "    # 设置截断colormap\n",
    "    orig_cmap = mpl.colormaps[colormaps[i]]\n",
    "    # 重新缩放 colormap，使最浅部分变深\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        \"truncated_cmap\", orig_cmap(np.linspace(0.3, 1, 256))  # 从 30% 处开始\n",
    "    )\n",
    "    \n",
    "    # 标题使用文件名特征\n",
    "    base_name = os.path.basename(p_f)\n",
    "    \n",
    "    # 绘制散点图\n",
    "    scatter = ax[i].scatter(df_motion['X'], df_motion['Y'], c=df_motion['Vol_Time'], \n",
    "                            cmap=new_cmap, s=1, vmin=0, vmax=vmax)  # 统一 colorbar 上限\n",
    "    ax[i].axvline(x=20, ls='dashed', c='r')  # 添加红色虚线\n",
    "    \n",
    "    # 标记起始位置\n",
    "    first_valid_idx = df_motion['X'].first_valid_index()\n",
    "    \n",
    "    ax[i].scatter(df_motion.loc[first_valid_idx, 'X'], df_motion.loc[first_valid_idx, 'Y'], c = 'r', marker = '*', s = 25)\n",
    "    \n",
    "    # 间隔指定时间打印时间节点文字\n",
    "    interval = 300   # s\n",
    "    df_motion_dn = df_motion.dropna(subset=['X', 'Y', 'Vol_Time']).reset_index(drop = True)  # 删除包含 NaN 的行\n",
    "    \n",
    "    # 最后打印的时间序列元素是原始时间\n",
    "    time_series = np.arange(df_motion_dn.Vol_Time.min(), df_motion_dn.Vol_Time.max(), interval)\n",
    "\n",
    "    # 转为字符串，便于打印\n",
    "    time_list = list(np.int32(time_series))\n",
    "    time_list_str = list(map(str, time_list))\n",
    "\n",
    "    # 取完整时间序列，扩展成矩阵\n",
    "    time_vec = df_motion_dn.Vol_Time\n",
    "    mat_time = np.tile(time_vec, (len(time_series), 1))\n",
    "\n",
    "    # 使用广播机制，将矩阵每一行减去时间序列对应元素\n",
    "    mat_time_subtracted = mat_time-time_series[:,np.newaxis]\n",
    "    # 返回每一行最小值的索引\n",
    "#     min_diff_idx = np.argmin(np.abs(mat_time_subtracted), axis=1)\n",
    "    min_diff_idx = np.abs(mat_time_subtracted).argmin(axis=1)\n",
    "#     ax[i].text(df_motion.loc[min_diff_idx, 'X'],df_motion.loc[min_diff_idx, 'Y'], time_list_str, c = 'r', fontsize = 5)\n",
    "    # zip将两个列表打包\n",
    "    for idx, time_str in zip(min_diff_idx, time_list_str):\n",
    "        ax[i].text(df_motion_dn.loc[idx, 'X'], \n",
    "                   df_motion_dn.loc[idx, 'Y'], \n",
    "                   time_str+'s', c='r', fontsize=10)\n",
    "\n",
    "    # 设置标题\n",
    "    ax[i].set_title(base_name, fontsize=20)\n",
    "    \n",
    "    # 设置 y 轴范围（增加缓冲区）\n",
    "    y_min = df_motion['Y'].min()\n",
    "    y_max = df_motion['Y'].max()\n",
    "    padding = 0.1 * (y_max - y_min)  # 10% 的缓冲区\n",
    "    ax[i].set_ylim(y_min - padding, y_max + padding)\n",
    "    ax[i].tick_params(axis = 'both', labelsize = 15)\n",
    "    # 设置子图宽高比\n",
    "    ax[i].set_aspect('equal')\n",
    "    ax[i].set_ylabel('mm', fontsize = 15)\n",
    "    if i == len(file_paths)-1:\n",
    "        \n",
    "        ax[i].set_xlabel('mm', fontsize = 15)\n",
    "    # 为每个子图添加 colorbar\n",
    "    divider = make_axes_locatable(ax[i])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)  # colorbar 宽度为 5%，间距为 0.1\n",
    "    cbar = plt.colorbar(scatter, cax=cax)\n",
    "    cbar.set_label('Time(s)', fontsize = 15)\n",
    "\n",
    "# 调整子图间距\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdea96",
   "metadata": {},
   "source": [
    "#### 分子图画轨迹分布\n",
    "+ 根据时间段出图，每一行为单个重复组的数据，每一列表示时间窗\n",
    "+ 将所有包含单次数据的文件夹放在一个总文件夹中，file_path_all\n",
    "+ 依次读出所有单次数据，不合并为一个df作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa76da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interval_min = 5  # min\n",
    "interval_s = interval_min*60 # s\n",
    "time_series = []\n",
    "bin_size = 2.5 # mm\n",
    "x_min, x_max = 0, 40  # 设定 X 的范围\n",
    "\n",
    "\n",
    "# 定义要使用的 sequential colormaps\n",
    "colormaps = ['Grey', 'Purple', 'Blue', 'Green', 'Orange', 'Red']\n",
    "num_colormap = len(colormaps)\n",
    "fs = 25\n",
    "\n",
    "# 找出最大时间\n",
    "max_times = []\n",
    "for i, p_f in enumerate(file_paths):\n",
    "    for f in os.listdir(p_f):\n",
    "        if 'NeuralAlignMotion' in f:\n",
    "            df_motion = pd.read_csv(os.path.join(p_f, f))\n",
    "            max_times.append(df_motion.Vol_Time.max())\n",
    "max_time = max(max_times)\n",
    "print(f'所有数据中的最大时间{max_time}')\n",
    "\n",
    "# 根据最大时间分时间段间隔\n",
    "if not len(time_series):\n",
    "    time_series = np.arange(0, max_time, interval_s)\n",
    "    \n",
    "\n",
    "# 创建大图\n",
    "figure_0, ax = plt.subplots(\n",
    "    len(file_paths), len(time_series)-1, \n",
    "    figsize=(5*(len(time_series)-1), 2.5 * len(file_paths)), \n",
    "    sharex=True,  # 共享 x 轴\n",
    "    sharey=True  \n",
    ")\n",
    "\n",
    "\n",
    "# 绘制每个子图\n",
    "for i, p_f in enumerate(file_paths):\n",
    "    for f in os.listdir(p_f):\n",
    "        if 'NeuralAlignMotion' in f:\n",
    "            df_motion = pd.read_csv(os.path.join(p_f, f))\n",
    "    \n",
    "    # 提取文件名\n",
    "    base_name = os.path.basename(p_f)\n",
    "#     name, _ = os.path.splitext(base_name)\n",
    "    \n",
    "    # 分割X区间\n",
    "    \n",
    "    bins = list(np.arange(x_min, x_max + bin_size, bin_size))  # 创建区间边界\n",
    "    bins_array = np.array(bins)\n",
    "    labels = list((bins_array[:-1]+bins_array[1:])/2)\n",
    "    # 使用 pd.cut() 将 X 分割成区间\n",
    "    df_motion['X_bin'] = pd.cut(df_motion['X'], bins=bins,labels=labels, right=False)  # 右开区间\n",
    "    \n",
    "    for j in range(len(time_series)-1):\n",
    "        \n",
    "        df_mot_time = df_motion[(df_motion['Vol_Time']>=time_series[j])&(df_motion['Vol_Time']<time_series[j+1])]\n",
    "    \n",
    "        # 绘制柱状图\n",
    "        sum_timepoint = df_mot_time['X_bin'].notna().count()\n",
    "        df_gb_xbin = df_mot_time.groupby('X_bin').agg({'Vol_Time':'count'}).reset_index()\n",
    "        df_gb_xbin = df_gb_xbin.rename(columns={'Vol_Time':'count'})\n",
    "        df_gb_xbin['ratio_count'] = df_gb_xbin['count']/sum_timepoint\n",
    "        ax[i][j].bar(df_gb_xbin['X_bin'],df_gb_xbin['ratio_count'], width=2,color = colormaps[i%num_colormap])\n",
    "#         ax[i].axvline(x=20, ls='dashed', c='r')  # 添加红色虚线\n",
    "        ax[i][j].tick_params(axis = 'both', labelsize = fs)\n",
    "        if i == 0:\n",
    "            # 设置标题\n",
    "            begin_min = int(time_series[j]/60)\n",
    "            end_min = int(time_series[j+1]/60)\n",
    "            ax[i][j].set_title(f'{str(begin_min)}~{str(end_min)}min', fontsize=fs*1.25)\n",
    "        if j == 0:\n",
    "            y_labels = '\\n'.join(base_name.split('_'))\n",
    "            ax[i][j].set_ylabel(y_labels, fontsize = fs, rotation=60, labelpad = fs*1.5)\n",
    "        # 设置x轴刻度范围\n",
    "        # 设置 X 轴刻度间隔\n",
    "        ax[i][j].set_xticks(np.arange(x_min, x_max+1, 10))  # 设置刻度间隔为 5mm\n",
    "        ax[i][j].grid(True, color='grey', alpha = 0.5, ls = 'dashed')\n",
    "        \n",
    "        if i == len(file_paths)-1:\n",
    "            ax[i][j].set_xlabel('mm', fontsize = fs, labelpad = fs*0.25)\n",
    "\n",
    "# 调整子图间距\n",
    "plt.subplots_adjust(hspace=0.2, wspace = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276796a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
